{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"CONTRIBUTING/","title":"Welcome to the AUCMEDI contributing guide","text":"<p>Thank you for investing your time in contributing to our project! </p> <p>In this guide you will get an overview of the contribution workflow from opening an issue, creating a PR, reviewing, and merging the PR.</p>"},{"location":"CONTRIBUTING/#list-of-contributors","title":"List of Contributors","text":"<p>AUCMEDI is designed as an open-source community project with a high focus on automated documentation and unittesting to allow 3rd party contributions.</p> <p>This means: Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p> </p>"},{"location":"CONTRIBUTING/#issue","title":"Issue","text":"<p>The simplest form of contribution is the communication to us! </p> <p>The issue tracker or discussion forum in our GitHub repository is a powerful tool for communication between user and developer.  </p>"},{"location":"CONTRIBUTING/#findreport-an-issue","title":"Find/Report an issue","text":"<p>If you spot a problem with AUCMEDI, search if an issue already exists. If a related issue doesn't exist, you can open a new issue.</p> <p>Reporting a bug, a feature request or just an experience is extremely helpful for us - thank you!</p> <p>Here are some advices for issue communication:</p> <ul> <li>A bug is a demonstrable problem that is caused by the code in the repository.</li> <li>Use the GitHub issue search \u2014 check if the issue has already been reported.</li> <li>Check if the issue has been fixed \u2014 try to reproduce it using the latest master or development branch in the repository.</li> <li>Isolate the problem \u2014 create a reduced test case and a live example.</li> <li>A good bug report shouldn't leave others needing to chase you up for more information.</li> <li>Please try to be as detailed as possible in your report. All details will help people to fix any potential bugs.</li> <li>Take a moment to find out whether your feature request fits with the scope and aims of this project.</li> </ul>"},{"location":"CONTRIBUTING/#solve-an-issue","title":"Solve an issue","text":"<p>Scan through our existing issues to find one that interests you. You can narrow down the search using <code>labels</code> as filters. If you find an issue to work on, you are welcome to open a PR with a fix / new feature.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Here, we want to cite the guideline of Marc Diethelm (slightly modified):</p> <p>1) Create a personal fork of the project on Github. 2) Clone the fork on your local machine. Your remote repo on Github is called origin. 3) Add the original repository as a remote called upstream. 4) If you created your fork a while ago be sure to pull upstream changes into your local repository. 5) Create a new branch to work on! Branch from development. 6) Implement/fix your feature, comment your code. 7) Follow the code style of the project, including indentation. 8) If the project has tests run them! (unittesting!) 9) Write or adapt tests as needed. 10) Add or change the documentation as needed. (docstrings!) 11) Push your branch to your fork on Github, the remote origin. 12) From your fork open a pull request in the correct branch. Target the project's development branch. 13) Multiple automatic checks (GitHub Actions) will be applied on your fork to ensure functionality. Wait until all checks are successful.   14) Wait for reviewers to have a look on your contributions. 15) Communicate with the reviewers and revise your contribution if necessary. 16) Once the pull request is approved and merged you can pull the changes from upstream to your local repo and delete your extra branch(es).  </p>"},{"location":"CONTRIBUTING/#special-contribution-notes-for-aucmedi","title":"Special Contribution Notes for AUCMEDI","text":"<ul> <li>Pull requests should always target the development branch!</li> <li>Unittesting is key in AUCMEDI. Please provide unittests for all new contributed features in your Pull Request.</li> <li>For documentation, AUCMEDI heavily utilizes docstrings. Internal docstrings are placed on top of the function / class if they should not appear in the API reference.</li> <li>80 character maximum line length for code and 500 character for docstrings. (this is a weak rule, but will be enforced).</li> <li>We recommend a code formater like Black (https://github.com/psf/black) to keep a clean code structure.</li> <li>Git commits have to follow git commit conventions: https://www.conventionalcommits.org/en/v1.0.0/</li> <li>Don't forget to link the pull request to the issue if you are solving one.</li> </ul>"},{"location":"CONTRIBUTING/#your-pr-is-merged","title":"Your PR is merged!","text":"<p>Congratulations  The AUCMEDI team thanks you .</p> <p>Once your PR is merged, your contributions will be publicly visible on GitHub - frankkramer-lab/aucmedi and AUCMEDI Website - Getting Started - Contributing.</p> <p>You are now part of the AUCMEDI contributor community!  </p>"},{"location":"README.PyPI/","title":"README.PyPI","text":"<p>The open-source software AUCMEDI allows fast setup of medical image classification pipelines with state-of-the-art methods via an intuitive, high-level Python API or via an AutoML deployment through Docker/CLI.</p>"},{"location":"README.PyPI/#resources","title":"Resources","text":"<ul> <li>Website: AUCMEDI Website - Home</li> <li>Git Repository: GitHub - frankkramer-lab/aucmedi</li> <li>Documentation: AUCMEDI Wiki - API Reference</li> <li>Getting Started: AUCMEDI Website - Getting Started</li> <li>Examples: AUCMEDI Wiki - Examples</li> <li>Tutorials: AUCMEDI Wiki - Tutorials</li> <li>Applications: AUCMEDI Wiki - Applications</li> <li>PyPI Package: PyPI - aucmedi</li> <li>Docker Image: GitHub - ghcr.io/frankkramer-lab/aucmedi</li> <li>Zenodo Repository: Zenodo - AUCMEDI</li> </ul>"},{"location":"README.PyPI/#how-to-cite","title":"How to cite","text":"<p>AUCMEDI is currently unpublished. But coming soon!</p> <p>In the meantime: Please cite our base framework MIScnn as well as the AUCMEDI GitHub repository:</p> <pre><code>M\u00fcller, D., Kramer, F. MIScnn: a framework for medical image segmentation with\nconvolutional neural networks and deep learning. BMC Med Imaging 21, 12 (2021).\nhttps://doi.org/10.1186/s12880-020-00543-7\n</code></pre> <pre><code>M\u00fcller, D., Mayer, S., Hartmann, D., Meyer, P., Schneider, P., Soto-Rey, I., &amp; Kramer, F. (2022).\nAUCMEDI: a framework for Automated Classification of Medical Images (Version X.Y.Z) [Computer software].\nGitHub repository. https://github.com/frankkramer-lab/aucmedi\n</code></pre> <p>Thank you for citing our work.</p>"},{"location":"README.PyPI/#lead-author","title":"Lead Author","text":"<p>Dominik M\u00fcller\\ Email: dominik.mueller@informatik.uni-augsburg.de\\ IT-Infrastructure for Translational Medical Research\\ University Augsburg\\ Bavaria, Germany</p>"},{"location":"README.PyPI/#license","title":"License","text":"<p>This project is licensed under the GNU GENERAL PUBLIC LICENSE Version 3.\\ See the LICENSE.md file for license rights and limitations.</p>"},{"location":"automl/dataset/","title":"Dataset Structure","text":""},{"location":"automl/dataset/#default-dataset-setup","title":"Default Dataset Setup","text":"<p>The AUCMEDI AutoML expects a fixed dataset structure if run on default parameters.</p> <p>A custom named data directory (in this example <code>aucmedi.data</code>) holds the two required image directories: <code>training</code> for the training process and <code>test</code> for the prediction process.</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/                     # Required for training\n\u2502   \u251c\u2500\u2500 class_a/\n\u2502   \u2502   \u251c\u2500\u2500 img_x.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_b/                  # Subdirectory for each class\n\u2502   \u2502   \u251c\u2500\u2500 img_y.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_c/\n\u2502   \u2502   \u251c\u2500\u2500 img_z.png             # Image names have to be unique\n\u2502   \u2502   \u2514\u2500\u2500 ...                   # between subdirectories\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/                         # Required for prediction\n\u2502   \u251c\u2500\u2500 unknown_img_n.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 model/                        # Will be created by training\n\u251c\u2500\u2500 xai/                          # Will be created by prediction (optional)\n\u251c\u2500\u2500 evaluation/                   # Will be created by evaluation\n\u2514\u2500\u2500 preds.csv                     # Will be created by prediction\n</code></pre> <p>The dataset structure is by default in the working directory for CLI or is mounted as volume into the container for Docker.</p>"},{"location":"automl/dataset/#dataset-io-parameters","title":"Dataset I/O Parameters","text":"<p>The dataset structure can be customized with the following parameters:</p> AutoML Mode I/O Parameter Default Training Input <code>path_imagedir</code> 'training' Training Output <code>path_modeldir</code> 'model' Prediction Input <code>path_imagedir</code> 'test' Prediction Input <code>path_modeldir</code> 'model' Prediction Output <code>path_pred</code> 'preds.csv' Prediction Output <code>xai_directory</code> 'xai' Evaluation Input <code>path_imagedir</code> 'training' Evaluation Input <code>path_pred</code> 'preds.csv' Evaluation Output <code>path_evaldir</code> 'evaluation'"},{"location":"automl/dataset/#csv-dataset-multi-label","title":"CSV Dataset (multi-label)","text":"<p>Instead of utilizing the directory interface for passing the training data class annotations via subdirectories, it is also possible to pass them as CSV.</p> <p>This is required for setup a multi-label classification pipeline.</p> <p>The default dataset structure then would look like this:</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/                     # Required for training\n\u2502   \u251c\u2500\u2500 img_x.png\n\u2502   \u251c\u2500\u2500 img_y.png                 # Single directory with all training images\n\u2502   \u251c\u2500\u2500 img_z.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/                         # Required for prediction\n\u2502    \u251c\u2500\u2500 unknown_img_a.png\n\u2502    \u251c\u2500\u2500 unknown_img_b.png\n\u2502    \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 annotations.csv               # CSV annotations for training\n</code></pre> <p>The CLI command for multi-label CSV data would then look like this:</p> <pre><code>aucmedi training --path_gt annotations.csv --ohe\n</code></pre> One-hot encoded CSV Example <p></p> <p>More information for CSV files can be found here: IO_CSV Interface</p>"},{"location":"automl/overview/","title":"Overview","text":"<p>AUCMEDI integrates Automated Machine Learning (AutoML).</p> <p>The mentality behind AutoML is to ensure easy application, integration and maintenance of complex medical image classification pipelines.</p> <p>Especially for clinical scientists without a computer science background, research and application of state-of-the-art medical image classification approaches are a complex task. The majority of implementations are so called \"island solutions\", which are implemented and specialized for a single dataset without the possibility of fast reproducibility or even translational application on new datasets (even if it is the same image modality).</p> <p>This situation lead to a frustration of clinicians that high performing models are existing but can not be applied without various pipeline modifications.</p> <p>Thus, AUCMEDI wants to provide a fast and intuitive interface through AUCMEDI AutoML for building, application and sharing of state-of-the-art medical image classification models.</p> <p>Wikipedia defines AutoML</p> <p>AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning.</p> <p>The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.</p> <p>Wikipedia - Source</p>"},{"location":"automl/overview/#automl-types-in-aucmedi","title":"AutoML Types in AUCMEDI","text":"<p>AUCMEDI offers two interfaces for automatic building and fast application of state-of-the-art medical image classification pipelines.</p> <p>AutoML Overview of AUCMEDI</p> AutoML Type Required Software Description Command Line Interface (CLI) Python &amp; Dependencies AUCMEDI application via command line script interaction in a local environment. Recommended for research settings. Docker Image Docker AUCMEDI application via command line script interaction in a secure and isolated environment. Recommended for clinical settings."},{"location":"automl/overview/#automl-application","title":"AutoML Application","text":"<p>The CLI as well as Docker interface both utilize the same AutoML modes and pipeline principles.</p> <p>This means that providing the same parameters to either AutoML interface will result into an identical pipeline setup.</p> <p>In a code-wise view, these two commands will perform the same operation:</p> <pre><code># Run AUCMEDI AutoML via CLI (dataset in working directory)\naucmedi training --architecture \"DenseNet121\"\n\n# Run AUCMEDI AutoML via Docker (dataset mounted via volume)\ndocker run \\\n  -v /home/dominik/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  training \\\n  --architecture \"DenseNet121\"\n</code></pre> <p>It is also possible to train a model with CLI and compute predictions in a secure Docker environment. Especially in clinical settings, such setup is commonly applied.</p> <p>An AutoML AUCMEDI call can always be splitted into 3 parts:  </p> <p>1) AutoML Type (program) 2) AutoML Mode 3) Parameters  </p> <p>Example</p> <pre><code># AUCMEDI AutoML call\naucmedi training --architecture \"MobileNetV2\"\n\n# Split into parts\naucmedi                           `# AutoML Type -&gt; here CLI`       \\\ntraining                          `# AutoML Mode -&gt; here training`  \\\n--architecture \"MobileNetV2\"      `# Parameters -&gt; here specific architecture`\n</code></pre>"},{"location":"automl/overview/#automl-workflow","title":"AutoML Workflow","text":"<p>AUCMEDI offers three AutoML modes: <code>training</code>, <code>prediction</code> and <code>evaluation</code>.</p> <p>Training: The training mode fits a single or multiple models with fixed or self-adjusting hyper parameters. Important hyper parameters as the neural network architecture or the general pipeline setup can be passed as arguments to the AutoML call. The training process takes as input images with annotated classification (ground truth) and outputs the fitted model(s).</p> <p>Prediction: The prediction mode utilizes the fitted model(s) to infer the classification of unknown images. Hyper parameters, model weights and general pipeline structure will be loaded from the model directory. The prediction process takes as input unknown images and outputs a CSV file with prediction probabilities.</p> <p>Evaluation: The evaluation mode compares ground truth annotations with predicted classifications to estimate model performance. The evaluation process takes as input images with annotated classification (ground truth) as well as predicted classifications, and outputs various performance evaluation figures and metrics.</p> <p>More information on parameters can be found here: AutoML - Parameters</p> <p> Flowchart diagram of AUCMEDI AutoML showing the pipeline workflow and three AutoML modes: training for model fitting, prediction for inference of unknown images, and evaluation for performance estimation.</p>"},{"location":"automl/parameters/","title":"Parameters","text":""},{"location":"automl/parameters/#automl-mode-training","title":"AutoML Mode: training","text":"<p>The training mode fits a single or multiple models with fixed or self-adjusting hyper parameters.</p> <p>The training process takes as input images with annotated classification (ground truth) and outputs the fitted model(s).</p> <p>Parameter overview for the training process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>training</code> Path to the directory containing the images. I/O <code>--path_modeldir</code> str <code>model</code> Path to the output directory in which fitted models and metadata are stored. I/O <code>--path_gt</code> str <code>None</code> Path to the index/class annotation file if required. (only for 'csv' interface). I/O <code>--ohe</code> bool <code>False</code> Boolean option whether annotation data is sparse categorical or one-hot encoded. Configuration <code>--analysis</code> str <code>standard</code> Analysis mode for the AutoML training. Options: <code>[\"minimal\", \"standard\", \"advanced\"]</code>. Configuration <code>--three_dim</code> bool <code>False</code> Boolean, whether data is 2D or 3D. Configuration <code>--shape_3D</code> str <code>128x128x128</code> Desired input shape of 3D volume for architecture (will be cropped into, format: <code>1x2x3</code>). Configuration <code>--epochs</code> int <code>500</code> Number of epochs. A single epoch is defined as one iteration through the complete data set. Configuration <code>--batch_size</code> int <code>24</code> Number of samples inside a single batch. Configuration <code>--workers</code> int <code>1</code> Number of workers/threads which preprocess batches during runtime. Configuration <code>--metalearner</code> str <code>mean</code> Key for Metalearner or Aggregate function. Configuration <code>--architecture</code> str <code>DenseNet121</code> Key of single or multiple Architectures (only supported for 'analysis=advanced', format: 'KEY' or 'KEY,KEY,KEY). Other <code>--help</code> bool <code>False</code> show this help message and exit. List of Architectures <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.</p> <ul> <li>2D Architectures: aucmedi.neural_network.architectures.image</li> <li>3D Architectures: aucmedi.neural_network.architectures.volume</li> </ul> List of Metalearner <ul> <li>Homogeneous pooling functions: Aggregate</li> <li>Heterogeneous pooling functions: Metalearner</li> </ul>"},{"location":"automl/parameters/#automl-mode-prediction","title":"AutoML Mode: prediction","text":"<p>The prediction mode utilizes the fitted model(s) to infer the classification of unknown images.</p> <p>The prediction process takes as input unknown images and outputs a CSV file with prediction probabilities.</p> <p>Parameter overview for the prediction process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>test</code> Path to the directory containing the images. I/O <code>--path_modeldir</code> str <code>model</code> Path to the output directory in which fitted models and metadata are stored. I/O <code>--path_pred</code> str <code>preds.csv</code> Path to the output file in which predicted csv file should be stored. Configuration <code>--xai_method</code> str <code>None</code> Key for XAI method. Configuration <code>--xai_directory</code> str <code>xai</code> Path to the output directory in which predicted image xai heatmaps should be stored. Configuration <code>--batch_size</code> int <code>24</code> Number of samples inside a single batch. Configuration <code>--workers</code> int <code>1</code> Number of workers/threads which preprocess batches during runtime. Other <code>--help</code> bool <code>False</code> show this help message and exit. List of XAI Methods <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use XAI methods: aucmedi.xai.methods</p>"},{"location":"automl/parameters/#automl-mode-evaluation","title":"AutoML Mode: evaluation","text":"<p>The evaluation mode compares ground truth annotations with predicted classifications to estimate model performance.</p> <p>The evaluation process takes as input images with annotated classification (ground truth) as well as predicted classifications, and outputs various performance evaluation figures and metrics.</p> <p>Parameter overview for the evaluation process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>training</code> Path to the directory containing the ground truth images. I/O <code>--path_gt</code> str <code>None</code> Path to the index/class annotation CSV file (only required for defining the ground truth via 'csv' instead of 'directory' interface). I/O <code>--ohe</code> bool <code>False</code> Boolean option whether annotation data is sparse categorical or one-hot encoded. I/O <code>--path_pred</code> str <code>preds.csv</code> Path to the input file in which predicted csv file is stored. I/O <code>--path_evaldir</code> str <code>evaluation</code> Path to the directory in which evaluation figures and tables should be stored. Other <code>--help</code> bool <code>False</code> show this help message and exit."},{"location":"automl/cli/intro/","title":"Introduction","text":"<p>For AutoML, AUCMEDI offers a Command Line Interface (CLI), which allows command line script interaction in a local environment. Recommended for research settings.</p>"},{"location":"automl/cli/intro/#command-line-interface","title":"Command Line Interface","text":"<p>The AutoML interface CLI allows quickly running AUCMEDI in any IT environment ranging from local laptops up to cluster infrastructure.</p> <p>Wikipedia defines Command-line interface</p> <p>A command-line interpreter or command-line processor uses a command-line interface (CLI) to receive commands from a user in the form of lines of text. This provides a means of setting parameters for the environment, invoking executables and providing information to them as to what actions they are to perform.</p> <p>Today, many users rely upon graphical user interfaces and menu-driven interactions. However, some programming and maintenance tasks may not have a graphical user interface and use a command line.</p> <p>Wikipedia - Source</p>"},{"location":"automl/cli/intro/#installation","title":"Installation","text":"<p>The AUCMEDI CLI is supported for console environments which can run Python: Linux, MacOS and Windows. Core support and unittesting is done on Ubuntu (Linux) with shell &amp; bash.</p> <p>For installation, there are no specific CLI dependencies required.</p> <p>For AUCMEDI, the AutoML CLI is recommended to be installed via PyPI (<code>pip install aucmedi</code>). As dependencies, it is required to install GPU drivers (CUDA) for running Tensorflow and Python.</p> <p>Further resources:</p> <ul> <li>TensorFlow 2 - Installation</li> <li>Python</li> </ul> <p>TensorFlow FAQ: GPU Support</p> <p>How do I install the NVIDIA driver?</p> <p>The recommended way is to use your package manager and install the cuda-drivers package (or equivalent). When no packages are available, you should use an official \"runfile\".</p> <p>Alternatively, the NVIDIA driver can be deployed through a container. Refer to the documentation for more information.</p> <p>TensorFlow - Source</p>"},{"location":"automl/cli/usage/","title":"Usage","text":"<p>This chapter demonstrates the most important processing steps of an AUCMEDI AutoML CLI usage.</p> <p>The dataset structure is by default in the working directory for CLI.</p>"},{"location":"automl/cli/usage/#training","title":"Training","text":"<p>For the start, our working directory must contain the subdirectory <code>training</code>. The <code>training</code> directory must contain all images for model training, sorted class subdirectories.</p> <pre><code>cwd/\n\u2514\u2500\u2500 training/\n    \u251c\u2500\u2500 01_TUMOR/\n    \u2502   \u251c\u2500\u2500 10009_CRC-Prim-HE-03_009.tif_Row_301_Col_151.tif\n    \u2502   \u251c\u2500\u2500 10062_CRC-Prim-HE-02_003b.tif_Row_1_Col_301.tif\n    \u2502   \u251c\u2500\u2500 100B0_CRC-Prim-HE-09_009.tif_Row_1_Col_301.tif\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 02_STROMA/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 03_COMPLEX/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 04_LYMPHO/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 05_DEBRIS/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 06_MUCOSA/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 07_ADIPOSE/\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 08_EMPTY/\n        \u2514\u2500\u2500 ...\n</code></pre> <p>In order to create a high-performance model for clinical decision support, it is required to have one or multiple already fitted models for this imaging task.</p> <p>In our usage example, we will train a ResNet50 model from scratch.</p> <p>run AUCMEDI AutoML training hub</p> $ aucmedi training --architecture ResNet50 --epochs 25<pre><code>2022-07-18 12:57:25.282772: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nEpoch 1/10\n2022-07-18 12:57:32.516662: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n177/177 [==============================] - ETA: 0s - loss: 0.5103 - auc: 0.9563 - f1_score: 0.7556   \nEpoch 1: val_loss improved from inf to 0.20766, saving model to model/model.best_loss.keras\n177/177 [==============================] - 21s 92ms/step - loss: 0.5103 - auc: 0.9563 - f1_score: 0.7556 - val_loss: 0.2077 - val_auc: 0.9864 - val_f1_score: 0.8958 - lr: 1.0000e-04\nEpoch 2/10\n177/177 [==============================] - ETA: 0s - loss: 0.1932 - auc: 0.9893 - f1_score: 0.8842\nEpoch 2: val_loss improved from 0.20766 to 0.18348, saving model to model/model.best_loss.keras\n177/177 [==============================] - 15s 84ms/step - loss: 0.1932 - auc: 0.9893 - f1_score: 0.8842 - val_loss: 0.1835 - val_auc: 0.9891 - val_f1_score: 0.9010 - lr: 1.0000e-04\n...\nEpoch 23/25\n177/177 [==============================] - ETA: 0s - loss: 0.0087 - auc: 0.9999 - f1_score: 0.9894\nEpoch 23: val_loss did not improve from 0.12966\nEpoch 23: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n177/177 [==============================] - 22s 126ms/step - loss: 0.0087 - auc: 0.9999 - f1_score: 0.9894 - val_loss: 0.1477 - val_auc: 0.9933 - val_f1_score: 0.9374 - lr: 1.0000e-05\nEpoch 24/25\n177/177 [==============================] - ETA: 0s - loss: 0.0051 - auc: 1.0000 - f1_score: 0.9950\nEpoch 24: val_loss did not improve from 0.12966\n177/177 [==============================] - 22s 125ms/step - loss: 0.0051 - auc: 1.0000 - f1_score: 0.9950 - val_loss: 0.1377 - val_auc: 0.9934 - val_f1_score: 0.9413 - lr: 1.0000e-06\nEpoch 25/25\n177/177 [==============================] - ETA: 0s - loss: 0.0061 - auc: 1.0000 - f1_score: 0.9941\nEpoch 25: val_loss did not improve from 0.12966\n177/177 [==============================] - 22s 125ms/step - loss: 0.0061 - auc: 1.0000 - f1_score: 0.9941 - val_loss: 0.1374 - val_auc: 0.9941 - val_f1_score: 0.9400 - lr: 1.0000e-06\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: model/plot.fitting_course.png\n</code></pre> <p>The result of the AUCMEDI AutoML training hub is a <code>model</code> directory in the working directory.</p> <p>It contains one or multiple AUCMEDI models with other metadata created during the fitting process.</p> <pre><code>cwd/\n\u251c\u2500\u2500 training/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 model/\n    \u251c\u2500\u2500 logs.training.csv\n    \u251c\u2500\u2500 meta.training.json\n    \u251c\u2500\u2500 model.best_loss.keras\n    \u251c\u2500\u2500 model.last.keras\n    \u2514\u2500\u2500 plot.fitting_course.png\n</code></pre> <p>More information about the parameters for training can be found here: AutoML - Parameters - training.</p>"},{"location":"automl/cli/usage/#inference","title":"Inference","text":"<p>For predicting the classification of unknown images, the images should be stored in the <code>test</code> directory.</p> <pre><code>cwd/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0001.tif\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0002.tif\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0003.tif\n    \u2514\u2500\u2500 UNKNOWN_IMAGE.0004.tif\n</code></pre> <p>The AUCMEDI AutoML prediction hub read out the pipeline configuration and fitted models from the provided <code>model</code> directory.</p> <p>run AUCMEDI AutoML prediction hub</p> $ aucmedi prediction<pre><code>2022-07-18 13:13:47.334439: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-18 13:13:52.783231: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n4/4 [==============================] - 3s 87ms/step\n</code></pre> <p>The results will be stored in a CSV file called <code>preds.csv</code> (by default).</p> <p>The CSV file shows the classification probability for an image for each class.</p> <pre><code>cwd/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0001.tif\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0002.tif\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0003.tif\n\u2502   \u2514\u2500\u2500 UNKNOWN_IMAGE.0004.tif\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p>show inference content of prediction file</p> $ cat preds.csv<pre><code>SAMPLE,01_TUMOR,02_STROMA,03_COMPLEX,04_LYMPHO,05_DEBRIS,06_MUCOSA,07_ADIPOSE,08_EMPTY\nUNKNOWN_IMAGE.0001,0.9947149,5.093858e-05,0.0032877475,0.0004719145,0.00061258266,0.0005081127,1.5236534e-05,0.0003385503\nUNKNOWN_IMAGE.0002,0.12757735,0.3084325,0.52998906,0.008813165,0.012200621,0.01229311,0.00034778274,0.00034644845\nUNKNOWN_IMAGE.0003,0.9978336,4.6700584e-06,0.000806501,6.4442225e-05,0.0011141102,6.125228e-05,5.657194e-05,5.8843718e-05\nUNKNOWN_IMAGE.0004,9.7639786e-05,0.0030071975,0.7069594,0.27908832,0.0037088492,0.0069722794,3.9823564e-05,0.00012642879\n</code></pre> <p>More information about the parameters for training can be found here: AutoML - Parameters - prediction.</p>"},{"location":"automl/cli/usage/#evaluation","title":"Evaluation","text":"<p>For performance estimation of the model, a <code>validation</code> set is required which means the classification prediction of images with a known class annotation.</p> <p>In order to demonstrate the CSV annotation, as well, the validation data is encoded in the following file structure:</p> <pre><code>cwd/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 10070_CRC-Prim-HE-04_036.tif_Row_601_Col_601.tif\n\u2502   \u2502   \u251c\u2500\u2500 10078_CRC-Prim-HE-03_001.tif_Row_151_Col_601.tif\n\u2502   \u2502   \u251c\u2500\u2500 1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301.tif\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 annotations.csv\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p>show annotation content of validation CSV file</p> $ cat validation/annotations.csv<pre><code>SAMPLE,CLASS\n101A0_CRC-Prim-HE-03_034.tif_Row_151_Col_1.tif,01_TUMOR\n1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301.tif,05_DEBRIS\n13111_CRC-Prim-HE-05_009a.tif_Row_751_Col_1351.tif,03_COMPLEX\n...\n</code></pre> <p>The evaluation mode of AUCMEDI requires another prediction call for the new validation images.</p> <p>Compute predictions for validation images with a specific input &amp; output path</p> $ aucmedi prediction --path_imagedir validation/images/ --path_pred validation/preds.csv<pre><code>2022-07-18 13:26:25.699603: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-18 13:26:31.281593: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n20/20 [==============================] - 4s 50ms/step\n</code></pre> <p>show inference content of prediction file</p> $ cat validation/preds.csv<pre><code>SAMPLE,01_TUMOR,02_STROMA,03_COMPLEX,04_LYMPHO,05_DEBRIS,06_MUCOSA,07_ADIPOSE,08_EMPTY\n10070_CRC-Prim-HE-04_036.tif_Row_601_Col_601,0.0002848482,0.031636566,0.00048324154,0.00043562692,0.042952724,0.00074527983,0.92224264,0.001219118\n10078_CRC-Prim-HE-03_001.tif_Row_151_Col_601,0.0065458445,0.486334,0.20805378,8.415832e-05,0.27862436,0.019759992,4.8646994e-05,0.0005491826\n1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301,0.0030586768,0.13436078,0.030891698,0.004924605,0.7850058,0.021087538,0.015678901,0.00499198\n...\n</code></pre> <p>Afterwards, it is possible to estimate the performance based on the annotations and predicted classifications of the validation set.</p> <p>compute performance via AUCMEDI AutoML evaluation</p> $ aucmedi evaluation --path_imagedir validation/images/ --path_gt validation/annotations.csv --path_pred validation/preds.csv<pre><code>/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 12 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.barplot.png\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 10 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.confusion_matrix.png\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 10 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.roc.png\n</code></pre> <p>show file structure of current working directory (after evaluation)</p> <pre><code>cwd/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 annotations.csv\n\u251c\u2500\u2500 evaluation/\n\u2502   \u251c\u2500\u2500 metrics.performance.csv\n\u2502   \u251c\u2500\u2500 plot.performance.barplot.png\n\u2502   \u251c\u2500\u2500 plot.performance.confusion_matrix.png\n\u2502   \u2514\u2500\u2500 plot.performance.roc.png\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p> Resulting evaluation result of AUCMEDI AutoML CLI usage example. File: evaluation/plot.performance.confusion_matrix.png.</p> <p>More information about the parameters for training can be found here: AutoML - Parameters - evaluation.</p>"},{"location":"automl/docker/intro/","title":"Introduction","text":"<p>For AutoML, AUCMEDI offers a Docker Image, which allows command line script interaction in a secure and isolated environment. Recommended for clinical settings.</p>"},{"location":"automl/docker/intro/#docker","title":"Docker","text":"<p>The AutoML interface Docker allows securely running AUCMEDI in a professional IT environment, without risking protection of data privacy or dependency conflicts.</p> <p>Wikipedia defines Docker</p> <p>Docker implements a high-level API to provide lightweight containers that run processes in isolation. Docker containers are standard processes, so it is possible to use kernel features to monitor their execution\u2014including for example the use of tools like strace to observe and intercede with system calls.</p> <p>Docker can package an application and its dependencies in a virtual container that can run on any Linux, Windows, or macOS computer. This enables the application to run in a variety of locations, such as on-premises, in public or private cloud.</p> <p>Wikipedia - Source</p> <p>Platform as a service (PaaS) or application platform as a service (aPaaS) or platform-based service is a category of cloud computing services that allows customers to provision, instantiate, run, and manage a modular bundle comprising a computing platform and one or more applications, without the complexity of building and maintaining the infrastructure typically associated with developing and launching the application(s); and to allow developers to create, develop, and package such software bundles.</p> <p>Wikipedia - Source</p>"},{"location":"automl/docker/intro/#installation","title":"Installation","text":"<p>For running AUCMEDI Docker is required to install the Docker engine, first.</p> <p>More information on How-to-install Docker can be found here: https://docs.docker.com/engine/install/</p> <p>Docker Engine</p> <p>Docker Engine is an open source containerization technology for building and containerizing your applications. Docker Engine acts as a client-server application with:</p> <ul> <li>A server with a long-running daemon process dockerd.</li> <li>APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.</li> <li>A command line interface (CLI) client docker.</li> </ul> <p>The Docker Engine is available on a variety of Linux platforms, macOS and Windows 10 through Docker Desktop, and as a static binary installation.</p> <p>Docker - Source</p>"},{"location":"automl/docker/intro/#docker-image-aucmedi","title":"Docker Image: AUCMEDI","text":"<p>Image Structure:</p> <p>The building structure of the AUCMEDI Docker Image is quite simple and can be summarized as following:</p> <ul> <li>Base Image: TensorFlow environment with GPU support (CUDA)</li> <li>Install required dependencies for AUCMEDI (drivers for cv2)</li> <li>Install AUCMEDI</li> <li>Prepare AUCMEDI dataset-volume interface</li> <li>Run AUCMEDI with user-provided arguments</li> </ul> <p>Base Image: </p> <p>The AUCMEDI Docker Image is based on the latest official Docker Image TensorFlow for GPU support (tensorflow/tensorflow:latest-gpu).</p> <p>The DockerHub page for the TensorFlow Image provides in detail various information which consequently also applies to the AUCMEDI Docker Image: https://hub.docker.com/r/tensorflow/tensorflow/</p> <p>Further information can also be found in the TensorFlow documentation: https://www.tensorflow.org/install/docker/</p> <p>Image Hosting on GitHub Container Registry: </p> <p>The AUCMEDI Docker Image is automatically built for each release in the GitHub repository and published in the GitHub Container Registry.</p> <p>The GitHub Container Registry offers unlimited push and pull actions for Docker Images as well as allows hosting the AUCMEDI Docker Image in the same infrastructure as its source code repository.</p> <p>In order to pull and run the AUCMEDI Docker Image, GitHub Container Registry provides the following Image namespace: <pre><code>docker pull ghcr.io/OWNER/IMAGE_NAME\n</code></pre></p> <p>For AUCMEDI, which is hosted under the organization <code>frankkramer-lab</code> in the repository <code>aucmedi</code>, this results into the following Docker Image:</p> <pre><code># Pull the image from the Container Registry\ndocker pull ghcr.io/frankkramer-lab/aucmedi\n\n# Run a container call\ndocker run \\\n  -v /home/dominik/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi \\\n  training \\\n  --architecture \"DenseNet121\"\n</code></pre> <p>Further information can be found in the GitHub documentation: https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry</p> <p>GPU Support:</p> <p>TensorFlow FAQ: GPU Support</p> <p>How do I install the NVIDIA driver?</p> <p>The recommended way is to use your package manager and install the cuda-drivers package (or equivalent). When no packages are available, you should use an official \"runfile\".</p> <p>Alternatively, the NVIDIA driver can be deployed through a container. Refer to the documentation for more information.</p> <p>TensorFlow - Source</p>"},{"location":"automl/docker/usage/","title":"Usage","text":"<p>This chapter demonstrates the most important processing steps of an AUCMEDI AutoML Docker usage.</p>"},{"location":"automl/docker/usage/#docker-call","title":"Docker Call","text":"<p>The Docker call to run AUCMEDI is formatted as follows:</p> <pre><code>docker run \\\n  -v &lt;ABSOLUTE_HOST_PATH&gt;:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  &lt;AUTOML_MODE&gt; \\\n  &lt;PARAMETERS&gt;\n</code></pre> <ul> <li>Parameter <code>-v</code>: Docker parameter to mount a host directory (left) into the Docker container (right). The container directory must be <code>/data</code> as expected working directory.</li> <li>Parameter <code>--rm</code>: Docker parameter to clean up the created container after execution.</li> <li>Parameter <code>ghcr.io/frankkramer-lab/aucmedi:latest</code>: AUCMEDI Docker Image hosted on GitHub Container Registry.</li> <li>Parameter <code>&lt;AUTOML_MODE&gt;</code>: AUCMEDI AutoML mode (hub) [\"training\", \"prediction\", \"evaluation\"].</li> <li>Parameter <code>&lt;PARAMETERS&gt;</code>: Optional arugments for AUCMEDI AutoML. See AutoML - Parameters.</li> </ul>"},{"location":"automl/docker/usage/#dataset-setup","title":"Dataset Setup","text":"<p>For the Docker interface, data has to be mounted into the Docker container in order to be accessible for AUCMEDI.</p> <p>In this example usage, the directory <code>aucmedi.data/</code> is utilized as Docker Volume mount later.</p> <p>A example dataset structure will look like this:</p> <pre><code>aucmedi.data/\n\u2514\u2500\u2500 training/                     # Required for training/\n    \u251c\u2500\u2500 01_TUMOR/\n    \u2502   \u251c\u2500\u2500 10009_CRC-Prim-HE-03_009.tif_Row_301_Col_151.tif\n    \u2502   \u251c\u2500\u2500 10062_CRC-Prim-HE-02_003b.tif_Row_1_Col_301.tif\n    \u2502   \u251c\u2500\u2500 100B0_CRC-Prim-HE-09_009.tif_Row_1_Col_301.tif\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 02_STROMA/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 03_COMPLEX/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 04_LYMPHO/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 05_DEBRIS/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 06_MUCOSA/\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 07_ADIPOSE/\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 08_EMPTY/\n        \u2514\u2500\u2500 ...\n</code></pre> <p>Danger</p> <p>Passing a host directory into the container as Volume, requires absolute paths!</p> <p>More information about Docker Volumes can be found here: https://docs.docker.com/storage/bind-mounts/</p> <p>Warning</p> <p>Be aware that newly created files in a Docker container have the user permission <code>root</code>.</p> <p>This can lead to issues in automatic processing pipelines.</p> <p>More information about the dataset structure can be found here: AutoML - Dataset.</p>"},{"location":"automl/docker/usage/#training","title":"Training","text":"<p>For the start, our Volume directory must contain the subdirectory <code>training</code>. The <code>training</code> directory must contain all images for model training, sorted class subdirectories.</p> <p>In order to create a high-performance model for clinical decision support, it is required to have one or multiple already fitted models for this imaging task.</p> <p>In our usage example, we will train a ResNet50 model from scratch.</p> <p>run AUCMEDI AutoML training hub</p> console<pre><code>docker run \\\n  -v /root/aucmedi.automl.docker/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  training \\\n  --architecture ResNet50 \\\n  --epochs 25\n</code></pre> output<pre><code>2022-07-18 12:57:25.282772: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nEpoch 1/10\n2022-07-18 12:57:32.516662: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n177/177 [==============================] - ETA: 0s - loss: 0.5103 - auc: 0.9563 - f1_score: 0.7556   \nEpoch 1: val_loss improved from inf to 0.20766, saving model to model/model.best_loss.keras\n177/177 [==============================] - 21s 92ms/step - loss: 0.5103 - auc: 0.9563 - f1_score: 0.7556 - val_loss: 0.2077 - val_auc: 0.9864 - val_f1_score: 0.8958 - lr: 1.0000e-04\nEpoch 2/10\n177/177 [==============================] - ETA: 0s - loss: 0.1932 - auc: 0.9893 - f1_score: 0.8842\nEpoch 2: val_loss improved from 0.20766 to 0.18348, saving model to model/model.best_loss.keras\n177/177 [==============================] - 15s 84ms/step - loss: 0.1932 - auc: 0.9893 - f1_score: 0.8842 - val_loss: 0.1835 - val_auc: 0.9891 - val_f1_score: 0.9010 - lr: 1.0000e-04\n...\nEpoch 23/25\n177/177 [==============================] - ETA: 0s - loss: 0.0087 - auc: 0.9999 - f1_score: 0.9894\nEpoch 23: val_loss did not improve from 0.12966\nEpoch 23: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n177/177 [==============================] - 22s 126ms/step - loss: 0.0087 - auc: 0.9999 - f1_score: 0.9894 - val_loss: 0.1477 - val_auc: 0.9933 - val_f1_score: 0.9374 - lr: 1.0000e-05\nEpoch 24/25\n177/177 [==============================] - ETA: 0s - loss: 0.0051 - auc: 1.0000 - f1_score: 0.9950\nEpoch 24: val_loss did not improve from 0.12966\n177/177 [==============================] - 22s 125ms/step - loss: 0.0051 - auc: 1.0000 - f1_score: 0.9950 - val_loss: 0.1377 - val_auc: 0.9934 - val_f1_score: 0.9413 - lr: 1.0000e-06\nEpoch 25/25\n177/177 [==============================] - ETA: 0s - loss: 0.0061 - auc: 1.0000 - f1_score: 0.9941\nEpoch 25: val_loss did not improve from 0.12966\n177/177 [==============================] - 22s 125ms/step - loss: 0.0061 - auc: 1.0000 - f1_score: 0.9941 - val_loss: 0.1374 - val_auc: 0.9941 - val_f1_score: 0.9400 - lr: 1.0000e-06\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: model/plot.fitting_course.png\n</code></pre> <p>The result of the AUCMEDI AutoML training hub is a <code>model</code> directory in the Volume directory.</p> <p>It contains one or multiple AUCMEDI models with other metadata created during the fitting process.</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 model/\n    \u251c\u2500\u2500 logs.training.csv\n    \u251c\u2500\u2500 meta.training.json\n    \u251c\u2500\u2500 model.best_loss.keras\n    \u251c\u2500\u2500 model.last.keras\n    \u2514\u2500\u2500 plot.fitting_course.png\n</code></pre> <p>More information about the parameters for training can be found here: AutoML - Parameters - training.</p>"},{"location":"automl/docker/usage/#prediction","title":"Prediction","text":"<p>For predicting the classification of unknown images, the images should be stored in the <code>test</code> directory.</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0001.tif\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0002.tif\n    \u251c\u2500\u2500 UNKNOWN_IMAGE.0003.tif\n    \u2514\u2500\u2500 UNKNOWN_IMAGE.0004.tif\n</code></pre> <p>The AUCMEDI AutoML prediction hub read out the pipeline configuration and fitted models from the provided <code>model</code> directory.</p> <p>run AUCMEDI AutoML prediction hub</p> console<pre><code>docker run \\\n  -v /root/aucmedi.automl.docker/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  prediction\n</code></pre> output<pre><code>2022-07-18 13:13:47.334439: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-18 13:13:52.783231: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n4/4 [==============================] - 3s 87ms/step\n</code></pre> <p>The results will be stored in a CSV file called <code>preds.csv</code> (by default).</p> <p>The CSV file shows the classification probability for an image for each class.</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0001.tif\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0002.tif\n\u2502   \u251c\u2500\u2500 UNKNOWN_IMAGE.0003.tif\n\u2502   \u2514\u2500\u2500 UNKNOWN_IMAGE.0004.tif\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p>show inference content of prediction file</p> $ cat aucmedi.data/preds.csv<pre><code>SAMPLE,01_TUMOR,02_STROMA,03_COMPLEX,04_LYMPHO,05_DEBRIS,06_MUCOSA,07_ADIPOSE,08_EMPTY\nUNKNOWN_IMAGE.0001,0.9947149,5.093858e-05,0.0032877475,0.0004719145,0.00061258266,0.0005081127,1.5236534e-05,0.0003385503\nUNKNOWN_IMAGE.0002,0.12757735,0.3084325,0.52998906,0.008813165,0.012200621,0.01229311,0.00034778274,0.00034644845\nUNKNOWN_IMAGE.0003,0.9978336,4.6700584e-06,0.000806501,6.4442225e-05,0.0011141102,6.125228e-05,5.657194e-05,5.8843718e-05\nUNKNOWN_IMAGE.0004,9.7639786e-05,0.0030071975,0.7069594,0.27908832,0.0037088492,0.0069722794,3.9823564e-05,0.00012642879\n</code></pre> <p>More information about the parameters for training can be found here: AutoML - Parameters - prediction.</p>"},{"location":"automl/docker/usage/#evaluation","title":"Evaluation","text":"<p>For performance estimation of the model, a <code>validation</code> set is required which means the classification prediction of images with a known class annotation.</p> <p>In order to demonstrate the CSV annotation, as well, the validation data is encoded in the following file structure:</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 10070_CRC-Prim-HE-04_036.tif_Row_601_Col_601.tif\n\u2502   \u2502   \u251c\u2500\u2500 10078_CRC-Prim-HE-03_001.tif_Row_151_Col_601.tif\n\u2502   \u2502   \u251c\u2500\u2500 1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301.tif\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 annotations.csv\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p>show annotation content of validation CSV file</p> $ cat aucmedi.data/validation/annotations.csv<pre><code>SAMPLE,CLASS\n101A0_CRC-Prim-HE-03_034.tif_Row_151_Col_1.tif,01_TUMOR\n1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301.tif,05_DEBRIS\n13111_CRC-Prim-HE-05_009a.tif_Row_751_Col_1351.tif,03_COMPLEX\n...\n</code></pre> <p>The evaluation mode of AUCMEDI requires another prediction call for the new validation images.</p> <p>Compute predictions for validation images with a specific input &amp; output path</p> console<pre><code>docker run \\\n  -v /root/aucmedi.automl.docker/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  prediction \\\n  --path_imagedir validation/images/ \\\n  --path_pred validation/preds.csv\n</code></pre> output<pre><code>2022-07-18 13:26:25.699603: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-18 13:26:31.281593: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n20/20 [==============================] - 4s 50ms/step\n</code></pre> <p>show inference content of prediction file</p> $ cat aucmedi.data/validation/preds.csv<pre><code>SAMPLE,01_TUMOR,02_STROMA,03_COMPLEX,04_LYMPHO,05_DEBRIS,06_MUCOSA,07_ADIPOSE,08_EMPTY\n10070_CRC-Prim-HE-04_036.tif_Row_601_Col_601,0.0002848482,0.031636566,0.00048324154,0.00043562692,0.042952724,0.00074527983,0.92224264,0.001219118\n10078_CRC-Prim-HE-03_001.tif_Row_151_Col_601,0.0065458445,0.486334,0.20805378,8.415832e-05,0.27862436,0.019759992,4.8646994e-05,0.0005491826\n1012B_CRC-Prim-HE-10_016.tif_Row_1_Col_301,0.0030586768,0.13436078,0.030891698,0.004924605,0.7850058,0.021087538,0.015678901,0.00499198\n...\n</code></pre> <p>Afterwards, it is possible to estimate the performance based on the annotations and predicted classifications of the validation set.</p> <p>compute performance via AUCMEDI AutoML evaluation</p> console<pre><code>docker run \\\n  -v /root/aucmedi.automl.docker/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  evaluation \\\n  --path_imagedir validation/images/ \\\n  --path_gt validation/annotations.csv \\\n  --path_pred validation/preds.csv\n</code></pre> output<pre><code>/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 12 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.barplot.png\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 10 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.confusion_matrix.png\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 10 x 9 in image.\n/usr/local/lib/python3.8/dist-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: evaluation/plot.performance.roc.png\n</code></pre> <p>show file structure of Volume directory (after evaluation)</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/\n\u251c\u2500\u2500 model/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 annotations.csv\n\u251c\u2500\u2500 evaluation/\n\u2502   \u251c\u2500\u2500 metrics.performance.csv\n\u2502   \u251c\u2500\u2500 plot.performance.barplot.png\n\u2502   \u251c\u2500\u2500 plot.performance.confusion_matrix.png\n\u2502   \u2514\u2500\u2500 plot.performance.roc.png\n\u2514\u2500\u2500 preds.csv\n</code></pre> <p> Resulting evaluation result of AUCMEDI AutoML CLI usage example. File: evaluation/plot.performance.confusion_matrix.png.</p> <p>More information about the parameters for training can be found here: AutoML - Parameters - evaluation.</p>"},{"location":"examples/applications/","title":"Applications","text":"<p>AUCMEDI was already / is currently applied in multiple projects, challenges or clinical studies. The Code for these applications is presented in separate GitHub repositories.</p> <p>Even so these implementations are more complex than standard examples, they represent state-of-the-art and functional pipelines which were top-ranked in challenges or are successfully integrated in clinical environments.</p>"},{"location":"examples/applications/#overview","title":"Overview","text":"Application Type AUCMEDI Link Multi-Disease Detection in Retinal Imaging Challenge (rank #7) v0.1.0 RIADD - Grand Challenge Ensemble Learning Analysis Research v0.3.0 ENSMIC Pneumonia Detector via Ensemble Learning Research v0.4.0 xray_pneumonia.ipynb COVID-19 Severity Detection Challenge (rank #5 - ongoing) v0.4.0 COVID-19 Algorithm - Grand Challenge Pathology - Gleason Score Estimation Clinical Study Latest Ongoing"},{"location":"examples/applications/#multi-disease-detection-in-retinal-imaging","title":"Multi-Disease Detection in Retinal Imaging","text":"<p>Preventable or undiagnosed visual impairment and blindness affects billion of people worldwide. Automated multi-disease detection models offer great potential to address this problem via clinical decision support in diagnosis. In this work, we proposed an innovative multi-disease detection pipeline for retinal imaging which utilizes ensemble learning to combine the predictive power of several heterogeneous deep convolutional neural network models. Our pipeline includes state-of-the-art strategies like transfer learning, class weighting, real-time image augmentation and focal loss utilization. Furthermore, we integrated ensemble learning techniques like heterogeneous deep learning models, bagging via 5-fold cross-validation and stacked logistic regression models.</p> <p>Participation at the Retinal Image Analysis for multi-Disease Detection Challenge (RIADD): https://riadd.grand-challenge.org/</p> <p>Reference: Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2021) Multi-Disease Detection in Retinal Imaging Based on Ensembling Heterogeneous Deep Learning Models https://pubmed.ncbi.nlm.nih.gov/34545816/</p>"},{"location":"examples/applications/#ensemble-learning-analysis","title":"Ensemble Learning Analysis","text":"<p>Novel and high-performance medical image classification pipelines are heavily utilizing ensemble learning strategies. The idea of ensemble learning is to assemble diverse models or multiple predictions and, thus, boost prediction performance. However, it is still an open question to what extend as well as which ensemble learning strategies are beneficial in deep learning based medical image classification pipelines.</p> <p>In this work, we proposed a reproducible medical image classification pipeline (ensmic) for analyzing the performance impact of the following ensemble learning techniques: Augmenting, Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing and image augmentation methods as well as nine deep convolution neural network architectures. It was applied on four popular medical imaging datasets with varying complexity. Furthermore, 12 pooling functions for combining multiple predictions were analyzed, ranging from simple statistical functions like unweighted averaging up to more complex learning-based functions like support vector machines.</p> <p>Our results revealed that Stacking achieved the largest performance gain of up to 13% F1-score increase. Augmenting showed consistent improvement capabilities by up to 4% and is also applicable to single model based pipelines. Cross-validation based Bagging demonstrated significant performance gain close to Stacking, which resulted in an F1-score increase up to +11%. Furthermore, we demonstrated that simple statistical pooling functions are equal or often even better than more complex pooling functions. We concluded that the integration of ensemble learning techniques is a powerful method for any medical image classification pipeline to improve robustness and boost performance.</p> <p>Reference: Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022) An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. arXiv e-print: https://arxiv.org/abs/2201.11440</p>"},{"location":"examples/applications/#pneumonia-detector-via-ensemble-learning","title":"Pneumonia Detector via Ensemble Learning","text":"<p>In this work we use the AUCMEDI-Framework to train a deep neural network to classify chest X-ray images as either normal or viral pneumonia. Stratified k-fold cross-validation with k=3 is used to generate the validation-set and 15% of the data are set aside for the evaluation of the models of the different folds and ensembles each. A random-forest ensemble as well as a Soft-Majority-Vote ensemble are built from the predictions of the different folds. Evaluation metrics (Classification-Report, macro F1-scores, Confusion-Matrices, ROC-Curves) of the individual folds and the ensembles show that the classifier works well. Finally Grad-CAM and LIME explainable artificial intelligence (XAI) algorithms are applied to visualize the image features that are most important for the prediction. For Grad-CAM the heatmaps of the three folds are furthermore averaged for all images in order to calculate a mean XAI-heatmap. As the heatmaps of the different folds for most images differ only slightly this averaging procedure works well. However, only medical professionals can evaluate the quality of the features marked by the XAI. A comparison of the evaluation metrics with metrics of standard procedures such as PCR would also be important. Further limitations are discussed.</p> <p>A manuscript describing this application was uploaded to arXiv: https://arxiv.org/abs/2110.01017</p>"},{"location":"examples/automl/","title":"AutoML","text":"<p>The AutoML examples for AUCMEDI are demonstrated in Markdown.</p>"},{"location":"examples/automl/#overview","title":"Overview","text":"Data Set Task AutoML Type Link CHMNIST Multi-Class classifier for Pathology CLI AutoML - CLI - Usage CHMNIST Multi-Class classifier for Pathology Docker AutoML - Docker - Usage"},{"location":"examples/automl/#colorectal-histology-mnist","title":"Colorectal Histology MNIST","text":"<p>Classes: 8 - EMPTY, COMPLEX, MUCOSA, DEBRIS, ADIPOSE, STROMA, LYMPHO, TUMOR Size: 5.000 images Source: https://www.kaggle.com/kmader/colorectal-histology-mnist  </p> <p>Short Description: Automatic recognition of different tissue types in histological images is an essential part in the digital pathology toolbox. Texture analysis is commonly used to address this problem; mainly in the context of estimating the tumour/stroma ratio on histological samples. However, although histological images typically contain more than two tissue types, only few studies have addressed the multi-class problem. For colorectal cancer, one of the most prevalent tumour types, there are in fact no published results on multiclass texture separation. The dataset serves as a much more interesting MNIST or CIFAR10 problem for biologists by focusing on histology tiles from patients with colorectal cancer. In particular, the data has 8 different classes of tissue (but Cancer/Not Cancer can also be an interesting problem).</p> <p>Reference: Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Z\u00f6llner FG. Multi-class texture analysis in colorectal cancer histology. Sci Rep. 2016 Jun 16;6:27988. doi: 10.1038/srep27988. PMID: 27306927; PMCID: PMC4910082.</p>"},{"location":"examples/framework/","title":"Framework","text":"<p>The framework/API examples for AUCMEDI are complete but explained applications implemented in Jupyter Notebooks.   </p> <p>Jupyter Notebooks offer reproducibility by including the output of each coding block, but can also integrate commentary blocks with Markdown. Also, Jupyter Notebooks can be directly displayed in GitHub without any additional software.</p>"},{"location":"examples/framework/#blog-like-example","title":"Blog-like Example","text":"<p>In this example, AUCMEDI is applied with a in-depth explanatory way in order to guide through the overall functionality.</p> <p>Title: \"How to implement a medical image classification pipeline with just a few lines of code\"</p> Type Link Task Article on Medium Blog - Article Brain Tumor Classification Jupyter Notebook blog.notebook.ipynb Skin Lesion Classification PDF blog.notebook.pdf Skin Lesion Classification"},{"location":"examples/framework/#standardized-examples","title":"Standardized Examples","text":"<p>In these examples, AUCMEDI was applied in a standardized way on various medical imaging modalities in order to demonstrate the high adaptability of AUCMEDI.</p> ID Medical Field Imaging Modality Link Manuscript 01 Dermatology Dermatoscopy 01.notebook.ipynb 01.manuscript.pdf 02 Gastroenterology Endoscopy (lower) 02.notebook.ipynb 02.manuscript.pdf 03 Gastroenterology Endoscopy (upper) 03.notebook.ipynb 03.manuscript.pdf 04 Histopathology Microscopy 04.notebook.ipynb 04.manuscript.pdf 05 Radiology X-ray 05.notebook.ipynb 05.manuscript.pdf 06 Gynaecology Ultrasound 06.notebook.ipynb 06.manuscript.pdf 07 Radiology Computed Tomography 07.notebook.ipynb 07.manuscript.pdf 08 Ophthalmology Retinal Imaging 08.notebook.ipynb 08.manuscript.pdf 09 Neurology Magnetic Resonance Imaging 09.notebook.ipynb 09.manuscript.pdf"},{"location":"examples/tutorials/","title":"Tutorials","text":"<p>The first steps to build complex medical image classification pipelines can be difficult.</p> <p>Here, several Jupyter Notebooks that aim to give an introduction into the core functionality of AUCMEDI, are presented. Even though deep learning models are easier to setup with AUCMEDI compared to TensorFlow, AUCMEDI is not less flexible. For this reason, rather than creating one huge \u201cdemonstration Notebook\u201d several small Notebooks were created, so that the reader can pick the sections of interest.</p>"},{"location":"examples/tutorials/#overview","title":"Overview","text":"Topic Link Three Pillars of AUCMEDI tutorial01.ipynb Data Loading from CSV tutorial02.ipynb Data Loading from Directories tutorial03.ipynb Architecture Selection tutorial04.ipynb Custom Image Preprocessing tutorial05.ipynb Data Exploration and Performance Evaluation tutorial06.ipynb Modern Image Augmentation tutorial07.ipynb Transfer Learning tutorial08.ipynb Early Stopping and Transfer Learning tutorial09.ipynb Explainable Artificial Intelligence tutorial10.ipynb Integrating Metadata into a Model tutorial11.ipynb"},{"location":"getstarted/cite/","title":"How to cite","text":"<p>AUCMEDI is currently unpublished. But coming soon!</p> <p>In the meantime: Please cite our application manuscript as well as the AUCMEDI GitHub repository:</p> <pre><code>Mayer, S., M\u00fcller, D., &amp; Kramer F. (2022). Standardized Medical Image Classification\nacross Medical Disciplines. [Preprint] https://arxiv.org/abs/2210.11091.\n\n@article{AUCMEDIapplicationMUELLER2022,\n  title={Standardized Medical Image Classification across Medical Disciplines},\n  author={Simone Mayer, Dominik M\u00fcller, Frank Kramer},\n  year={2022}\n  eprint={2210.11091},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre> <pre><code>M\u00fcller, D., Mayer, S., Hartmann, D., Schneider, P., Soto-Rey, I., &amp; Kramer, F. (2022).\nAUCMEDI: a framework for Automated Classification of Medical Images (Version X.Y.Z) [Computer software].\nhttps://doi.org/10.5281/zenodo.6633540. GitHub repository. https://github.com/frankkramer-lab/aucmedi\n</code></pre> <p>CITATION.cff of the AUCMEDI GitHub repository:</p> <pre><code>cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"M\u00fcller\"\n  given-names: \"Dominik\"\n  orcid: \"https://orcid.org/0000-0003-0838-9885\"\n- family-names: \"Mayer\"\n  given-names: \"Simone\"\n  orcid: \"https://orcid.org/0000-0002-7825-5738\"\n- family-names: \"Hartmann\"\n  given-names: \"Dennis\"\n- family-names: \"Schneider\"\n  given-names: \"Pia\"\n- family-names: \"Soto-Rey\"\n  given-names: \"I\u00f1aki\"\n  orcid: \"https://orcid.org/0000-0003-3061-5818\"\n- family-names: \"Kramer\"\n  given-names: \"Frank\"\n  orcid: \"https://orcid.org/0000-0002-2857-7122\"\ntitle: \"AUCMEDI: a framework for Automated Classification of Medical Images\"\nversion: X.Y.Z\ndoi: 10.5281/zenodo.6633540\ndate-released: 2022-06-01\nurl: \"https://github.com/frankkramer-lab/aucmedi\"\n</code></pre>"},{"location":"getstarted/contribution/","title":"Welcome to the AUCMEDI contributing guide","text":"<p>Thank you for investing your time in contributing to our project! </p> <p>In this guide you will get an overview of the contribution workflow from opening an issue, creating a PR, reviewing, and merging the PR.</p>"},{"location":"getstarted/contribution/#list-of-contributors","title":"List of Contributors","text":"<p>AUCMEDI is designed as an open-source community project with a high focus on automated documentation and unittesting to allow 3rd party contributions.</p> <p>This means: Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p> </p>"},{"location":"getstarted/contribution/#issue","title":"Issue","text":"<p>The simplest form of contribution is the communication to us! </p> <p>The issue tracker or discussion forum in our GitHub repository is a powerful tool for communication between user and developer.  </p>"},{"location":"getstarted/contribution/#findreport-an-issue","title":"Find/Report an issue","text":"<p>If you spot a problem with AUCMEDI, search if an issue already exists. If a related issue doesn't exist, you can open a new issue.</p> <p>Reporting a bug, a feature request or just an experience is extremely helpful for us - thank you!</p> <p>Here are some advices for issue communication:</p> <ul> <li>A bug is a demonstrable problem that is caused by the code in the repository.</li> <li>Use the GitHub issue search \u2014 check if the issue has already been reported.</li> <li>Check if the issue has been fixed \u2014 try to reproduce it using the latest master or development branch in the repository.</li> <li>Isolate the problem \u2014 create a reduced test case and a live example.</li> <li>A good bug report shouldn't leave others needing to chase you up for more information.</li> <li>Please try to be as detailed as possible in your report. All details will help people to fix any potential bugs.</li> <li>Take a moment to find out whether your feature request fits with the scope and aims of this project.</li> </ul>"},{"location":"getstarted/contribution/#solve-an-issue","title":"Solve an issue","text":"<p>Scan through our existing issues to find one that interests you. You can narrow down the search using <code>labels</code> as filters. If you find an issue to work on, you are welcome to open a PR with a fix / new feature.</p>"},{"location":"getstarted/contribution/#pull-requests","title":"Pull Requests","text":"<p>Here, we want to cite the guideline of Marc Diethelm (slightly modified):</p> <p>1) Create a personal fork of the project on Github. 2) Clone the fork on your local machine. Your remote repo on Github is called origin. 3) Add the original repository as a remote called upstream. 4) If you created your fork a while ago be sure to pull upstream changes into your local repository. 5) Create a new branch to work on! Branch from development. 6) Implement/fix your feature, comment your code. 7) Follow the code style of the project, including indentation. 8) If the project has tests run them! (unittesting!) 9) Write or adapt tests as needed. 10) Add or change the documentation as needed. (docstrings!) 11) Push your branch to your fork on Github, the remote origin. 12) From your fork open a pull request in the correct branch. Target the project's development branch. 13) Multiple automatic checks (GitHub Actions) will be applied on your fork to ensure functionality. Wait until all checks are successful.   14) Wait for reviewers to have a look on your contributions. 15) Communicate with the reviewers and revise your contribution if necessary. 16) Once the pull request is approved and merged you can pull the changes from upstream to your local repo and delete your extra branch(es).  </p>"},{"location":"getstarted/contribution/#special-contribution-notes-for-aucmedi","title":"Special Contribution Notes for AUCMEDI","text":"<ul> <li>Pull requests should always target the development branch!</li> <li>Unittesting is key in AUCMEDI. Please provide unittests for all new contributed features in your Pull Request.</li> <li>For documentation, AUCMEDI heavily utilizes docstrings. Internal docstrings are placed on top of the function / class if they should not appear in the API reference.</li> <li>80 character maximum line length for code and 500 character for docstrings. (this is a weak rule, but will be enforced).</li> <li>We recommend a code formater like Black (https://github.com/psf/black) to keep a clean code structure.</li> <li>Git commits have to follow git commit conventions: https://www.conventionalcommits.org/en/v1.0.0/</li> <li>Don't forget to link the pull request to the issue if you are solving one.</li> </ul>"},{"location":"getstarted/contribution/#your-pr-is-merged","title":"Your PR is merged!","text":"<p>Congratulations  The AUCMEDI team thanks you .</p> <p>Once your PR is merged, your contributions will be publicly visible on GitHub - frankkramer-lab/aucmedi and AUCMEDI Website - Getting Started - Contributing.</p> <p>You are now part of the AUCMEDI contributor community!  </p>"},{"location":"getstarted/faq/","title":"Faq","text":"<p>Coming soon.</p> <ul> <li>how to cite</li> </ul>"},{"location":"getstarted/intro/","title":"Introduction","text":"<p>The field of AI-based medical image classification has experienced rapid growth in recent years. The reason for this can be traced back to the increasing availability of medical imaging as well as the improvement in the predictive capabilities of automated image analysis through deep neural networks\u2060. The possible integration of such deep neural networks into clinical routine is therefore currently a highly relevant research topic. Hence, clinicians, especially the imaging disciplines, are using models as clinical decision support in order to improve diagnostic certainty or to automate time-consuming manual processes.</p> <p>However, clinical application studies reveal that the integration of image classification pipelines into a hospital environment presents significant challenges. Solutions from the literature that have already been implemented are usually independent software, so-called isolated solutions, which have been developed and optimized for a specific disease or a single data set\u2060. Due to the lack of generalizability, clinicians are faced with the problem of reusability in their own data sets and therefore no practical use in clinical research is possible.</p> <p>The open-source Python framework AUCMEDI offers a solution for the challenges described. The software package not only offers a library as a high-level API for the standardized construction of modern medical image classification pipelines, but also reproducible installation and direct application via CLI or Docker. With AUCMEDI, researchers are able to set up a complete and easy-to-integrate pipeline for medical image classification with just a few lines of code.</p> <p>AUCMEDI provides two modules:</p> <ul> <li>The framework or API for building medical image classification pipelines in Python  </li> <li>The AutoML module via CLI or Docker for fast application  </li> </ul>"},{"location":"getstarted/intro/#philosophy-of-aucmedi","title":"Philosophy of AUCMEDI","text":"<p>User friendliness: AUCMEDI is an intuitive API designed for human beings, not machines. Building state-of-the-art medical image classification pipelines does not have to mean re-inventing the wheel for every user. To meet the continuously growing interest in medical imaging processing, AUCMEDI provides consistent and simple APIs for minimizing the number of user actions required for common use cases.</p> <p>Modularity: The general steps in medical image processing are identical for nearly all projects. Nevertheless, switching to another neural network architecture or data set format breaks most of the publicly available medical image processing software today. AUCMEDI changes that! In particular, data I/O, pre-/postprocessing functions, metrics and model architectures are standalone interfaces that are easily interchangeable.</p> <p>Easy extensibility: Contributions are simple to integrate into the AUCMEDI pipeline. AUCMEDI provides interfaces and abstract base classes for all types of classes or functions. That facilitates defining the structure and setup of 3rd party code and results in easy integration of e.g. architectures, subfunctions or adapting AUCMEDI to your data structure.</p>"},{"location":"getstarted/intro/#support","title":"Support","text":"<p>For help on how to use AUCMEDI, check out the tutorials, examples and further documentation.</p> <p>If you have any problem or identified a bug, please use the issue system of our GitHub repository. Share your problems and contribute to improve AUCMEDI.</p>"},{"location":"getstarted/intro/#why-this-name-aucmedi","title":"Why this name, AUCMEDI?","text":"<p>Maybe, you are asking yourself: What abbreviation AUCMEDI stands for? The answer is quite simple: Automated Classification of Medical Images.</p> <p>And how should I pronounce it correctly? Answer: AUC-MEDI</p> <p>As two words:  </p> <ul> <li>AUC as in AUROC (area under the curve of a receiver operating characteristic)  </li> <li>MEDI as in MEDICAL  </li> </ul>"},{"location":"getstarted/modules/","title":"Work in Progress!","text":"<p>The software package AUCMEDI not only offers a library as a high-level API for the standardized construction of modern medical image classification pipelines, but also reproducible installation and direct application via CLI or Docker.</p> <p>With AUCMEDI, researchers are able to set up a complete and easy-to-integrate pipeline for medical image classification with just a few lines of code.</p> <p>AUCMEDI Modules</p> Module Description Framework / API for Python The framework or API for building medical image classification pipelines in Python. Can be installed via PyPI (pip ) and used in scripts or Jupyter Notebooks. AutoML via CLI The AutoML module via CLI for fast application. Can be used in a command language fashion (like Unix Shell in Linux) for straightforward integration in IT environments. AutoML via Docker The AutoML module via Docker for robust application. Allow secure and stable integration in critical IT environments (like clinical environments). <p>With the two modules, AUCMEDI combines straightforward and fast setup via AutoML with the option of detailed customization via the framework/API as toolbox.</p> <p> Flowchart diagram of the AUCMEDI modules showing the Framework/API and the AutoML module.</p>"},{"location":"getstarted/setup/","title":"Installation","text":""},{"location":"getstarted/setup/#prerequisite","title":"Prerequisite","text":"<p>AUCMEDI is tested and supported on the following systems:</p> <ul> <li>64-bit Ubuntu 16.04 or later  </li> <li>Python 3.9+ </li> </ul> <p>AUCMEDI is heavily based on TensorFlow 2: an approachable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity.</p> <p>In order to install AUCMEDI, verify that all requirements are complied and functional.</p>"},{"location":"getstarted/setup/#aucmedi-installation","title":"AUCMEDI Installation","text":"<p>There are three ways to install AUCMEDI which depends on the preferred usage:</p>"},{"location":"getstarted/setup/#1-install-from-pypi-recommended","title":"1) Install from PyPI (recommended)","text":"<p>Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows or in a virtual environment without root, you will need to remove sudo to run the commands below.</p> <p>This will allow utilizing the framework (in your favorite Python IDE or in a Jupyter Notebook) or the AutoML module via the command line interface (CLI).</p> <pre><code>pip install aucmedi\n</code></pre>"},{"location":"getstarted/setup/#2-install-from-github-container-registry","title":"2) Install from GitHub Container Registry","text":"<p>This will allow utilizing the AutoML module via Docker.</p> <pre><code>docker pull ghcr.io/frankkramer-lab/aucmedi:latest\n</code></pre>"},{"location":"getstarted/setup/#3-install-from-source-code","title":"3) Install from Source Code","text":"<p>First, clone AUCMEDI using git:</p> <pre><code>git clone https://github.com/frankkramer-lab/aucmedi\n</code></pre> <p>Then, cd to the 'aucmedi' folder and run the install command:</p> Installation with setup.py (framework &amp; AutoML)<pre><code>cd aucmedi\npython setup.py install\n</code></pre> Installation with pip (framework &amp; AutoML)<pre><code>cd aucmedi\npip install .\n</code></pre> Installation with Docker (AutoML)<pre><code>cd aucmedi\ndocker build -t ghcr.io/frankkramer-lab/aucmedi:latest .\n</code></pre>"},{"location":"getstarted/status/","title":"Current Status","text":"<p>AUCMEDI is still under active development, which is why the official status is:  </p>  Work in Progress"},{"location":"getstarted/status/#roadmap","title":"Roadmap","text":"<p>Current Progress: </p> <p> <p>23 of 26 (88%)</p> </p> <p>Features that are already supported by AUCMEDI:</p> <ul> <li> Binary, multi-class and multi-label image classification</li> <li> Support for 2D as well as 3D data</li> <li> Handling class imbalance through class weights &amp; loss weighting like Focal loss</li> <li> Stratified iterative sampling like percentage split and k-fold cross-validation</li> <li> Standard preprocessing functions like Padding, Resizing, Cropping, Normalization</li> <li> Extensive online image augmentation</li> <li> Automated data loading and batch generation</li> <li> Data IO interfaces for csv and subdirectory encoded datasets</li> <li> Transfer Learning on ImageNet weights</li> <li> Large library of popular modern deep convolutional neural network architectures</li> <li> Ensemble Learning techniques like Inference Augmenting</li> <li> Explainable AI (XAI) via Grad-Cam, Backpropagation, ...</li> <li> Clean implementation of the state-of-the-art for competitive application like challenges</li> <li> Full (and automatic) documentation of the complete API reference</li> <li> Started creating examples &amp; applications for the community</li> <li> Available from PyPI for simple installation in various environments</li> <li> Interface for metadata / pandas or NumPy table inclusion in model architectures</li> <li> Unittesting -&gt; CI/CD</li> <li> Clean up Website</li> <li> Integration of bagging and stacking pipelines for utilizing ensemble learning techniques</li> <li> Integrate evaluation functions</li> <li> Support for AutoML via CLI and Docker</li> <li> Documentation for AutoML</li> </ul> <p>Planed milestones and features are: </p> <ul> <li> Examples</li> <li> Tutorials</li> <li> Publication</li> </ul>"},{"location":"getstarted/usage_api/","title":"Quick Start - Framework","text":""},{"location":"getstarted/usage_api/#pillar-structure","title":"Pillar Structure","text":"<p>AUCMEDI is based on 3 pillars which allow building any state-of-the-art medical image classification pipeline.</p> <p>Pillars of AUCMEDI</p> Pillar Type Description #1: input_interface() Function Obtaining general information from the dataset. #2: NeuralNetwork Class Building the deep learning model. #3: DataGenerator Class Powerful interface for loading any images/volumes into the model. <p>The different pillars are represented in Python as function and classes. The passed parameters allow configuration of the automatically created image classification pipeline. The pillar interface of AUCMEDI combines intuitive and straightforward setup with the possibility of detailed custom specification.</p> <p> Flowchart diagram of the AUCMEDI pipeline showing the three pillars acting as access points: input_interface() for extracting relevant dataset information, NeuralNetwork for building the deep learning model and DataGenerator for data loading as well as preprocessing.</p>"},{"location":"getstarted/usage_api/#basic-usage","title":"Basic Usage","text":"<p>In this example, a standard AUCMEDI pipeline build is demonstrated.</p> <p>Install AUCMEDI via PyPI</p> <p>Simply install AUCMEDI with a single line of code via pip.</p> <pre><code>pip install aucmedi\n</code></pre> <p>Build a pipeline</p> <p>Let's build a COVID-19 Detection AI on 2D CT slides!</p> <p><pre><code># AUCMEDI library\nfrom aucmedi import *\n\n# Pillar #1: Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"/home/muellerdo/COVdataset/ct_slides/\",\n                     path_data=\"/home/muellerdo/COVdataset/classes.csv\",\n                     ohe=False,           # OHE short for one-hot encoding\n                     col_sample=\"ID\", col_class=\"PCRpositive\")\n(index_list, class_ohe, nclasses, class_names, image_format) = ds\n\n# Pillar #2: Initialize a DenseNet121 model with ImageNet weights\nmodel = NeuralNetwork(n_labels=nclasses, channels=3,\n                       architecture=\"2D.DenseNet121\",\n                       pretrained_weights=True)\n</code></pre> Congratulations to your ready-to-use Medical Image Classification pipeline including data I/O, preprocessing and a deep learning based neural network model.</p> <p>Train a model and use it!</p> <p>Now, we want to use our pipeline to train it on our training data. Afterwards, we use our fitted model to classify 500 unknown ct slides.</p> <pre><code># Pillar #3: Initialize training Data Generator for first 1000 samples\ntrain_gen = DataGenerator(samples=index_list[:1000],\n                          path_imagedir=\"/home/muellerdo/COVdataset/ct_slides/\",\n                          labels=class_ohe[:1000],\n                          image_format=image_format,\n                          resize=model.meta_input,\n                          standardize_mode=model.meta_standardize)\n# Run model training with Transfer Learning\nmodel.train(train_gen, epochs=20, transfer_learning=True)\n\n# Pillar #3: Initialize testing Data Generator for 500 samples\ntest_gen = DataGenerator(samples=index_list[1000:1500],\n                         path_imagedir=\"/home/muellerdo/COVdataset/ct_slides/\",\n                         labels=None,\n                         image_format=image_format,\n                         resize=model.meta_input,\n                         standardize_mode=model.meta_standardize)\n# Run model inference for unknown samples\npreds = model.predict(test_gen)\n\n# preds &lt;-&gt; NumPy array with shape (500,2)\n# -&gt; 500 predictions with softmax probabilities for our 2 classes\n</code></pre>"},{"location":"getstarted/usage_api/#more-details","title":"More Details","text":"<p>More examples can be found here: Examples - Framework</p> <p>The full documentation for AUCMEDI AutoML can be found here: API Reference</p>"},{"location":"getstarted/usage_automl/","title":"Quick Start - AutoML","text":""},{"location":"getstarted/usage_automl/#automl-types-in-aucmedi","title":"AutoML Types in AUCMEDI","text":"<p>AUCMEDI offers a CLI and Docker interface for automatic building and fast application of state-of-the-art medical image classification pipelines.</p> <p>AutoML Overview of AUCMEDI</p> <p> Flowchart diagram of AUCMEDI AutoML showing the pipeline workflow and three AutoML modes: training for model fitting, prediction for inference of unknown images, and evaluation for performance estimation.</p>"},{"location":"getstarted/usage_automl/#dataset-setup","title":"Dataset Setup","text":"<p>AUCMEDI AutoML expects a fixed dataset structure if run on default parameters. The dataset structure is by default in the working directory for CLI or is mounted as volume into the container for Docker.</p> <pre><code>aucmedi.data/\n\u251c\u2500\u2500 training/                     # Required for training\n\u2502   \u251c\u2500\u2500 class_a/\n\u2502   \u2502   \u251c\u2500\u2500 img_x.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_b/                  # Subdirectory for each class\n\u2502   \u2502   \u251c\u2500\u2500 img_y.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_c/\n\u2502   \u2502   \u251c\u2500\u2500 img_z.png             # Image names have to be unique\n\u2502   \u2502   \u2514\u2500\u2500 ...                   # between subdirectories\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test/                         # Required for prediction\n\u2502   \u251c\u2500\u2500 unknown_img_n.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 model/                        # Will be created by training\n\u251c\u2500\u2500 evaluation/                   # Will be created by evaluation\n\u2514\u2500\u2500 preds.csv                     # Will be created by prediction\n</code></pre>"},{"location":"getstarted/usage_automl/#basic-usage-cli","title":"Basic Usage - CLI","text":"<p>This example demonstrates the basic installation and application of AUCMEDI AutoML with the CLI. The dataset have to be located in the working directory (inside of <code>aucmedi.data/</code>).</p> <p>Install AUCMEDI via PyPI <pre><code>pip install aucmedi\n</code></pre></p> <p>Train a model and classify unknown images <pre><code># Run training with default arguments, but a specific architecture\naucmedi training --architecture \"DenseNet121\"\n\n# Run prediction with default arguments\naucmedi prediction\n</code></pre></p>"},{"location":"getstarted/usage_automl/#basic-usage-docker","title":"Basic Usage - Docker","text":"<p>This example demonstrates the basic installation and application of AUCMEDI AutoML with Docker. The dataset have to be mounted with a volume (with an absolute file path like in the example).</p> <p>Install AUCMEDI via GitHub Container Registry <pre><code>docker pull ghcr.io/frankkramer-lab/aucmedi:latest\n</code></pre></p> <p>Train a model and classify unknown images <pre><code># Run training with default arguments, but a specific architecture\ndocker run \\\n  -v /home/dominik/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  training \\\n  --architecture \"DenseNet121\"\n\n# Run prediction with default arguments\ndocker run \\\n  -v /home/dominik/aucmedi.data:/data \\\n  --rm \\\n  ghcr.io/frankkramer-lab/aucmedi:latest \\\n  prediction\n</code></pre></p>"},{"location":"getstarted/usage_automl/#more-details","title":"More Details","text":"<p>More examples can be found here: Examples - AutoML</p> <p>The full documentation for AUCMEDI AutoML can be found here: AutoML - Overview</p>"},{"location":"reference/","title":"API Reference","text":"<p>This is the API reference for the AUCMEDI framework.</p> <p>Build your state-of-the-art medical image classification pipeline with the 3 AUCMEDI pillars:</p> <p>Pillars of AUCMEDI</p> Pillar Description #1: input_interface() Obtaining general information from the dataset. #2: NeuralNetwork Building the deep learning model. #3: DataGenerator Powerful interface for loading any images/volumes into the model. A typical AUCMEDI pipeline <pre><code># AUCMEDI library\nfrom aucmedi import *\n\n# Pillar #1: Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/classes.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(index_list, class_ohe, nclasses, class_names, image_format) = ds\n\n# Pillar #2: Initialize a DenseNet121 model with ImageNet weights\nmodel = NeuralNetwork(n_labels=nclasses, channels=3,\n                       architecture=\"2D.DenseNet121\",\n                       pretrained_weights=True)\n\n# Pillar #3: Initialize training Data Generator for first 1000 samples\ntrain_gen = DataGenerator(samples=index_list[:1000],\n                          path_imagedir=\"dataset/images/\",\n                          labels=class_ohe[:1000],\n                          image_format=image_format,\n                          resize=model.meta_input,\n                          standardize_mode=model.meta_standardize)\n# Run model training with Transfer Learning\nmodel.train(train_gen, epochs=20, transfer_learning=True)\n\n# Pillar #3: Initialize testing Data Generator for 500 samples\ntest_gen = DataGenerator(samples=index_list[1000:1500],\n                         path_imagedir=\"dataset/images/\",\n                         labels=None,\n                         image_format=image_format,\n                         resize=model.meta_input,\n                         standardize_mode=model.meta_standardize)\n# Run model inference for unknown samples\npreds = model.predict(test_gen)\n</code></pre>"},{"location":"reference/automl/","title":"Automl","text":"<p>API reference for the AUCMEDI AutoML pipeline.</p> <p>The mentality behind AutoML is to ensure easy application, integration and maintenance of complex medical image classification pipelines. AUCMEDI provides a fast and intuitive interface through AUCMEDI AutoML for building, application and sharing of state-of-the-art medical image classification models.</p> <p>The AutoML pipelines are categorized into the following modes: <code>training</code>, <code>prediction</code> and <code>evaluation</code>.</p> <ul> <li>The console entry <code>aucmedi</code> refers to aucmedi.automl.main:main.</li> <li>The Argparse interface for CLI is defined in aucmedi.automl.cli</li> <li>Each AutoML mode is implemented as a code block defining the AUCMEDI pipeline.</li> </ul> <p>Info</p> AutoML Mode Argparse Code Block (Pipeline) <code>training</code> CLI - Training Block - Train <code>prediction</code> CLI - Prediction Block - Predict <code>evaluation</code> CLI - Evaluation Block - Evaluate <p>More information can be found in the docs: Documentation - AutoML</p>"},{"location":"reference/automl/block_eval/","title":"Block eval","text":""},{"location":"reference/automl/block_eval/#aucmedi.automl.block_eval.block_evaluate","title":"<code>block_evaluate(config)</code>","text":"<p>Internal code block for AutoML evaluation.</p> <p>This function is called by the Command-Line-Interface (CLI) of AUCMEDI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing all required                                 parameters for performing an AutoML evaluation.</p> required <p>The following attributes are stored in the <code>config</code> dictionary:</p> <p>Attributes:</p> Name Type Description <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the ground truth images.</p> <code>path_gt</code> <code>str</code> <p>Path to the index/class annotation file if required. (only for 'csv' interface).</p> <code>path_pred</code> <code>str</code> <p>Path to the input file in which predicted csv file is stored.</p> <code>path_evaldir</code> <code>str</code> <p>Path to the directory in which evaluation figures and tables should be stored.</p> <code>ohe</code> <code>bool</code> <p>Boolean option whether annotation data is sparse categorical or one-hot encoded.</p> Source code in <code>aucmedi/automl/block_eval.py</code> <pre><code>def block_evaluate(config):\n    \"\"\" Internal code block for AutoML evaluation.\n\n    This function is called by the Command-Line-Interface (CLI) of AUCMEDI.\n\n    Args:\n        config (dict):                      Configuration dictionary containing all required\n                                            parameters for performing an AutoML evaluation.\n\n    The following attributes are stored in the `config` dictionary:\n\n    Attributes:\n        path_imagedir (str):                Path to the directory containing the ground truth images.\n        path_gt (str):                      Path to the index/class annotation file if required. (only for 'csv' interface).\n        path_pred (str):                    Path to the input file in which predicted csv file is stored.\n        path_evaldir (str):                 Path to the directory in which evaluation figures and tables should be stored.\n        ohe (bool):                         Boolean option whether annotation data is sparse categorical or one-hot encoded.\n    \"\"\"\n    # Obtain interface\n    if config[\"path_gt\"] is None : config[\"interface\"] = \"directory\"\n    else : config[\"interface\"] = \"csv\"\n    # Peak into the dataset via the input interface\n    ds = input_interface(config[\"interface\"],\n                         config[\"path_imagedir\"],\n                         path_data=config[\"path_gt\"],\n                         training=True,\n                         ohe=config[\"ohe\"],\n                         image_format=None)\n    (index_list, class_ohe, class_n, class_names, image_format) = ds\n\n    # Create output directory\n    if not os.path.exists(config[\"path_evaldir\"]):\n        os.mkdir(config[\"path_evaldir\"])\n\n    # Read prediction csv\n    df_pred = pd.read_csv(config[\"path_pred\"])\n\n    # Create ground truth pandas dataframe\n    df_index = pd.DataFrame(data={\"SAMPLE\": index_list})\n    df_gt_data = pd.DataFrame(data=class_ohe, columns=class_names)\n    df_gt = pd.concat([df_index, df_gt_data], axis=1, sort=False)\n\n\n    # Verify - maybe there is a file path encoded in the index?\n    if os.path.sep in df_gt.iloc[0,0]:\n        samples_split = df_gt[\"SAMPLE\"].str.split(pat=os.path.sep,\n                                                  expand=False)\n        df_gt[\"SAMPLE\"] = samples_split.str[-1]\n    # Verify - maybe the image format is present in the index?\n    if image_format is None and bool(re.fullmatch(r\"^.*\\.[A-Za-z]+$\",\n                                                  df_gt.iloc[0,0])):\n        samples_split = df_gt[\"SAMPLE\"].str.split(pat=\".\",\n                                                  expand=False)\n        df_gt[\"SAMPLE\"] = samples_split.str[:-1].str.join(\".\")\n\n    # Merge dataframes to verify correct order\n    df_merged = df_pred.merge(df_gt, on=\"SAMPLE\", suffixes=(\"_pd\", \"_gt\"))\n\n    # Extract pd and gt again to NumPy\n    data_pd = df_merged.iloc[:, 1:(class_n+1)].to_numpy()\n    data_gt = df_merged.iloc[:, (class_n+1):].to_numpy()\n\n    # Identify task (multi-class vs multi-label)\n    if np.sum(data_pd) &gt; (class_ohe.shape[0] + 1.5) : multi_label = True\n    else : multi_label = False\n\n    # Evaluate performance via AUCMEDI evaluation submodule\n    evaluate_performance(data_pd, data_gt,\n                         out_path=config[\"path_evaldir\"],\n                         class_names=class_names,\n                         multi_label=multi_label,\n                         metrics_threshold=0.5,\n                         suffix=None,\n                         store_csv=True,\n                         plot_barplot=True,\n                         plot_confusion_matrix=True,\n                         plot_roc_curve=True)\n</code></pre>"},{"location":"reference/automl/block_pred/","title":"Block pred","text":""},{"location":"reference/automl/block_pred/#aucmedi.automl.block_pred.block_predict","title":"<code>block_predict(config)</code>","text":"<p>Internal code block for AutoML inference.</p> <p>This function is called by the Command-Line-Interface (CLI) of AUCMEDI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing all required                                 parameters for performing an AutoML inference.</p> required <p>The following attributes are stored in the <code>config</code> dictionary:</p> <p>Attributes:</p> Name Type Description <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images for prediction.</p> <code>path_modeldir</code> <code>str</code> <p>Path to the model directory in which fitted model weights and metadata are stored.</p> <code>path_pred</code> <code>str</code> <p>Path to the output file in which predicted csv file should be stored.</p> <code>xai_method</code> <code>str or None</code> <p>Key for XAI method.</p> <code>xai_directory</code> <code>str or None</code> <p>Path to the output directory in which predicted image xai heatmaps should be stored.</p> <code>batch_size</code> <code>int</code> <p>Number of samples inside a single batch.</p> <code>workers</code> <code>int</code> <p>Number of workers/threads which preprocess batches during runtime.</p> Source code in <code>aucmedi/automl/block_pred.py</code> <pre><code>def block_predict(config):\n    \"\"\" Internal code block for AutoML inference.\n\n    This function is called by the Command-Line-Interface (CLI) of AUCMEDI.\n\n    Args:\n        config (dict):                      Configuration dictionary containing all required\n                                            parameters for performing an AutoML inference.\n\n    The following attributes are stored in the `config` dictionary:\n\n    Attributes:\n        path_imagedir (str):                Path to the directory containing the images for prediction.\n        path_modeldir (str):                Path to the model directory in which fitted model weights and metadata are stored.\n        path_pred (str):                    Path to the output file in which predicted csv file should be stored.\n        xai_method (str or None):           Key for XAI method.\n        xai_directory (str or None):        Path to the output directory in which predicted image xai heatmaps should be stored.\n        batch_size (int):                   Number of samples inside a single batch.\n        workers (int):                      Number of workers/threads which preprocess batches during runtime.\n    \"\"\"\n    # Peak into the dataset via the input interface\n    ds = input_interface(\"directory\",\n                         config[\"path_imagedir\"],\n                         path_data=None,\n                         training=False,\n                         ohe=False,\n                         image_format=None)\n    (index_list, _, _, _, image_format) = ds\n\n    # Verify existence of input directory\n    if not os.path.exists(config[\"path_modeldir\"]):\n        raise FileNotFoundError(config[\"path_modeldir\"])\n\n    # Load metadata from training\n    path_meta = os.path.join(config[\"path_modeldir\"], \"meta.training.json\")\n    with open(path_meta, \"r\") as json_file:\n        meta_training = json.load(json_file)\n\n    # Define neural network parameters\n    nn_paras = {\"n_labels\": 1,                                  # placeholder\n                \"channels\": 1,                                  # placeholder\n    }\n    # Select input shape for 3D\n    if meta_training[\"three_dim\"]:\n        nn_paras[\"input_shape\"] = tuple(meta_training[\"shape_3D\"])\n\n    # Subfunctions\n    sf_list = []\n    if meta_training[\"three_dim\"]:\n        sf_norm = Standardize(mode=\"grayscale\")\n        sf_pad = Padding(mode=\"constant\", shape=meta_training[\"shape_3D\"])\n        sf_crop = Crop(shape=meta_training[\"shape_3D\"], mode=\"random\")\n        sf_chromer = Chromer(target=\"rgb\")\n        sf_list.extend([sf_norm, sf_pad, sf_crop, sf_chromer])\n\n    # Define parameters for DataGenerator\n    paras_datagen = {\n        \"path_imagedir\": config[\"path_imagedir\"],\n        \"batch_size\": config[\"batch_size\"],\n        \"img_aug\": None,\n        \"subfunctions\": sf_list,\n        \"prepare_images\": False,\n        \"sample_weights\": None,\n        \"seed\": None,\n        \"image_format\": image_format,\n        \"workers\": config[\"workers\"],\n        \"shuffle\": False,\n        \"grayscale\": False,\n    }\n    if not meta_training[\"three_dim\"] : paras_datagen[\"loader\"] = image_loader\n    else : paras_datagen[\"loader\"] = sitk_loader\n\n    # Apply MIC pipelines\n    if meta_training[\"analysis\"] == \"minimal\":\n        # Setup neural network\n        if not meta_training[\"three_dim\"]:\n            arch_dim = \"2D.\" + meta_training[\"architecture\"]\n        else : arch_dim = \"3D.\" + meta_training[\"architecture\"]\n        model = NeuralNetwork(architecture=arch_dim, **nn_paras)\n\n        # Build DataGenerator\n        pred_gen = DataGenerator(samples=index_list,\n                                 labels=None,\n                                 resize=model.meta_input,\n                                 standardize_mode=model.meta_standardize,\n                                 **paras_datagen)\n        # Load model\n        path_model = os.path.join(config[\"path_modeldir\"], \"model.last.keras\")\n        model.load(path_model)\n        # Start model inference\n        preds = model.predict(prediction_generator=pred_gen)\n    elif meta_training[\"analysis\"] == \"standard\":\n        # Setup neural network\n        if not meta_training[\"three_dim\"]:\n            arch_dim = \"2D.\" + meta_training[\"architecture\"]\n        else : arch_dim = \"3D.\" + meta_training[\"architecture\"]\n        model = NeuralNetwork(architecture=arch_dim, **nn_paras)\n\n        # Build DataGenerator\n        pred_gen = DataGenerator(samples=index_list,\n                                 labels=None,\n                                 resize=model.meta_input,\n                                 standardize_mode=model.meta_standardize,\n                                 **paras_datagen)\n        # Load model\n        path_model = os.path.join(config[\"path_modeldir\"],\n                                  \"model.best_loss.keras\")\n        model.load(path_model)\n        # Start model inference via Augmenting\n        preds = predict_augmenting(model, pred_gen)\n    else:\n        # Build multi-model list\n        model_list = []\n        for arch in meta_training[\"architecture\"]:\n            if not meta_training[\"three_dim\"] : arch_dim = \"2D.\" + arch\n            else : arch_dim = \"3D.\" + arch\n            model_part = NeuralNetwork(architecture=arch_dim, **nn_paras)\n            model_list.append(model_part)\n        el = Composite(model_list, metalearner=meta_training[\"metalearner\"],\n                       k_fold=len(meta_training[\"architecture\"]))\n\n        # Build DataGenerator\n        pred_gen = DataGenerator(samples=index_list,\n                                 labels=None,\n                                 resize=None,\n                                 standardize_mode=None,\n                                 **paras_datagen)\n        # Load composite model directory\n        el.load(config[\"path_modeldir\"])\n        # Start model inference via ensemble learning\n        preds = el.predict(pred_gen)\n\n    # Create prediction dataset\n    df_index = pd.DataFrame(data={\"SAMPLE\": index_list})\n    df_pd = pd.DataFrame(data=preds, columns=meta_training[\"class_names\"])\n    df_merged = pd.concat([df_index, df_pd], axis=1, sort=False)\n    df_merged.sort_values(by=[\"SAMPLE\"], inplace=True)\n    # Store predictions to disk\n    df_merged.to_csv(config[\"path_pred\"], index=False)\n\n    # Create XAI heatmaps\n    if config[\"xai_method\"] is not None and config[\"xai_directory\"] is not None:\n        if meta_training[\"analysis\"] == \"advanced\":\n            raise ValueError(\"XAI is only supported for single model pipelines!\")\n        # Create xai output directory\n        if not os.path.exists(config[\"xai_directory\"]):\n            os.mkdir(config[\"xai_directory\"])\n        # Run XAI decoder\n        xai_decoder(pred_gen, model, preds=preds, method=config[\"xai_method\"],\n                    layerName=None, alpha=0.4, out_path=config[\"xai_directory\"])\n</code></pre>"},{"location":"reference/automl/block_train/","title":"Block train","text":""},{"location":"reference/automl/block_train/#aucmedi.automl.block_train.block_train","title":"<code>block_train(config)</code>","text":"<p>Internal code block for AutoML training.</p> <p>This function is called by the Command-Line-Interface (CLI) of AUCMEDI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing all required                                 parameters for performing an AutoML training.</p> required <p>The following attributes are stored in the <code>config</code> dictionary:</p> <p>Attributes:</p> Name Type Description <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> <code>path_modeldir</code> <code>str</code> <p>Path to the output directory in which fitted models and metadata are stored.</p> <code>path_gt</code> <code>str</code> <p>Path to the index/class annotation file if required. (only for 'csv' interface).</p> <code>analysis</code> <code>str</code> <p>Analysis mode for the AutoML training. Options: <code>[\"minimal\", \"standard\", \"advanced\"]</code>.</p> <code>ohe</code> <code>bool</code> <p>Boolean option whether annotation data is sparse categorical or one-hot encoded.</p> <code>three_dim</code> <code>bool</code> <p>Boolean, whether data is 2D or 3D.</p> <code>shape_3D</code> <code>tuple of int</code> <p>Desired input shape of 3D volume for architecture (will be cropped).</p> <code>epochs</code> <code>int</code> <p>Number of epochs. A single epoch is defined as one iteration through                                 the complete data set.</p> <code>batch_size</code> <code>int</code> <p>Number of samples inside a single batch.</p> <code>workers</code> <code>int</code> <p>Number of workers/threads which preprocess batches during runtime.</p> <code>metalearner</code> <code>str</code> <p>Key for Metalearner or Aggregate function.</p> <code>architecture</code> <code>str or list of str</code> <p>Key (str) of a neural network model Architecture class instance.</p> Source code in <code>aucmedi/automl/block_train.py</code> <pre><code>def block_train(config):\n    \"\"\" Internal code block for AutoML training.\n\n    This function is called by the Command-Line-Interface (CLI) of AUCMEDI.\n\n    Args:\n        config (dict):                      Configuration dictionary containing all required\n                                            parameters for performing an AutoML training.\n\n    The following attributes are stored in the `config` dictionary:\n\n    Attributes:\n        path_imagedir (str):                Path to the directory containing the images.\n        path_modeldir (str):                Path to the output directory in which fitted models and metadata are stored.\n        path_gt (str):                      Path to the index/class annotation file if required. (only for 'csv' interface).\n        analysis (str):                     Analysis mode for the AutoML training. Options: `[\"minimal\", \"standard\", \"advanced\"]`.\n        ohe (bool):                         Boolean option whether annotation data is sparse categorical or one-hot encoded.\n        three_dim (bool):                   Boolean, whether data is 2D or 3D.\n        shape_3D (tuple of int):            Desired input shape of 3D volume for architecture (will be cropped).\n        epochs (int):                       Number of epochs. A single epoch is defined as one iteration through\n                                            the complete data set.\n        batch_size (int):                   Number of samples inside a single batch.\n        workers (int):                      Number of workers/threads which preprocess batches during runtime.\n        metalearner (str):                  Key for Metalearner or Aggregate function.\n        architecture (str or list of str):  Key (str) of a neural network model Architecture class instance.\n    \"\"\"\n    # Obtain interface\n    if config[\"path_gt\"] is None : config[\"interface\"] = \"directory\"\n    else : config[\"interface\"] = \"csv\"\n    # Peak into the dataset via the input interface\n    ds = input_interface(config[\"interface\"],\n                         config[\"path_imagedir\"],\n                         path_data=config[\"path_gt\"],\n                         training=True,\n                         ohe=config[\"ohe\"],\n                         image_format=None)\n    (index_list, class_ohe, class_n, class_names, image_format) = ds\n\n    # Create output directory\n    if not os.path.exists(config[\"path_modeldir\"]):\n        os.mkdir(config[\"path_modeldir\"])\n\n    # Identify task (multi-class vs multi-label)\n    if np.sum(class_ohe) &gt; class_ohe.shape[0] : config[\"multi_label\"] = True\n    else : config[\"multi_label\"] = False\n\n    # Sanity check on multi-label metalearner\n    multilabel_metalearner_supported = [\"mlp\", \"k_neighbors\", \"random_forest\",\n                                        \"weighted_mean\", \"best_model\",\n                                        \"decision_tree\", \"mean\", \"median\"]\n    if config[\"multi_label\"] and config[\"analysis\"] == \"advanced\" and \\\n       config[\"metalearner\"] not in multilabel_metalearner_supported:\n        raise ValueError(\"Non-compatible metalearner selected for multi-label\"\\\n                         + \" classification. Supported metalearner:\",\n                          multilabel_metalearner_supported)\n\n    # Store meta information\n    config[\"class_names\"] = class_names\n    path_meta = os.path.join(config[\"path_modeldir\"], \"meta.training.json\")\n    with open(path_meta, \"w\") as json_io:\n        json.dump(config, json_io)\n\n    # Define Callbacks\n    callbacks = []\n    if config[\"analysis\"] == \"standard\":\n        cb_loss = ModelCheckpoint(os.path.join(config[\"path_modeldir\"],\n                                               \"model.best_loss.keras\"),\n                                  monitor=\"val_loss\", verbose=1,\n                                  save_best_only=True)\n        callbacks.append(cb_loss)\n    if config[\"analysis\"] in [\"minimal\", \"standard\"]:\n        cb_cl = CSVLogger(os.path.join(config[\"path_modeldir\"],\n                                       \"logs.training.csv\"),\n                          separator=',', append=True)\n        callbacks.append(cb_cl)\n    if config[\"analysis\"] != \"minimal\":\n        cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5,\n                              verbose=1, mode='min', min_lr=1e-7)\n        cb_es = EarlyStopping(monitor='val_loss', patience=12, verbose=1)\n        callbacks.extend([cb_lr, cb_es])\n\n    # Initialize loss function for multi-class\n    if not config[\"multi_label\"]:\n        # Compute class weights\n        class_weights, _ = compute_class_weights(ohe_array=class_ohe)\n        # Initialize focal loss\n        loss = categorical_focal_loss(class_weights)\n    # Initialize loss function for multi-label\n    else:\n        # Compute class weights\n        class_weights = compute_multilabel_weights(ohe_array=class_ohe)\n        # Initialize focal loss\n        loss = multilabel_focal_loss(class_weights)\n\n    # Define neural network parameters\n    nn_paras = {\"n_labels\": class_n,\n                \"channels\": 3,\n                \"loss\": loss,\n                \"metrics\": [AUC(100)],\n                \"pretrained_weights\": True,\n    }\n    # Select input shape for 3D\n    if config[\"three_dim\"] : nn_paras[\"input_shape\"] = config[\"shape_3D\"]\n    # Select task type\n    if config[\"multi_label\"] : nn_paras[\"activation_output\"] = \"sigmoid\"\n    else : nn_paras[\"activation_output\"] = \"softmax\"\n\n    # Initialize Augmentation for 2D image data\n    if not config[\"three_dim\"]:\n        data_aug = ImageAugmentation(flip=True, rotate=True, scale=False,\n                                     brightness=True, contrast=True,\n                                     saturation=False, hue=False, crop=False,\n                                     grid_distortion=False, compression=False,\n                                     gamma=True, gaussian_noise=False,\n                                     gaussian_blur=False, downscaling=False,\n                                     elastic_transform=True)\n    # Initialize Augmentation for 3D volume data\n    elif config[\"three_dim\"]:\n        data_aug = BatchgeneratorsAugmentation(image_shape=config[\"shape_3D\"],\n                        mirror=True, rotate=True, scale=True,\n                        elastic_transform=True, gaussian_noise=False,\n                        brightness=False, contrast=False, gamma=True)\n    else : data_aug = None\n\n    # Subfunctions\n    sf_list = []\n    if config[\"three_dim\"]:\n        sf_norm = Standardize(mode=\"grayscale\")\n        sf_pad = Padding(mode=\"constant\", shape=config[\"shape_3D\"])\n        sf_crop = Crop(shape=config[\"shape_3D\"], mode=\"random\")\n        sf_chromer = Chromer(target=\"rgb\")\n        sf_list.extend([sf_norm, sf_pad, sf_crop, sf_chromer])\n\n    # Define parameters for DataGenerator\n    paras_datagen = {\n        \"path_imagedir\": config[\"path_imagedir\"],\n        \"batch_size\": config[\"batch_size\"],\n        \"img_aug\": data_aug,\n        \"subfunctions\": sf_list,\n        \"prepare_images\": False,\n        \"sample_weights\": None,\n        \"seed\": None,\n        \"image_format\": image_format,\n        \"workers\": config[\"workers\"],\n    }\n    if not config[\"three_dim\"] : paras_datagen[\"loader\"] = image_loader\n    else : paras_datagen[\"loader\"] = sitk_loader\n\n    # Gather training parameters\n    paras_train = {\n        \"epochs\": config[\"epochs\"],\n        \"iterations\": None,\n        \"callbacks\": callbacks,\n        \"class_weights\": None,\n        \"transfer_learning\": True,\n    }\n\n    # Apply MIC pipelines\n    if config[\"analysis\"] == \"minimal\":\n        # Setup neural network\n        if not config[\"three_dim\"] : arch_dim = \"2D.\" + config[\"architecture\"]\n        else : arch_dim = \"3D.\" + config[\"architecture\"]\n        model = NeuralNetwork(architecture=arch_dim, **nn_paras)\n\n        # Build DataGenerator\n        train_gen = DataGenerator(samples=index_list,\n                                  labels=class_ohe,\n                                  shuffle=True,\n                                  resize=model.meta_input,\n                                  standardize_mode=model.meta_standardize,\n                                  **paras_datagen)\n\n        # Start model training\n        hist = model.train(training_generator=train_gen, **paras_train)\n        # Store model\n        path_model = os.path.join(config[\"path_modeldir\"], \"model.last.keras\")\n        model.dump(path_model)\n    elif config[\"analysis\"] == \"standard\":\n        # Setup neural network\n        if not config[\"three_dim\"] : arch_dim = \"2D.\" + config[\"architecture\"]\n        else : arch_dim = \"3D.\" + config[\"architecture\"]\n        model = NeuralNetwork(architecture=arch_dim, **nn_paras)\n\n        # Apply percentage split sampling\n        ps_sampling = sampling_split(index_list, class_ohe,\n                                     sampling=[0.85, 0.15],\n                                     stratified=True, iterative=True,\n                                     seed=0)\n\n        # Build DataGenerator\n        train_gen = DataGenerator(samples=ps_sampling[0][0],\n                                  labels=ps_sampling[0][1],\n                                  shuffle=True,\n                                  resize=model.meta_input,\n                                  standardize_mode=model.meta_standardize,\n                                  **paras_datagen)\n        val_gen = DataGenerator(samples=ps_sampling[1][0],\n                                labels=ps_sampling[1][1],\n                                shuffle=False,\n                                resize=model.meta_input,\n                                standardize_mode=model.meta_standardize,\n                                **paras_datagen)\n\n        # Start model training\n        hist = model.train(training_generator=train_gen,\n                           validation_generator=val_gen,\n                           **paras_train)\n        # Store model\n        path_model = os.path.join(config[\"path_modeldir\"], \"model.last.keras\")\n        model.dump(path_model)\n    else:\n        # Sanity check of architecutre config\n        if not isinstance(config[\"architecture\"], list):\n            raise ValueError(\"key 'architecture' in config has to be a list \" \\\n                             + \"if 'advanced' was selected as analysis.\")\n        # Build multi-model list\n        model_list = []\n        for arch in config[\"architecture\"]:\n            if not config[\"three_dim\"] : arch_dim = \"2D.\" + arch\n            else : arch_dim = \"3D.\" + arch\n            model_part = NeuralNetwork(architecture=arch_dim, **nn_paras)\n            model_list.append(model_part)\n        el = Composite(model_list, metalearner=config[\"metalearner\"],\n                       k_fold=len(config[\"architecture\"]))\n\n        # Build DataGenerator\n        train_gen = DataGenerator(samples=index_list,\n                                  labels=class_ohe,\n                                  shuffle=True,\n                                  resize=None,\n                                  standardize_mode=None,\n                                  **paras_datagen)\n        # Start model training\n        hist = el.train(training_generator=train_gen, **paras_train)\n        # Store model directory\n        el.dump(config[\"path_modeldir\"])\n\n    # Plot fitting history\n    evaluate_fitting(train_history=hist, out_path=config[\"path_modeldir\"])\n</code></pre>"},{"location":"reference/automl/cli/","title":"Cli","text":"<p>Argparse for the AutoML Command Line Interface of aucmedi.automl.main.</p> <p>The parameters are summarized in the docs: Documentation - AutoML - Parameters</p>"},{"location":"reference/automl/cli/#aucmedi.automl.cli.cli_core","title":"<code>cli_core()</code>","text":"<p>Internal function for Command-Line-Interface (CLI) setup. </p> Source code in <code>aucmedi/automl/cli.py</code> <pre><code>def cli_core():\n    \"\"\" Internal function for Command-Line-Interface (CLI) setup. \"\"\"\n    # Set description for cli core\n    desc = \"\"\" AutoML command-line interface for AUCMEDI: a framework for\n               Automated Classification of Medical Images \"\"\"\n    # Setup ArgumentParser interface\n    parser = argparse.ArgumentParser(prog=\"aucmedi\", description=desc)\n    # Add optional core arguments\n    version = pkg_resources.require(\"aucmedi\")[0].version\n    parser.add_argument(\"-v\", \"--version\", action=\"version\",\n                        version='%(prog)s_v' + version)\n\n    # Add subparser interface\n    subparsers = parser.add_subparsers(title=\"Application Modes\",\n                                       dest=\"hub\")\n    # Return parsers\n    return parser, subparsers\n</code></pre>"},{"location":"reference/automl/cli/#aucmedi.automl.cli.cli_evaluation","title":"<code>cli_evaluation(subparsers)</code>","text":"<p>Parameter overview for the evaluation process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>training</code> Path to the directory containing the ground truth images. I/O <code>--path_gt</code> str <code>None</code> Path to the index/class annotation CSV file (only required for defining the ground truth via 'csv' instead of 'directory' interface). I/O <code>--ohe</code> bool <code>False</code> Boolean option whether annotation data is sparse categorical or one-hot encoded. I/O <code>--path_pred</code> str <code>preds.csv</code> Path to the input file in which predicted csv file is stored. I/O <code>--path_evaldir</code> str <code>evaluation</code> Path to the directory in which evaluation figures and tables should be stored. Other <code>--help</code> bool <code>False</code> show this help message and exit. Source code in <code>aucmedi/automl/cli.py</code> <pre><code>def cli_evaluation(subparsers):\n    \"\"\" Parameter overview for the evaluation process.\n\n    | Category      | Argument               | Type       | Default        | Description |\n    | :------------ | :--------------------- | :--------- | :------------- | :---------- |\n    | I/O           | `--path_imagedir`      | str        | `training`     | Path to the directory containing the ground truth images. |\n    | I/O           | `--path_gt`            | str        | `None`         | Path to the index/class annotation CSV file (only required for defining the ground truth via 'csv' instead of 'directory' interface). |\n    | I/O           | `--ohe`                | bool       | `False`        | Boolean option whether annotation data is sparse categorical or one-hot encoded. |\n    | I/O           | `--path_pred`          | str        | `preds.csv`    | Path to the input file in which predicted csv file is stored. |\n    | I/O           | `--path_evaldir`       | str        | `evaluation`   | Path to the directory in which evaluation figures and tables should be stored. |\n    | Other         | `--help`               | bool       | `False`        | show this help message and exit. |\n    \"\"\"\n    # Set description for cli evaluation\n    desc = \"\"\" Pipeline hub for Evaluation via AUCMEDI AutoML \"\"\"\n    # Setup SubParser\n    parser_evaluate = subparsers.add_parser(\"evaluation\",\n                                            help=desc,\n                                            add_help=False)\n\n    # Add IO arguments\n    od = parser_evaluate.add_argument_group(\"Arguments - I/O\")\n    od.add_argument(\"--path_imagedir\",\n                    type=str,\n                    required=False,\n                    default=\"training\",\n                    help=\"Path to the directory containing the ground truth\" + \\\n                         \" images \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_gt\",\n                    type=str,\n                    required=False,\n                    help=\"Path to the index/class annotation CSV file \" + \\\n                         \"(only required for defining the ground truth via \" + \\\n                         \"'csv' instead of 'directory' interface)\",\n                    )\n    od.add_argument(\"--ohe\",\n                    action=\"store_true\",\n                    required=False,\n                    default=False,\n                    help=\"Boolean option whether annotation data is sparse \" + \\\n                         \"categorical or one-hot encoded \" + \\\n                         \"(only required for interface 'csv' and multi-\" + \\\n                         \"label data, \" + \\\n                         \"default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_pred\",\n                    type=str,\n                    required=False,\n                    default=\"preds.csv\",\n                    help=\"Path to the output file in which predicted csv \" + \\\n                         \"file are stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_evaldir\",\n                    type=str,\n                    required=False,\n                    default=\"evaluation\",\n                    help=\"Path to the directory in which evaluation \" + \\\n                         \"figures and tables should be stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n\n    # Add other arguments\n    oo = parser_evaluate.add_argument_group(\"Arguments - Other\")\n    oo.add_argument(\"-h\",\n                    \"--help\",\n                    action=\"help\",\n                    help=\"show this help message and exit\")\n</code></pre>"},{"location":"reference/automl/cli/#aucmedi.automl.cli.cli_prediction","title":"<code>cli_prediction(subparsers)</code>","text":"<p>Parameter overview for the prediction process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>test</code> Path to the directory containing the images. I/O <code>--path_modeldir</code> str <code>model</code> Path to the output directory in which fitted models and metadata are stored. I/O <code>--path_pred</code> str <code>preds.csv</code> Path to the output file in which predicted csv file should be stored. Configuration <code>--xai_method</code> str <code>None</code> Key for XAI method. Configuration <code>--xai_directory</code> str <code>xai</code> Path to the output directory in which predicted image xai heatmaps should be stored. Configuration <code>--batch_size</code> int <code>24</code> Number of samples inside a single batch. Configuration <code>--workers</code> int <code>1</code> Number of workers/threads which preprocess batches during runtime. Other <code>--help</code> bool <code>False</code> show this help message and exit. List of XAI Methods <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use XAI methods: aucmedi.xai.methods</p> Source code in <code>aucmedi/automl/cli.py</code> <pre><code>def cli_prediction(subparsers):\n    \"\"\" Parameter overview for the prediction process.\n\n    | Category      | Argument               | Type       | Default        | Description |\n    | :------------ | :--------------------- | :--------- | :------------- | :---------- |\n    | I/O           | `--path_imagedir`      | str        | `test`         | Path to the directory containing the images. |\n    | I/O           | `--path_modeldir`      | str        | `model`        | Path to the output directory in which fitted models and metadata are stored. |\n    | I/O           | `--path_pred`          | str        | `preds.csv`    | Path to the output file in which predicted csv file should be stored. |\n    | Configuration | `--xai_method`         | str        | `None`         | Key for XAI method.  |\n    | Configuration | `--xai_directory`      | str        | `xai`          | Path to the output directory in which predicted image xai heatmaps should be stored. |\n    | Configuration | `--batch_size`         | int        | `24`           | Number of samples inside a single batch. |\n    | Configuration | `--workers`            | int        | `1`            | Number of workers/threads which preprocess batches during runtime. |\n    | Other         | `--help`               | bool       | `False`        | show this help message and exit. |\n\n    ??? info \"List of XAI Methods\"\n        AUCMEDI provides a large library of state-of-the-art and ready-to-use XAI methods:\n        [aucmedi.xai.methods][]\n    \"\"\"\n    # Set description for cli prediction\n    desc = \"\"\" Pipeline hub for Inference via AUCMEDI AutoML \"\"\"\n    # Setup SubParser\n    parser_predict = subparsers.add_parser(\"prediction\",\n                                           help=desc,\n                                           add_help=False)\n\n    # Add IO arguments\n    od = parser_predict.add_argument_group(\"Arguments - I/O\")\n    od.add_argument(\"--path_imagedir\",\n                    type=str,\n                    required=False,\n                    default=\"test\",\n                    help=\"Path to the directory containing the images \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_modeldir\",\n                    type=str,\n                    required=False,\n                    default=\"model\",\n                    help=\"Path to the model directory in which fitted \" + \\\n                         \"model weights and metadata are stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_pred\",\n                    type=str,\n                    required=False,\n                    default=\"preds.csv\",\n                    help=\"Path to the output file in which predicted csv \" + \\\n                         \"file should be stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n\n    # Add configuration arguments\n    oc = parser_predict.add_argument_group(\"Arguments - Configuration\")\n    oc.add_argument(\"--xai_method\",\n                    type=str,\n                    required=False,\n                    help=\"Key for XAI method \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--xai_directory\",\n                    type=str,\n                    required=False,\n                    default=\"xai\",\n                    help=\"Path to the output directory in which predicted \" + \\\n                         \"image xai heatmaps should be stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--batch_size\",\n                    type=int,\n                    required=False,\n                    default=12,\n                    help=\"Number of samples inside a single batch \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--workers\",\n                    type=int,\n                    required=False,\n                    default=1,\n                    help=\"Number of workers/threads which preprocess \" + \\\n                         \"batches during runtime \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n\n    # Add other arguments\n    oo = parser_predict.add_argument_group(\"Arguments - Other\")\n    oo.add_argument(\"-h\",\n                    \"--help\",\n                    action=\"help\",\n                    help=\"show this help message and exit\")\n</code></pre>"},{"location":"reference/automl/cli/#aucmedi.automl.cli.cli_training","title":"<code>cli_training(subparsers)</code>","text":"<p>Parameter overview for the training process.</p> Category Argument Type Default Description I/O <code>--path_imagedir</code> str <code>training</code> Path to the directory containing the images. I/O <code>--path_modeldir</code> str <code>model</code> Path to the output directory in which fitted models and metadata are stored. I/O <code>--path_gt</code> str <code>None</code> Path to the index/class annotation file if required. (only for 'csv' interface). I/O <code>--ohe</code> bool <code>False</code> Boolean option whether annotation data is sparse categorical or one-hot encoded. Configuration <code>--analysis</code> str <code>standard</code> Analysis mode for the AutoML training. Options: <code>[\"minimal\", \"standard\", \"advanced\"]</code>. Configuration <code>--three_dim</code> bool <code>False</code> Boolean, whether data is 2D or 3D. Configuration <code>--shape_3D</code> str <code>128x128x128</code> Desired input shape of 3D volume for architecture (will be cropped into, format: <code>1x2x3</code>). Configuration <code>--epochs</code> int <code>500</code> Number of epochs. A single epoch is defined as one iteration through the complete data set. Configuration <code>--batch_size</code> int <code>24</code> Number of samples inside a single batch. Configuration <code>--workers</code> int <code>1</code> Number of workers/threads which preprocess batches during runtime. Configuration <code>--metalearner</code> str <code>mean</code> Key for Metalearner or Aggregate function. Configuration <code>--architecture</code> str <code>DenseNet121</code> Key of single or multiple Architectures (only supported for 'analysis=advanced', format: 'KEY' or 'KEY,KEY,KEY). Other <code>--help</code> bool <code>False</code> show this help message and exit. List of Architectures <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.</p> <ul> <li>2D Architectures: aucmedi.neural_network.architectures.image</li> <li>3D Architectures: aucmedi.neural_network.architectures.volume</li> </ul> List of Metalearner <ul> <li>Homogeneous pooling functions: Aggregate</li> <li>Heterogeneous pooling functions: Metalearner</li> </ul> Source code in <code>aucmedi/automl/cli.py</code> <pre><code>def cli_training(subparsers):\n    \"\"\" Parameter overview for the training process.\n\n    | Category      | Argument               | Type       | Default        | Description |\n    | :------------ | :--------------------- | :--------- | :------------- | :---------- |\n    | I/O           | `--path_imagedir`      | str        | `training`     | Path to the directory containing the images. |\n    | I/O           | `--path_modeldir`      | str        | `model`        | Path to the output directory in which fitted models and metadata are stored. |\n    | I/O           | `--path_gt`            | str        | `None`         | Path to the index/class annotation file if required. (only for 'csv' interface). |\n    | I/O           | `--ohe`                | bool       | `False`        | Boolean option whether annotation data is sparse categorical or one-hot encoded. |\n    | Configuration | `--analysis`           | str        | `standard`     | Analysis mode for the AutoML training. Options: `[\"minimal\", \"standard\", \"advanced\"]`. |\n    | Configuration | `--three_dim`          | bool       | `False`        | Boolean, whether data is 2D or 3D. |\n    | Configuration | `--shape_3D`           | str        | `128x128x128`  | Desired input shape of 3D volume for architecture (will be cropped into, format: `1x2x3`). |\n    | Configuration | `--epochs`             | int        | `500`          | Number of epochs. A single epoch is defined as one iteration through the complete data set. |\n    | Configuration | `--batch_size`         | int        | `24`           | Number of samples inside a single batch. |\n    | Configuration | `--workers`            | int        | `1`            | Number of workers/threads which preprocess batches during runtime. |\n    | Configuration | `--metalearner`        | str        | `mean`         | Key for Metalearner or Aggregate function. |\n    | Configuration | `--architecture`       | str        | `DenseNet121`  | Key of single or multiple Architectures (only supported for 'analysis=advanced', format: 'KEY' or 'KEY,KEY,KEY). |\n    | Other         | `--help`               | bool       | `False`        | show this help message and exit. |\n\n    ??? info \"List of Architectures\"\n        AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.\n\n        - 2D Architectures: [aucmedi.neural_network.architectures.image][]\n        - 3D Architectures: [aucmedi.neural_network.architectures.volume][]\n\n    ??? info \"List of Metalearner\"\n        - Homogeneous pooling functions: [Aggregate][aucmedi.ensemble.aggregate]\n        - Heterogeneous pooling functions: [Metalearner][aucmedi.ensemble.metalearner]\n    \"\"\"\n    # Set description for cli training\n    desc = \"\"\" Pipeline hub for Training via AUCMEDI AutoML \"\"\"\n    # Setup SubParser\n    parser_train = subparsers.add_parser(\"training\", help=desc, add_help=False)\n\n    # Add IO arguments\n    od = parser_train.add_argument_group(\"Arguments - I/O\")\n    od.add_argument(\"--path_imagedir\",\n                    type=str,\n                    required=False,\n                    default=\"training\",\n                    help=\"Path to the directory containing the images \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_modeldir\",\n                    type=str,\n                    required=False,\n                    default=\"model\",\n                    help=\"Path to the output directory in which fitted \" + \\\n                         \"models and metadata are stored \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    od.add_argument(\"--path_gt\",\n                    type=str,\n                    required=False,\n                    help=\"Path to the index/class annotation CSV file \" + \\\n                         \"(only required for defining the ground truth via \" + \\\n                         \"'csv' instead of 'directory' interface)\",\n                    )\n\n    od.add_argument(\"--ohe\",\n                    action=\"store_true\",\n                    required=False,\n                    default=False,\n                    help=\"Boolean option whether annotation data is sparse \" + \\\n                         \"categorical or one-hot encoded \" + \\\n                         \"(only required for interface 'csv' and multi-\" + \\\n                         \"label data, \" + \\\n                         \"default: '%(default)s')\",\n                    )\n\n    # Add configuration arguments\n    oc = parser_train.add_argument_group(\"Arguments - Configuration\")\n    oc.add_argument(\"--analysis\",\n                    type=str,\n                    required=False,\n                    default=\"standard\",\n                    choices=[\"minimal\", \"standard\", \"advanced\"],\n                    help=\"Analysis mode for the AutoML training \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--three_dim\",\n                    action=\"store_true\",\n                    required=False,\n                    default=False,\n                    help=\"Boolean, whether imaging data is 2D or 3D \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--shape_3D\",\n                    type=str,\n                    required=False,\n                    default=\"128x128x128\",\n                    help=\"Desired input shape of 3D volume for architecture \"+ \\\n                         \"(will be cropped into, \" + \\\n                         \"format: '1x2x3', \" + \\\n                         \"default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--epochs\",\n                    type=int,\n                    required=False,\n                    default=500,\n                    help=\"Number of epochs. A single epoch is defined as \" + \\\n                         \"one iteration through the complete data set \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--batch_size\",\n                    type=int,\n                    required=False,\n                    default=24,\n                    help=\"Number of samples inside a single batch \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--workers\",\n                    type=int,\n                    required=False,\n                    default=1,\n                    help=\"Number of workers/threads which preprocess \" + \\\n                         \"batches during runtime \" + \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--metalearner\",\n                    type=str,\n                    required=False,\n                    default=\"mean\",\n                    help=\"Key for Metalearner or Aggregate function \"+ \\\n                         \"(default: '%(default)s')\",\n                    )\n    oc.add_argument(\"--architecture\",\n                    type=str,\n                    required=False,\n                    default=\"DenseNet121\",\n                    help=\"Key of single or multiple Architectures \" + \\\n                         \"(multiple Architectures are only supported for \" + \\\n                         \"'analysis=advanced', \" + \\\n                         \"format: 'KEY' or 'KEY,KEY,KEY', \" + \\\n                         \"default: '%(default)s')\",\n                    )\n\n    # Add other arguments\n    oo = parser_train.add_argument_group(\"Arguments - Other\")\n    oo.add_argument(\"-h\",\n                    \"--help\",\n                    action=\"help\",\n                    help=\"show this help message and exit\")\n</code></pre>"},{"location":"reference/automl/main/","title":"Main","text":"<p>Entry script (runner/main function) which will be called in AUCMEDI AutoML.</p> <p>The console entry <code>aucmedi</code> refers to <code>aucmedi.automl.main:main</code>.</p> <p>Executes AUCMEDI AutoML pipeline for training, prediction and evaluation.</p> <p>More information can be found in the docs: Documentation - AutoML</p>"},{"location":"reference/automl/parser_cli/","title":"Parser cli","text":""},{"location":"reference/automl/parser_cli/#aucmedi.automl.parser_cli.parse_cli","title":"<code>parse_cli(args)</code>","text":"<p>Internal function for parsing CLI arguments to a valid configuration dictionary.</p> Source code in <code>aucmedi/automl/parser_cli.py</code> <pre><code>def parse_cli(args):\n    \"\"\" Internal function for parsing CLI arguments to a valid configuration\n    dictionary.\n    \"\"\"\n    # Parse argument namespace to config dict\n    config = vars(args)\n\n    # Convert variables\n    if config[\"hub\"] == \"training\":\n        # Handle 3D shape - from str to tuple\n        config[\"shape_3D\"] = tuple(map(int, config[\"shape_3D\"].split(\"x\")))\n        # Handle architecture list\n        if \",\" in config[\"architecture\"]:\n            config[\"architecture\"] = config[\"architecture\"].split(\",\")\n\n    # Return valid configs\n    return config\n</code></pre>"},{"location":"reference/automl/parser_yaml/","title":"Parser yaml","text":""},{"location":"reference/automl/parser_yaml/#aucmedi.automl.parser_yaml.parse_yaml","title":"<code>parse_yaml(args)</code>","text":"<p>Internal function for parsing a YAML file to a valid configuration dictionary.</p> Source code in <code>aucmedi/automl/parser_yaml.py</code> <pre><code>def parse_yaml(args):\n    \"\"\" Internal function for parsing a YAML file to a valid configuration\n    dictionary.\n    \"\"\"\n    # Extract filepath to YAML configuration file\n\n    # verify existence\n\n    # read yaml file\n\n    # convert variables\n\n    # Return valid configs\n    pass\n</code></pre>"},{"location":"reference/data_processing/","title":"Data processing","text":"<p>The data processing is a important part of any AUCMEDI pipeline and represented by two core pillars.</p> <ul> <li>The input_interface function provides important information on the dataset.</li> <li>The DataGenerator class is a powerful interface handling all data processing in AUCMEDI.</li> </ul> Pillars of AUCMEDI <ul> <li>aucmedi.data_processing.io_data.input_interface</li> <li>aucmedi.data_processing.data_generator.DataGenerator</li> <li>aucmedi.neural_network.model.NeuralNetwork</li> </ul> <p>With an initialized Neural Network instance, it is possible to run training and predictions.</p> Example <pre><code># Import\nfrom aucmedi import *\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize model\nmodel = NeuralNetwork(n_labels=nclasses, channels=3, architecture=\"2D.ResNet50\")\n\n# Do some training\ndatagen_train = DataGenerator(samples[:100], \"images_dir/\", labels=class_ohe[:100], image_format=image_format,\n                              resize=model.meta_input, standardize_mode=model.meta_standardize)\nmodel.train(datagen_train, epochs=50)\n\n# Do some predictions\ndatagen_test = DataGenerator(samples[100:150], \"images_dir/\", labels=None, image_format=image_format,\n                             resize=model.meta_input, standardize_mode=model.meta_standardize)\npreds = model.predict(datagen_test)\n</code></pre>"},{"location":"reference/data_processing/data_generator/","title":"Data generator","text":""},{"location":"reference/data_processing/data_generator/#aucmedi.data_processing.data_generator.DataGenerator","title":"<code>DataGenerator</code>","text":"<p>         Bases: <code>Sequence</code></p> <p>Infinite Data Generator which automatically creates batches from a list of samples.</p> <p>The created batches are model ready. This generator can be supplied directly to a NeuralNetwork train() &amp; predict() function (also compatible to tensorflow.keras.model fit() &amp; predict() function).</p> <p>The DataGenerator is the second of the three pillars of AUCMEDI.</p> Pillars of AUCMEDI <ul> <li>aucmedi.data_processing.io_data.input_interface</li> <li>aucmedi.data_processing.data_generator.DataGenerator</li> <li>aucmedi.neural_network.model.NeuralNetwork</li> </ul> <p>The DataGenerator can be used for training, validation as well as for prediction.</p> Example <pre><code># Import\nfrom aucmedi import *\n\n# Initialize model\nmodel = NeuralNetwork(\n    n_labels=8,\n    channels=3,\n    architecture=\"2D.ResNet50\"\n)\n\n# Do some training\ndatagen_train = DataGenerator(\n    samples=samples[:100],\n    path_imagedir=\"images_dir/\",\n    image_format=image_format,\n    labels=class_ohe[:100],\n    resize=model.meta_input,\n    standardize_mode=model.meta_standardize\n)\n\nmodel.train(datagen_train, epochs=50)\n\n# Do some predictions\ndatagen_test = DataGenerator(\n    samples=samples[100:150],\n    path_imagedir=\"images_dir/\",\n    image_format=image_format,\n    labels=None,\n    resize=model.meta_input,\n    standardize_mode=model.meta_standardize\n)\n\npreds = model.predict(datagen_test)\n</code></pre> <p>It supports real-time batch generation as well as beforehand preprocessing of images, which are then temporarily stored on disk (requires enough disk space!).</p> <p>The resulting batches are created based the following pipeline:</p> <ol> <li>Image Loading</li> <li>Application of Subfunctions</li> <li>Resize image</li> <li>Application of Data Augmentation</li> <li>Standardize image</li> <li>Stacking processed images to a batch</li> </ol> Warning <p>When instantiating a <code>DataGenerator</code>, it is highly recommended, to pass the <code>image_format</code> parameter provided by the <code>input_interface()</code> and the <code>resize</code> &amp; <code>standardize_mode</code> parameters provided by the <code>NeuralNetwork</code> class attributes <code>meta_input</code> &amp; <code>meta_standardize</code>.</p> <p>It assures, that the samples contain the expected file extension, input shape and standardization.</p> Build on top of the library <p>Tensorflow.Keras Iterator: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/Iterator</p> Example: How to integrate metadata in AUCMEDI? <pre><code>from aucmedi import *\nimport numpy as np\n\nmy_metadata = np.random.rand(len(samples), 10)\n\nmy_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                          meta_variables=10)\n\nmy_dg = DataGenerator(samples, \"images_dir/\",\n                      labels=None, metadata=my_metadata,\n                      resize=my_model.meta_input,                  # (224,224)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Source code in <code>aucmedi/data_processing/data_generator.py</code> <pre><code>class DataGenerator(Sequence):\n    \"\"\" Infinite Data Generator which automatically creates batches from a list of samples.\n\n    The created batches are model ready. This generator can be supplied directly\n    to a [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork] train() &amp; predict()\n    function (also compatible to tensorflow.keras.model fit() &amp; predict() function).\n\n    The DataGenerator is the second of the three pillars of AUCMEDI.\n\n    ??? info \"Pillars of AUCMEDI\"\n        - [aucmedi.data_processing.io_data.input_interface][]\n        - [aucmedi.data_processing.data_generator.DataGenerator][]\n        - [aucmedi.neural_network.model.NeuralNetwork][]\n\n    The DataGenerator can be used for training, validation as well as for prediction.\n\n    ???+ example\n        ```python\n        # Import\n        from aucmedi import *\n\n        # Initialize model\n        model = NeuralNetwork(\n            n_labels=8,\n            channels=3,\n            architecture=\"2D.ResNet50\"\n        )\n\n        # Do some training\n        datagen_train = DataGenerator(\n            samples=samples[:100],\n            path_imagedir=\"images_dir/\",\n            image_format=image_format,\n            labels=class_ohe[:100],\n            resize=model.meta_input,\n            standardize_mode=model.meta_standardize\n        )\n\n        model.train(datagen_train, epochs=50)\n\n        # Do some predictions\n        datagen_test = DataGenerator(\n            samples=samples[100:150],\n            path_imagedir=\"images_dir/\",\n            image_format=image_format,\n            labels=None,\n            resize=model.meta_input,\n            standardize_mode=model.meta_standardize\n        )\n\n        preds = model.predict(datagen_test)\n        ```\n\n    It supports real-time batch generation as well as beforehand preprocessing of images,\n    which are then temporarily stored on disk (requires enough disk space!).\n\n    The resulting batches are created based the following pipeline:\n\n    1. Image Loading\n    2. Application of Subfunctions\n    3. Resize image\n    4. Application of Data Augmentation\n    5. Standardize image\n    6. Stacking processed images to a batch\n\n    ???+ warning\n        When instantiating a `DataGenerator`, it is highly recommended, to pass the `image_format` parameter provided\n        by the `input_interface()` and the `resize` &amp; `standardize_mode` parameters provided by the\n        `NeuralNetwork` class attributes `meta_input` &amp; `meta_standardize`.\n\n        It assures, that the samples contain the expected file extension, input shape and standardization.\n\n    ???+ abstract \"Build on top of the library\"\n        Tensorflow.Keras Iterator: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/Iterator\n\n    ??? example \"Example: How to integrate metadata in AUCMEDI?\"\n        ```python\n        from aucmedi import *\n        import numpy as np\n\n        my_metadata = np.random.rand(len(samples), 10)\n\n        my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                                  meta_variables=10)\n\n        my_dg = DataGenerator(samples, \"images_dir/\",\n                              labels=None, metadata=my_metadata,\n                              resize=my_model.meta_input,                  # (224,224)\n                              standardize_mode=my_model.meta_standardize)  # \"torch\"\n        ```\n    \"\"\"\n    #-----------------------------------------------------#\n    #                    Initialization                   #\n    #-----------------------------------------------------#\n    def __init__(self, samples, path_imagedir, labels=None, metadata=None,\n                 image_format=None, subfunctions=[], batch_size=32,\n                 resize=(224, 224), standardize_mode=\"z-score\", data_aug=None,\n                 shuffle=False, grayscale=False, sample_weights=None, workers=1,\n                 prepare_images=False, loader=image_loader, seed=None,\n                 **kwargs):\n        \"\"\" Initialization function of the DataGenerator which acts as a configuration hub.\n\n        If using for prediction, the 'labels' parameter has to be `None`.\n\n        For more information on Subfunctions, read here: [aucmedi.data_processing.subfunctions][].\n\n        Data augmentation is applied even for prediction if a DataAugmentation object is provided!\n\n        ???+ warning\n            Augmentation should only be applied to a **training** DataGenerator!\n\n            For test-time augmentation, [aucmedi.ensemble.augmenting][] should be used.\n\n        Applying `None` to `resize` will result into no image resizing. Default (224, 224)\n\n        ???+ info \"IO_loader Functions\"\n            | Interface                                                        | Description                                  |\n            | ---------------------------------------------------------------- | -------------------------------------------- |\n            | [image_loader()][aucmedi.data_processing.io_loader.image_loader] | Image Loader for image loading via Pillow. |\n            | [sitk_loader()][aucmedi.data_processing.io_loader.sitk_loader]   | SimpleITK Loader for loading NIfTI (nii) or Metafile (mha) formats.    |\n            | [numpy_loader()][aucmedi.data_processing.io_loader.numpy_loader] | NumPy Loader for image loading of .npy files.    |\n            | [cache_loader()][aucmedi.data_processing.io_loader.cache_loader] | Cache Loader for passing already loaded images. |\n\n            More information on IO_loader functions can be found here: [aucmedi.data_processing.io_loader][]. &lt;br&gt;\n            Parameters defined in `**kwargs` are passed down to IO_loader functions.\n\n        Args:\n            samples (list of str):              List of sample/index encoded as Strings. Provided by\n                                                [input_interface][aucmedi.data_processing.io_data.input_interface].\n            path_imagedir (str):                Path to the directory containing the images.\n            labels (numpy.ndarray):             Classification list with One-Hot Encoding. Provided by\n                                                [input_interface][aucmedi.data_processing.io_data.input_interface].\n            metadata (numpy.ndarray):           NumPy Array with additional metadata. Have to be shape (n_samples, meta_variables).\n            image_format (str):                 Image format to add at the end of the sample index for image loading.\n                                                Provided by [input_interface][aucmedi.data_processing.io_data.input_interface].\n            subfunctions (List of Subfunctions):List of Subfunctions class instances which will be SEQUENTIALLY executed on the data set.\n            batch_size (int):                   Number of samples inside a single batch.\n            resize (tuple of int):              Resizing shape consisting of a X and Y size. (optional Z size for Volumes)\n            standardize_mode (str):             Standardization modus in which image intensity values are scaled.\n                                                Calls the [Standardize][aucmedi.data_processing.subfunctions.standardize] Subfunction.\n            data_aug (Augmentation Interface):  Data Augmentation class instance which performs diverse augmentation techniques.\n                                                If `None` is provided, no augmentation will be performed.\n            shuffle (bool):                     Boolean, whether dataset should be shuffled.\n            grayscale (bool):                   Boolean, whether images are grayscale or RGB.\n            sample_weights (list of float):     List of weights for samples. Can be computed via\n                                                [compute_sample_weights()][aucmedi.utils.class_weights.compute_sample_weights].\n            workers (int):                      Number of workers. If n_workers &gt; 1 = use multi-threading for image preprocessing.\n            prepare_images (bool):              Boolean, whether all images should be prepared and backup to disk before training.\n                                                Recommended for large images or volumes to reduce CPU computing time.\n            loader (io_loader function):        Function for loading samples/images from disk.\n            seed (int):                         Seed to ensure reproducibility for random function.\n            **kwargs (dict):                    Additional parameters for the sample loader.\n        \"\"\"\n        # Cache class variables\n        self.samples = samples\n        self.labels = labels\n        self.metadata = metadata\n        self.sample_weights = sample_weights\n        self.prepare_images = prepare_images\n        self.workers = workers\n        self.sample_loader = loader\n        self.kwargs = kwargs\n        self.path_imagedir = path_imagedir\n        self.image_format = image_format\n        self.grayscale = grayscale\n        self.subfunctions = subfunctions\n        self.batch_size = batch_size\n        self.data_aug = data_aug\n        self.standardize_mode = standardize_mode\n        self.resize = resize\n        self.shuffle = shuffle\n        self.seed = seed\n        # Cache keras.Sequence class variables\n        self.n = len(samples)\n        self.max_iterations = (self.n + self.batch_size - 1) // self.batch_size\n        self.iterations = self.max_iterations\n        self.seed_walk = 0\n        self.index_array = None\n\n        # Initialize Standardization Subfunction\n        if standardize_mode is not None:\n            self.sf_standardize = Standardize(mode=standardize_mode)\n        else : self.sf_standardize = None\n        # Initialize Resizing Subfunction\n        if resize is not None : self.sf_resize = Resize(shape=resize)\n        else : self.sf_resize = None\n        # Sanity check for full sample list\n        if samples is not None and len(samples) == 0:\n            raise ValueError(\"Provided sample list is empty!\", len(samples))\n        # Sanity check for label correctness\n        if labels is not None and len(samples) != len(labels):\n            raise ValueError(\"Samples and labels do not have same size!\",\n                             len(samples), len(labels))\n        # Sanity check for metadata correctness\n        if metadata is not None and len(samples) != len(metadata):\n            raise ValueError(\"Samples and metadata do not have same size!\",\n                             len(samples), len(metadata))\n        # Sanity check for sample weights correctness\n        if sample_weights is not None and len(samples) != len(sample_weights):\n            raise ValueError(\"Samples and sample weights do not have same size!\",\n                             len(samples), len(sample_weights))\n        # Verify that labels, metadata and sample weights are NumPy arrays\n        if labels is not None and not isinstance(labels, np.ndarray):\n            self.labels = np.asarray(self.labels)\n        if metadata is not None and not isinstance(metadata, np.ndarray):\n            self.metadata = np.asarray(self.metadata)\n        if sample_weights is not None and not isinstance(sample_weights,\n                                                         np.ndarray):\n            self.sample_weights = np.asarray(self.sample_weights)\n\n        # If prepare_image modus activated\n        # -&gt; Preprocess images beforehand and store them to disk for fast usage later\n        if self.prepare_images:\n            self.prepare_dir_object = tempfile.TemporaryDirectory(\n                                               prefix=\"aucmedi.tmp.\",\n                                               suffix=\".data\")\n            self.prepare_dir = self.prepare_dir_object.name\n\n            # Preprocess image for each index - Sequential\n            if self.workers == 0 or self.workers == 1:\n                for i in range(0, len(samples)):\n                    self.preprocess_image(index=i, prepared_image=False,\n                                          run_resize=True, run_aug=False,\n                                          run_standardize=False,\n                                          dump_pickle=True)\n            # Preprocess image for each index - Multi-threading\n            else:\n                with ThreadPool(self.workers) as pool:\n                    index_array = list(range(0, len(samples)))\n                    mp_params = zip(index_array, repeat(False), repeat(True),\n                                    repeat(False), repeat(False), repeat(True))\n                    pool.starmap(self.preprocess_image, mp_params)\n            print(\"A directory for image preparation was created:\",\n                  self.prepare_dir)\n\n    #-----------------------------------------------------#\n    #              Batch Generation Function              #\n    #-----------------------------------------------------#\n    \"\"\" Internal function for batch generation given a list of random selected samples. \"\"\"\n    def _get_batches_of_transformed_samples(self, index_array):\n        # Initialize Batch stack\n        batch_stack = ([],)\n        if self.labels is not None : batch_stack += ([],)\n        if self.sample_weights is not None : batch_stack += ([],)\n\n        # Process image for each index - Sequential\n        if self.workers == 0 or self.workers == 1:\n            for i in index_array:\n                batch_img = self.preprocess_image(index=i,\n                                                  prepared_image=self.prepare_images)\n                batch_stack[0].append(batch_img)\n        # Process image for each index - Multi-threading\n        else:\n            with ThreadPool(self.workers) as pool:\n                mp_params = zip(index_array, repeat(self.prepare_images))\n                batches_img = pool.starmap(self.preprocess_image, mp_params)\n            batch_stack[0].extend(batches_img)\n\n        # Add classification to batch if available\n        if self.labels is not None:\n            batch_stack[1].extend(self.labels[index_array])\n        # Add sample weight to batch if available\n        if self.sample_weights is not None:\n            batch_stack[2].extend(self.sample_weights[index_array])\n\n        # Stack images and optional metadata together into a batch\n        input_stack = np.stack(batch_stack[0], axis=0)\n        if self.metadata is not None:\n            input_stack = (input_stack, self.metadata[index_array])\n        batch = (input_stack, )\n        # Stack classifications together into a batch if available\n        if self.labels is not None:\n            batch += (np.stack(batch_stack[1], axis=0), )\n        # Stack sample weights together into a batch if available\n        if self.sample_weights is not None:\n            batch += (np.stack(batch_stack[2], axis=0), )\n        # Return generated Batch\n        return batch\n\n    #-----------------------------------------------------#\n    #                 Image Preprocessing                 #\n    #-----------------------------------------------------#\n    def preprocess_image(self, index, prepared_image=False, run_resize=True,\n                         run_aug=True, run_standardize=True, dump_pickle=False):\n        \"\"\" Internal preprocessing function for applying Subfunctions, augmentation, resizing and standardization\n        on an image given its index.\n\n        Activating the prepared_image option also allows loading a beforehand preprocessed image from disk.\n\n        Deactivating the run_aug &amp; run_standardize option to output image without augmentation and standardization.\n\n        Activating dump_pickle will store the preprocessed image as pickle on disk instead of returning.\n        \"\"\"\n        # Load prepared image from disk\n        if prepared_image:\n            # Load from disk\n            path_img = os.path.join(self.prepare_dir, \"img_\" + str(index))\n            with open(path_img + \".pickle\", \"rb\") as pickle_loader:\n                img = pickle.load(pickle_loader)\n            # Apply image augmentation on image if activated\n            if self.data_aug is not None and run_aug:\n                img = self.data_aug.apply(img)\n            # Apply standardization on image if activated\n            if self.sf_standardize is not None and run_standardize:\n                img = self.sf_standardize.transform(img)\n        # Preprocess image during runtime\n        else:\n            # Load image from disk\n            img = self.sample_loader(self.samples[index], self.path_imagedir,\n                                     image_format=self.image_format,\n                                     grayscale=self.grayscale,\n                                     **self.kwargs)\n            # Apply subfunctions on image\n            for sf in self.subfunctions:\n                img = sf.transform(img)\n            # Apply resizing on image if activated\n            if self.sf_resize is not None and run_resize:\n                img = self.sf_resize.transform(img)\n            # Apply image augmentation on image if activated\n            if self.data_aug is not None and run_aug:\n                img = self.data_aug.apply(img)\n            # Apply standardization on image if activated\n            if self.sf_standardize is not None and run_standardize:\n                img = self.sf_standardize.transform(img)\n        # Dump preprocessed image to disk (for later usage via prepared_image)\n        if dump_pickle:\n            path_img = os.path.join(self.prepare_dir, \"img_\" + str(index))\n            with open(path_img + \".pickle\", \"wb\") as pickle_writer:\n                pickle.dump(img, pickle_writer)\n        # Return preprocessed image\n        else : return img\n\n    #-----------------------------------------------------#\n    #              Sample Generation Function             #\n    #-----------------------------------------------------#\n    \"\"\" Internal function for calling the batch generation process. \"\"\"\n    def __getitem__(self, raw_idx):\n        # Obtain the index based on the passed index offset to allow repetition\n        idx = raw_idx % self.max_iterations\n        # Build index array for the start\n        if self.index_array is None:\n            self.__set_index_array__()\n        # Select samples for next batch\n        index_array = self.index_array[\n            self.batch_size * idx : self.batch_size * (idx + 1)\n        ]\n        # Generate batch\n        return self._get_batches_of_transformed_samples(index_array)\n\n    #-----------------------------------------------------#\n    #                 Generator Functions                 #\n    #-----------------------------------------------------#\n    \"\"\" Internal function for identifying the generator length. \"\"\"\n    def __len__(self):\n        return self.iterations\n\n    \"\"\" Configuration function for fixing the number of iterations. \"\"\"\n    def set_length(self, iterations):\n        self.iterations = iterations\n\n    \"\"\" Configuration function for reseting the number of iterations. \"\"\"\n    def reset_length(self):\n        self.iterations = self.max_iterations\n\n    \"\"\" Internal function for initializing and shuffling the index array. \"\"\"\n    def __set_index_array__(self):\n        # Generate index array\n        self.index_array = np.arange(self.n)\n        # Shuffle if needed\n        if self.shuffle:\n            # Update seed for repeated permutation of the index_array\n            if self.seed is not None:\n                np.random.seed(self.seed + self.seed_walk)\n                self.seed_walk += 1\n            # Permutate index array\n            self.index_array = np.random.permutation(self.n)\n\n    \"\"\" Internal function at the end of an epoch. \"\"\"\n    def on_epoch_end(self):\n        self.__set_index_array__()\n</code></pre>"},{"location":"reference/data_processing/data_generator/#aucmedi.data_processing.data_generator.DataGenerator.__init__","title":"<code>__init__(samples, path_imagedir, labels=None, metadata=None, image_format=None, subfunctions=[], batch_size=32, resize=(224, 224), standardize_mode='z-score', data_aug=None, shuffle=False, grayscale=False, sample_weights=None, workers=1, prepare_images=False, loader=image_loader, seed=None, **kwargs)</code>","text":"<p>Initialization function of the DataGenerator which acts as a configuration hub.</p> <p>If using for prediction, the 'labels' parameter has to be <code>None</code>.</p> <p>For more information on Subfunctions, read here: aucmedi.data_processing.subfunctions.</p> <p>Data augmentation is applied even for prediction if a DataAugmentation object is provided!</p> Warning <p>Augmentation should only be applied to a training DataGenerator!</p> <p>For test-time augmentation, aucmedi.ensemble.augmenting should be used.</p> <p>Applying <code>None</code> to <code>resize</code> will result into no image resizing. Default (224, 224)</p> IO_loader Functions Interface Description image_loader() Image Loader for image loading via Pillow. sitk_loader() SimpleITK Loader for loading NIfTI (nii) or Metafile (mha) formats. numpy_loader() NumPy Loader for image loading of .npy files. cache_loader() Cache Loader for passing already loaded images. <p>More information on IO_loader functions can be found here: aucmedi.data_processing.io_loader.  Parameters defined in <code>**kwargs</code> are passed down to IO_loader functions.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Provided by                                 input_interface.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                                 input_interface.</p> <code>None</code> <code>metadata</code> <code>numpy.ndarray</code> <p>NumPy Array with additional metadata. Have to be shape (n_samples, meta_variables).</p> <code>None</code> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading.                                 Provided by input_interface.</p> <code>None</code> <code>subfunctions</code> <code>List of Subfunctions</code> <p>List of Subfunctions class instances which will be SEQUENTIALLY executed on the data set.</p> <code>[]</code> <code>batch_size</code> <code>int</code> <p>Number of samples inside a single batch.</p> <code>32</code> <code>resize</code> <code>tuple of int</code> <p>Resizing shape consisting of a X and Y size. (optional Z size for Volumes)</p> <code>(224, 224)</code> <code>standardize_mode</code> <code>str</code> <p>Standardization modus in which image intensity values are scaled.                                 Calls the Standardize Subfunction.</p> <code>'z-score'</code> <code>data_aug</code> <code>Augmentation Interface</code> <p>Data Augmentation class instance which performs diverse augmentation techniques.                                 If <code>None</code> is provided, no augmentation will be performed.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Boolean, whether dataset should be shuffled.</p> <code>False</code> <code>grayscale</code> <code>bool</code> <p>Boolean, whether images are grayscale or RGB.</p> <code>False</code> <code>sample_weights</code> <code>list of float</code> <p>List of weights for samples. Can be computed via                                 compute_sample_weights().</p> <code>None</code> <code>workers</code> <code>int</code> <p>Number of workers. If n_workers &gt; 1 = use multi-threading for image preprocessing.</p> <code>1</code> <code>prepare_images</code> <code>bool</code> <p>Boolean, whether all images should be prepared and backup to disk before training.                                 Recommended for large images or volumes to reduce CPU computing time.</p> <code>False</code> <code>loader</code> <code>io_loader function</code> <p>Function for loading samples/images from disk.</p> <code>image_loader</code> <code>seed</code> <code>int</code> <p>Seed to ensure reproducibility for random function.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the sample loader.</p> <code>{}</code> Source code in <code>aucmedi/data_processing/data_generator.py</code> <pre><code>def __init__(self, samples, path_imagedir, labels=None, metadata=None,\n             image_format=None, subfunctions=[], batch_size=32,\n             resize=(224, 224), standardize_mode=\"z-score\", data_aug=None,\n             shuffle=False, grayscale=False, sample_weights=None, workers=1,\n             prepare_images=False, loader=image_loader, seed=None,\n             **kwargs):\n    \"\"\" Initialization function of the DataGenerator which acts as a configuration hub.\n\n    If using for prediction, the 'labels' parameter has to be `None`.\n\n    For more information on Subfunctions, read here: [aucmedi.data_processing.subfunctions][].\n\n    Data augmentation is applied even for prediction if a DataAugmentation object is provided!\n\n    ???+ warning\n        Augmentation should only be applied to a **training** DataGenerator!\n\n        For test-time augmentation, [aucmedi.ensemble.augmenting][] should be used.\n\n    Applying `None` to `resize` will result into no image resizing. Default (224, 224)\n\n    ???+ info \"IO_loader Functions\"\n        | Interface                                                        | Description                                  |\n        | ---------------------------------------------------------------- | -------------------------------------------- |\n        | [image_loader()][aucmedi.data_processing.io_loader.image_loader] | Image Loader for image loading via Pillow. |\n        | [sitk_loader()][aucmedi.data_processing.io_loader.sitk_loader]   | SimpleITK Loader for loading NIfTI (nii) or Metafile (mha) formats.    |\n        | [numpy_loader()][aucmedi.data_processing.io_loader.numpy_loader] | NumPy Loader for image loading of .npy files.    |\n        | [cache_loader()][aucmedi.data_processing.io_loader.cache_loader] | Cache Loader for passing already loaded images. |\n\n        More information on IO_loader functions can be found here: [aucmedi.data_processing.io_loader][]. &lt;br&gt;\n        Parameters defined in `**kwargs` are passed down to IO_loader functions.\n\n    Args:\n        samples (list of str):              List of sample/index encoded as Strings. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n        path_imagedir (str):                Path to the directory containing the images.\n        labels (numpy.ndarray):             Classification list with One-Hot Encoding. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n        metadata (numpy.ndarray):           NumPy Array with additional metadata. Have to be shape (n_samples, meta_variables).\n        image_format (str):                 Image format to add at the end of the sample index for image loading.\n                                            Provided by [input_interface][aucmedi.data_processing.io_data.input_interface].\n        subfunctions (List of Subfunctions):List of Subfunctions class instances which will be SEQUENTIALLY executed on the data set.\n        batch_size (int):                   Number of samples inside a single batch.\n        resize (tuple of int):              Resizing shape consisting of a X and Y size. (optional Z size for Volumes)\n        standardize_mode (str):             Standardization modus in which image intensity values are scaled.\n                                            Calls the [Standardize][aucmedi.data_processing.subfunctions.standardize] Subfunction.\n        data_aug (Augmentation Interface):  Data Augmentation class instance which performs diverse augmentation techniques.\n                                            If `None` is provided, no augmentation will be performed.\n        shuffle (bool):                     Boolean, whether dataset should be shuffled.\n        grayscale (bool):                   Boolean, whether images are grayscale or RGB.\n        sample_weights (list of float):     List of weights for samples. Can be computed via\n                                            [compute_sample_weights()][aucmedi.utils.class_weights.compute_sample_weights].\n        workers (int):                      Number of workers. If n_workers &gt; 1 = use multi-threading for image preprocessing.\n        prepare_images (bool):              Boolean, whether all images should be prepared and backup to disk before training.\n                                            Recommended for large images or volumes to reduce CPU computing time.\n        loader (io_loader function):        Function for loading samples/images from disk.\n        seed (int):                         Seed to ensure reproducibility for random function.\n        **kwargs (dict):                    Additional parameters for the sample loader.\n    \"\"\"\n    # Cache class variables\n    self.samples = samples\n    self.labels = labels\n    self.metadata = metadata\n    self.sample_weights = sample_weights\n    self.prepare_images = prepare_images\n    self.workers = workers\n    self.sample_loader = loader\n    self.kwargs = kwargs\n    self.path_imagedir = path_imagedir\n    self.image_format = image_format\n    self.grayscale = grayscale\n    self.subfunctions = subfunctions\n    self.batch_size = batch_size\n    self.data_aug = data_aug\n    self.standardize_mode = standardize_mode\n    self.resize = resize\n    self.shuffle = shuffle\n    self.seed = seed\n    # Cache keras.Sequence class variables\n    self.n = len(samples)\n    self.max_iterations = (self.n + self.batch_size - 1) // self.batch_size\n    self.iterations = self.max_iterations\n    self.seed_walk = 0\n    self.index_array = None\n\n    # Initialize Standardization Subfunction\n    if standardize_mode is not None:\n        self.sf_standardize = Standardize(mode=standardize_mode)\n    else : self.sf_standardize = None\n    # Initialize Resizing Subfunction\n    if resize is not None : self.sf_resize = Resize(shape=resize)\n    else : self.sf_resize = None\n    # Sanity check for full sample list\n    if samples is not None and len(samples) == 0:\n        raise ValueError(\"Provided sample list is empty!\", len(samples))\n    # Sanity check for label correctness\n    if labels is not None and len(samples) != len(labels):\n        raise ValueError(\"Samples and labels do not have same size!\",\n                         len(samples), len(labels))\n    # Sanity check for metadata correctness\n    if metadata is not None and len(samples) != len(metadata):\n        raise ValueError(\"Samples and metadata do not have same size!\",\n                         len(samples), len(metadata))\n    # Sanity check for sample weights correctness\n    if sample_weights is not None and len(samples) != len(sample_weights):\n        raise ValueError(\"Samples and sample weights do not have same size!\",\n                         len(samples), len(sample_weights))\n    # Verify that labels, metadata and sample weights are NumPy arrays\n    if labels is not None and not isinstance(labels, np.ndarray):\n        self.labels = np.asarray(self.labels)\n    if metadata is not None and not isinstance(metadata, np.ndarray):\n        self.metadata = np.asarray(self.metadata)\n    if sample_weights is not None and not isinstance(sample_weights,\n                                                     np.ndarray):\n        self.sample_weights = np.asarray(self.sample_weights)\n\n    # If prepare_image modus activated\n    # -&gt; Preprocess images beforehand and store them to disk for fast usage later\n    if self.prepare_images:\n        self.prepare_dir_object = tempfile.TemporaryDirectory(\n                                           prefix=\"aucmedi.tmp.\",\n                                           suffix=\".data\")\n        self.prepare_dir = self.prepare_dir_object.name\n\n        # Preprocess image for each index - Sequential\n        if self.workers == 0 or self.workers == 1:\n            for i in range(0, len(samples)):\n                self.preprocess_image(index=i, prepared_image=False,\n                                      run_resize=True, run_aug=False,\n                                      run_standardize=False,\n                                      dump_pickle=True)\n        # Preprocess image for each index - Multi-threading\n        else:\n            with ThreadPool(self.workers) as pool:\n                index_array = list(range(0, len(samples)))\n                mp_params = zip(index_array, repeat(False), repeat(True),\n                                repeat(False), repeat(False), repeat(True))\n                pool.starmap(self.preprocess_image, mp_params)\n        print(\"A directory for image preparation was created:\",\n              self.prepare_dir)\n</code></pre>"},{"location":"reference/data_processing/data_generator/#aucmedi.data_processing.data_generator.DataGenerator.preprocess_image","title":"<code>preprocess_image(index, prepared_image=False, run_resize=True, run_aug=True, run_standardize=True, dump_pickle=False)</code>","text":"<p>Internal preprocessing function for applying Subfunctions, augmentation, resizing and standardization on an image given its index.</p> <p>Activating the prepared_image option also allows loading a beforehand preprocessed image from disk.</p> <p>Deactivating the run_aug &amp; run_standardize option to output image without augmentation and standardization.</p> <p>Activating dump_pickle will store the preprocessed image as pickle on disk instead of returning.</p> Source code in <code>aucmedi/data_processing/data_generator.py</code> <pre><code>def preprocess_image(self, index, prepared_image=False, run_resize=True,\n                     run_aug=True, run_standardize=True, dump_pickle=False):\n    \"\"\" Internal preprocessing function for applying Subfunctions, augmentation, resizing and standardization\n    on an image given its index.\n\n    Activating the prepared_image option also allows loading a beforehand preprocessed image from disk.\n\n    Deactivating the run_aug &amp; run_standardize option to output image without augmentation and standardization.\n\n    Activating dump_pickle will store the preprocessed image as pickle on disk instead of returning.\n    \"\"\"\n    # Load prepared image from disk\n    if prepared_image:\n        # Load from disk\n        path_img = os.path.join(self.prepare_dir, \"img_\" + str(index))\n        with open(path_img + \".pickle\", \"rb\") as pickle_loader:\n            img = pickle.load(pickle_loader)\n        # Apply image augmentation on image if activated\n        if self.data_aug is not None and run_aug:\n            img = self.data_aug.apply(img)\n        # Apply standardization on image if activated\n        if self.sf_standardize is not None and run_standardize:\n            img = self.sf_standardize.transform(img)\n    # Preprocess image during runtime\n    else:\n        # Load image from disk\n        img = self.sample_loader(self.samples[index], self.path_imagedir,\n                                 image_format=self.image_format,\n                                 grayscale=self.grayscale,\n                                 **self.kwargs)\n        # Apply subfunctions on image\n        for sf in self.subfunctions:\n            img = sf.transform(img)\n        # Apply resizing on image if activated\n        if self.sf_resize is not None and run_resize:\n            img = self.sf_resize.transform(img)\n        # Apply image augmentation on image if activated\n        if self.data_aug is not None and run_aug:\n            img = self.data_aug.apply(img)\n        # Apply standardization on image if activated\n        if self.sf_standardize is not None and run_standardize:\n            img = self.sf_standardize.transform(img)\n    # Dump preprocessed image to disk (for later usage via prepared_image)\n    if dump_pickle:\n        path_img = os.path.join(self.prepare_dir, \"img_\" + str(index))\n        with open(path_img + \".pickle\", \"wb\") as pickle_writer:\n            pickle.dump(img, pickle_writer)\n    # Return preprocessed image\n    else : return img\n</code></pre>"},{"location":"reference/data_processing/io_data/","title":"Io data","text":""},{"location":"reference/data_processing/io_data/#aucmedi.data_processing.io_data.ACCEPTABLE_IMAGE_FORMATS","title":"<code>ACCEPTABLE_IMAGE_FORMATS = ['jpeg', 'jpg', 'tif', 'tiff', 'png', 'bmp', 'gif', 'npy', 'nii', 'gz', 'mha']</code>  <code>module-attribute</code>","text":"<p>List of accepted image formats. </p>"},{"location":"reference/data_processing/io_data/#aucmedi.data_processing.io_data.input_interface","title":"<code>input_interface(interface, path_imagedir, path_data=None, training=True, ohe=False, image_format=None, **kwargs)</code>","text":"<p>Data Input Interface for all automatically extract various information of dataset structures.</p> <p>Different image file structures and annotation information are processed by corresponding format interfaces. These extracted information can be parsed to the DataGenerator and the NeuralNetwork.</p> <p>The input_interface() function is the first of the three pillars of AUCMEDI.</p> Pillars of AUCMEDI <ul> <li>aucmedi.data_processing.io_data.input_interface</li> <li>aucmedi.data_processing.data_generator.DataGenerator</li> <li>aucmedi.neural_network.model.NeuralNetwork</li> </ul> <p>Basically a wrapper function for calling the correct format interface, which loads a dataset from disk via the associated format parser.</p> <p>Possible format interfaces: <code>[\"csv\", \"json\", \"directory\"]</code></p> Format Interfaces Interface Internal Function Description <code>\"csv\"</code> io_csv() Storing class annotations in a CSV file. <code>\"directory\"</code> io_directory() Storing class annotations in subdirectories. <code>\"json\"</code> io_json() Storing class annotations in a JSON file. Example <pre><code># AUCMEDI library\nfrom aucmedi import *\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",                       # Interface type\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(index_list, class_ohe, nclasses, class_names, image_format) = ds\n\n# Pass variables to other AUCMEDI pillars like DataGenerator\ndatagen = DataGenerator(samples=index_list,                 # from input_interface()\n                        path_imagedir=\"dataset/images/\",\n                        labels=class_ohe,                   # from input_interface()\n                        image_format=image_format)          # from input_interface()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>interface</code> <code>str</code> <p>String defining format interface for loading/storing data.</p> required <code>path_data</code> <code>str</code> <p>Path to the index/class annotation file if required. (csv/json)</p> <code>None</code> <code>training</code> <code>bool</code> <p>Boolean option whether annotation data is available.</p> <code>True</code> <code>ohe</code> <code>bool</code> <p>Boolean option whether annotation data is sparse categorical or one-hot encoded.</p> <code>False</code> <code>image_format</code> <code>str</code> <p>Force to use a specific image format. By default, image format is determined automatically.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the format interfaces.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>index_list</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Required in DataGenerator as <code>samples</code>.</p> <code>class_ohe</code> <code>numpy.ndarray</code> <p>Classification list as One-Hot encoding. Required in DataGenerator as <code>labels</code>.</p> <code>class_n</code> <code>int</code> <p>Number of classes. Required in NeuralNetwork for Architecture design as <code>n_labels</code>.</p> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for later prediction storage or evaluation.</p> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading. Required in DataGenerator.</p> Source code in <code>aucmedi/data_processing/io_data.py</code> <pre><code>def input_interface(interface, path_imagedir, path_data=None, training=True,\n                    ohe=False, image_format=None, **kwargs):\n    \"\"\" Data Input Interface for all automatically extract various information of dataset structures.\n\n    Different image file structures and annotation information are processed by\n    corresponding format interfaces. These extracted information can be parsed to the\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator] and the\n    [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork].\n\n    The input_interface() function is the first of the three pillars of AUCMEDI.\n\n    ??? info \"Pillars of AUCMEDI\"\n        - [aucmedi.data_processing.io_data.input_interface][]\n        - [aucmedi.data_processing.data_generator.DataGenerator][]\n        - [aucmedi.neural_network.model.NeuralNetwork][]\n\n    Basically a wrapper function for calling the correct format interface,\n    which loads a dataset from disk via the associated format parser.\n\n    Possible format interfaces: `[\"csv\", \"json\", \"directory\"]`\n\n    ???+ info \"Format Interfaces\"\n        | Interface      | Internal Function                                                    | Description                                  |\n        | -------------- | -------------------------------------------------------------------- | -------------------------------------------- |\n        |  `\"csv\"`       | [io_csv()][aucmedi.data_processing.io_interfaces.io_csv]             | Storing class annotations in a CSV file.     |\n        |  `\"directory\"` | [io_directory()][aucmedi.data_processing.io_interfaces.io_directory] | Storing class annotations in subdirectories. |\n        |  `\"json\"`      | [io_json()][aucmedi.data_processing.io_interfaces.io_json]           | Storing class annotations in a JSON file.    |\n\n    ???+ example\n        ```python\n        # AUCMEDI library\n        from aucmedi import *\n\n        # Initialize input data reader\n        ds = input_interface(interface=\"csv\",                       # Interface type\n                             path_imagedir=\"dataset/images/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (index_list, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Pass variables to other AUCMEDI pillars like DataGenerator\n        datagen = DataGenerator(samples=index_list,                 # from input_interface()\n                                path_imagedir=\"dataset/images/\",\n                                labels=class_ohe,                   # from input_interface()\n                                image_format=image_format)          # from input_interface()\n        ```\n\n    Args:\n        path_imagedir (str):            Path to the directory containing the images.\n        interface (str):                String defining format interface for loading/storing data.\n        path_data (str):                Path to the index/class annotation file if required. (csv/json)\n        training (bool):                Boolean option whether annotation data is available.\n        ohe (bool):                     Boolean option whether annotation data is sparse categorical or one-hot encoded.\n        image_format (str):             Force to use a specific image format. By default, image format is determined automatically.\n        **kwargs (dict):                Additional parameters for the format interfaces.\n\n    Returns:\n        index_list (list of str):       List of sample/index encoded as Strings. Required in DataGenerator as `samples`.\n        class_ohe (numpy.ndarray):      Classification list as One-Hot encoding. Required in DataGenerator as `labels`.\n        class_n (int):                  Number of classes. Required in NeuralNetwork for Architecture design as `n_labels`.\n        class_names (list of str):      List of names for corresponding classes. Used for later prediction storage or evaluation.\n        image_format (str):             Image format to add at the end of the sample index for image loading. Required in DataGenerator.\n    \"\"\"\n    # Transform selected interface to lower case\n    interface = interface.lower()\n    # Pass image format if provided\n    if image_format != None : allowed_image_formats = [image_format]\n    else : allowed_image_formats = ACCEPTABLE_IMAGE_FORMATS\n    # Verify if provided interface is valid\n    if interface not in [\"csv\", \"json\", \"directory\"]:\n        raise Exception(\"Unknown interface code provided.\", interface)\n    # Verify that annotation file is available if CSV/JSON interface is used\n    if interface in [\"csv\", \"json\"] and path_data is None:\n        raise Exception(\"No annotation file provided for CSV/JSON interface!\")\n\n    # Initialize parameter dictionary\n    parameters = {\"path_data\": path_data,\n                  \"path_imagedir\": path_imagedir,\n                  \"allowed_image_formats\": allowed_image_formats,\n                  \"training\": training, \"ohe\": ohe}\n    # Identify correct dataset loader and parameters for CSV format\n    if interface == \"csv\":\n        ds_loader = io.csv_loader\n        additional_parameters = [\"ohe_range\", \"col_sample\", \"col_class\"]\n        for para in additional_parameters:\n            if para in kwargs : parameters[para] = kwargs[para]\n    # Identify correct dataset loader and parameters for JSON format\n    elif interface == \"json\" : ds_loader = io.json_loader\n    # Identify correct dataset loader and parameters for directory format\n    elif interface == \"directory\":\n        ds_loader = io.directory_loader\n        del parameters[\"ohe\"]\n        del parameters[\"path_data\"]\n\n    # Load the dataset with the selected format interface and return results\n    return ds_loader(**parameters)\n</code></pre>"},{"location":"reference/data_processing/augmentation/","title":"Augmentation","text":"<p>The Augmentation classes of AUCMEDI allow creating interfaces to powerful     augmentation frameworks and easily integrate them into the AUCMEDI pipeline.</p> <p>An Augmentation class is a preprocessing method, which is randomly applied on each sample if provided to a DataGenerator.</p> Warning <p>Augmentation should only be applied to a training DataGenerator!</p> <p>For test-time augmentation, aucmedi.ensemble.augmenting should be used.</p> <p>Data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.</p> <p>The point of data augmentation is, that the model will learn meaningful patterns instead of meaningless characteristics due to a small data set size.</p> Data Augmentation Interfaces Interface Description ImageAugmentation Interface to package: Albumentations. Handles only images (2D data). VolumeAugmentation Interface to package: Volumentations. Handles only volumes (3D data). BatchgeneratorsAugmentation Interface to package: batchgenerators (DKFZ). Handles images and volumes (2D+3D data). <p>Recommendation:  - For images (2D data): ImageAugmentation()  - For volumes (3D data): BatchgeneratorsAugmentation() </p> Example <p>For 2D data: <pre><code>from aucmedi import *\n\naug = ImageAugmentation(flip=True, rotate=True, brightness=True, contrast=True,\n             saturation=True, hue=True, scale=True, crop=False,\n             grid_distortion=False, compression=False, gaussian_noise=False,\n             gaussian_blur=False, downscaling=False, gamma=False,\n             elastic_transform=False)\n\ndatagen = DataGenerator(samples=index_list,\n                        path_imagedir=\"dataset/images/\",\n                        labels=class_ohe,\n                        data_aug=aug,\n                        resize=model.meta_input,\n                        image_format=image_format)\n</code></pre></p> <p>For 3D data: <pre><code>from aucmedi import *\n\naug = BatchgeneratorsAugmentation(model.meta_input, mirror=False, rotate=True,\n             scale=True, elastic_transform=False, gaussian_noise=True,\n             brightness=True, contrast=True, gamma=True)\n\ndatagen = DataGenerator(samples=index_list,\n                        path_imagedir=\"dataset/volumes/\",\n                        labels=class_ohe,\n                        data_aug=aug,\n                        resize=model.meta_input,\n                        image_format=image_format)\n</code></pre></p>"},{"location":"reference/data_processing/augmentation/aug_batchgenerators/","title":"Aug batchgenerators","text":""},{"location":"reference/data_processing/augmentation/aug_batchgenerators/#aucmedi.data_processing.augmentation.aug_batchgenerators.BatchgeneratorsAugmentation","title":"<code>BatchgeneratorsAugmentation</code>","text":"<p>The Batchgenerators Augmentation class performs diverse augmentation methods on given     numpy array. The class acts as an easy to use function/interface for applying     all types of augmentations with just one function call.</p> <p>The class can be configured beforehand by selecting desired augmentation techniques and method ranges or strength. Afterwards, the class is passed to the DataGenerator which utilizes it during batch generation.</p> <p>The specific configurations of selected methods can be adjusted by class variables.</p> Build on top of the library <p>Batchgenerators from the DKFZ - https://github.com/MIC-DKFZ/batchgenerators</p> Reference - Publication <p>Isensee Fabian, J\u00e4ger Paul, Wasserthal Jakob, Zimmerer David, Petersen Jens, Kohl Simon, Schock Justus, Klein Andre, Ro\u00df Tobias, Wirkert Sebastian, Neher Peter, Dinkelacker Stefan, K\u00f6hler Gregor, Maier-Hein Klaus (2020). batchgenerators - a python framework for data augmentation. doi:10.5281/zenodo.3632567</p> Source code in <code>aucmedi/data_processing/augmentation/aug_batchgenerators.py</code> <pre><code>class BatchgeneratorsAugmentation():\n    \"\"\" The Batchgenerators Augmentation class performs diverse augmentation methods on given\n        numpy array. The class acts as an easy to use function/interface for applying\n        all types of augmentations with just one function call.\n\n    The class can be configured beforehand by selecting desired augmentation techniques\n    and method ranges or strength.\n    Afterwards, the class is passed to the [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator]\n    which utilizes it during batch generation.\n\n    The specific configurations of selected methods can be adjusted by class variables.\n\n    ???+ abstract \"Build on top of the library\"\n        Batchgenerators from the DKFZ - https://github.com/MIC-DKFZ/batchgenerators\n\n    ???+ abstract \"Reference - Publication\"\n        Isensee Fabian, J\u00e4ger Paul, Wasserthal Jakob, Zimmerer David, Petersen Jens, Kohl Simon,\n        Schock Justus, Klein Andre, Ro\u00df Tobias, Wirkert Sebastian, Neher Peter, Dinkelacker Stefan,\n        K\u00f6hler Gregor, Maier-Hein Klaus (2020). batchgenerators - a python framework for data\n        augmentation. doi:10.5281/zenodo.3632567\n    \"\"\"\n    #-----------------------------------------------------#\n    #              Augmentation Configuration             #\n    #-----------------------------------------------------#\n    # Define augmentation operator\n    operator = None\n    # Option for augmentation refinement (clipping)\n    refine = True\n    # Augmentation: Mirror\n    aug_mirror = False\n    aug_mirror_p = 0.5\n    aug_mirror_axes = (0, 1, 2)\n    # Augmentation: 90 degree rotate\n    aug_rotate = False\n    aug_rotate_p = 0.5\n    aug_rotate_angleX = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi)\n    aug_rotate_angleY = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi)\n    aug_rotate_angleZ = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi)\n    # Augmentation: Brightness\n    aug_brightness = False\n    aug_brightness_p = 0.5\n    aug_brightness_range = (0.5, 2)\n    aug_brightness_per_channel = False\n    # Augmentation: Contrast\n    aug_contrast = False\n    aug_contrast_p = 0.5\n    aug_contrast_range = (0.3, 3.0)\n    aug_contrast_per_channel = False\n    aug_contrast_preserverange = True\n    # Augmentation: Scale\n    aug_scale = False\n    aug_scale_p = 0.5\n    aug_scale_range = (0.85, 1.25)\n    # Augmentation: Gaussian Noise\n    aug_gaussianNoise = False\n    aug_gaussianNoise_p = 0.5\n    aug_gaussianNoise_range = (0.0, 0.05)\n    # Augmentation: Gamma\n    aug_gamma = False\n    aug_gamma_p = 0.5\n    aug_gamma_range = (0.7, 1.5)\n    aug_gamma_per_channel = False\n    # Augmentation: Elastic Transformation\n    aug_elasticTransform = False\n    aug_elasticTransform_p = 0.5\n    aug_elasticTransform_alpha = (0.0, 900.0)\n    aug_elasticTransform_sigma = (9.0, 13.0)\n\n    #-----------------------------------------------------#\n    #                    Initialization                   #\n    #-----------------------------------------------------#\n    def __init__(self, image_shape, mirror=False, rotate=True, scale=True,\n                 elastic_transform=False, gaussian_noise=True,\n                 brightness=True, contrast=True, gamma=True):\n        \"\"\" Initialization function for the Batchgenerators Augmentation interface.\n\n        With boolean switches, it is possible to selected desired augmentation techniques.\n        Recommended augmentation configurations are defined as class variables.\n        Of course, these configs can be adjusted if needed.\n\n        Args:\n            image_shape (tuple of int):     Target shape of image, which will be passed to the neural network model.\n            mirror (bool):                  Boolean, whether mirroring should be performed as data augmentation.\n            rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n            scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n            elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n            gaussian_noise (bool):          Boolean, whether Gaussian noise should be added as data augmentation.\n            brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n            contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n            gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n\n        !!! warning\n            If class variables (attributes) are modified, the internal augmentation operator\n            has to be rebuild via the following call:\n\n            ```python\n            # initialize\n            aug = BatchgeneratorsAugmentation(model.meta_input, mirror=True)\n\n            # set probability to 100% = always\n            aug.aug_mirror_p = 1.0\n            # rebuild\n            aug.build()\n            ```\n\n        Attributes:\n            refine (bool):                  Boolean, whether clipping to [0,255] should be performed if outside of range.\n            aug_mirror_p (float):           Probability of mirroring application if activated. Default=0.5.\n            aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n            aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n            aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n            aug_gaussianNoise_p (float):    Probability of Gaussian noise application if activated. Default=0.5.\n            aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n            aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n            aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n        \"\"\"\n        # Cache class variables\n        self.image_shape = image_shape\n        self.aug_mirror = mirror\n        self.aug_rotate = rotate\n        self.aug_scale = scale\n        self.aug_elasticTransform = elastic_transform\n        self.aug_gaussianNoise = gaussian_noise\n        self.aug_brightness = brightness\n        self.aug_contrast = contrast\n        self.aug_gamma = gamma\n        # Build augmentation operator\n        self.build()\n\n    #-----------------------------------------------------#\n    #               Batchgenerators Builder               #\n    #-----------------------------------------------------#\n    def build(self):\n        \"\"\" Builds the batchgenerators augmentator by initializing  all transformations.\n\n        The activated transformation and their configurations are defined as\n        class variables.\n\n        -&gt; Builds a new self.operator\n        \"\"\"\n        # Initialize transform list\n        transforms = []\n        # Fill transform list\n        if self.aug_mirror:\n            tf = MirrorTransform(axes=self.aug_mirror_axes,\n                                 p_per_sample=self.aug_mirror_p)\n            transforms.append(tf)\n        if self.aug_contrast:\n            tf = ContrastAugmentationTransform(\n                                 self.aug_contrast_range,\n                                 preserve_range=self.aug_contrast_preserverange,\n                                 per_channel=self.aug_contrast_per_channel,\n                                 p_per_sample=self.aug_contrast_p)\n            transforms.append(tf)\n        if self.aug_brightness:\n            tf = BrightnessMultiplicativeTransform(\n                                 self.aug_brightness_range,\n                                 per_channel=self.aug_brightness_per_channel,\n                                 p_per_sample=self.aug_brightness_p)\n            transforms.append(tf)\n        if self.aug_gaussianNoise:\n            tf = GaussianNoiseTransform(self.aug_gaussianNoise_range,\n                                        p_per_sample=self.aug_gaussianNoise_p)\n            transforms.append(tf)\n        if self.aug_gamma:\n            tf = GammaTransform(self.aug_gamma_range,\n                                invert_image=False,\n                                per_channel=self.aug_gamma_per_channel,\n                                retain_stats=True,\n                                p_per_sample=self.aug_gamma_p)\n            transforms.append(tf)\n        if self.aug_rotate or self.aug_scale or self.aug_elasticTransform:\n            tf = SpatialTransform(self.image_shape,\n                                  [i // 2 for i in self.image_shape],\n                                  do_elastic_deform=self.aug_elasticTransform,\n                                  alpha=self.aug_elasticTransform_alpha,\n                                  sigma=self.aug_elasticTransform_sigma,\n                                  do_rotation=self.aug_rotate,\n                                  angle_x=self.aug_rotate_angleX,\n                                  angle_y=self.aug_rotate_angleY,\n                                  angle_z=self.aug_rotate_angleZ,\n                                  do_scale=self.aug_scale,\n                                  scale=self.aug_scale_range,\n                                  border_mode_data='constant',\n                                  border_cval_data=0,\n                                  border_mode_seg='constant',\n                                  border_cval_seg=0,\n                                  order_data=3, order_seg=0,\n                                  p_el_per_sample=self.aug_elasticTransform_p,\n                                  p_rot_per_sample=self.aug_rotate_p,\n                                  p_scale_per_sample=self.aug_scale_p,\n                                  random_crop=False)\n            transforms.append(tf)\n\n        # Compose transforms\n        self.operator = Compose(transforms)\n\n    #-----------------------------------------------------#\n    #                 Perform Augmentation                #\n    #-----------------------------------------------------#\n    def apply(self, image):\n        \"\"\" Performs image augmentation with defined configuration on an image.\n\n        This **internal** function is called in the DataGenerator during batch generation.\n\n        Args:\n            image (numpy.ndarray):          An image encoded as NumPy array with shape (z, y, x, channels).\n        Returns:\n            aug_image (numpy.ndarray):      An augmented / transformed image.\n        \"\"\"\n        # Convert image to batchgenerators format (float32, channel first and with batch axis)\n        image_bg = image.astype(np.float32)\n        image_bg = np.expand_dims(image_bg, axis=0)\n        image_bg = np.moveaxis(image_bg, -1, 1)\n        # Perform image augmentation\n        aug_image = self.operator(data=image_bg)[\"data\"]\n        # Remove batch axis and return to channel last\n        aug_image = np.moveaxis(aug_image, 1, -1)\n        aug_image = np.squeeze(aug_image, axis=0)\n        # Perform clipping if image is out of grayscale/RGB encodings\n        if self.refine and (np.min(aug_image) &lt; 0 or np.max(aug_image) &gt; 255):\n            aug_image = np.clip(aug_image, a_min=0, a_max=255)\n        # Return augmented image\n        return aug_image\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_batchgenerators/#aucmedi.data_processing.augmentation.aug_batchgenerators.BatchgeneratorsAugmentation.__init__","title":"<code>__init__(image_shape, mirror=False, rotate=True, scale=True, elastic_transform=False, gaussian_noise=True, brightness=True, contrast=True, gamma=True)</code>","text":"<p>Initialization function for the Batchgenerators Augmentation interface.</p> <p>With boolean switches, it is possible to selected desired augmentation techniques. Recommended augmentation configurations are defined as class variables. Of course, these configs can be adjusted if needed.</p> <p>Parameters:</p> Name Type Description Default <code>image_shape</code> <code>tuple of int</code> <p>Target shape of image, which will be passed to the neural network model.</p> required <code>mirror</code> <code>bool</code> <p>Boolean, whether mirroring should be performed as data augmentation.</p> <code>False</code> <code>rotate</code> <code>bool</code> <p>Boolean, whether rotations should be performed as data augmentation.</p> <code>True</code> <code>scale</code> <code>bool</code> <p>Boolean, whether scaling should be performed as data augmentation.</p> <code>True</code> <code>elastic_transform</code> <code>bool</code> <p>Boolean, whether elastic deformation should be performed as data augmentation.</p> <code>False</code> <code>gaussian_noise</code> <code>bool</code> <p>Boolean, whether Gaussian noise should be added as data augmentation.</p> <code>True</code> <code>brightness</code> <code>bool</code> <p>Boolean, whether brightness changes should be added as data augmentation.</p> <code>True</code> <code>contrast</code> <code>bool</code> <p>Boolean, whether contrast changes should be added as data augmentation.</p> <code>True</code> <code>gamma</code> <code>bool</code> <p>Boolean, whether gamma changes should be added as data augmentation.</p> <code>True</code> <p>Warning</p> <p>If class variables (attributes) are modified, the internal augmentation operator has to be rebuild via the following call:</p> <pre><code># initialize\naug = BatchgeneratorsAugmentation(model.meta_input, mirror=True)\n\n# set probability to 100% = always\naug.aug_mirror_p = 1.0\n# rebuild\naug.build()\n</code></pre> <p>Attributes:</p> Name Type Description <code>refine</code> <code>bool</code> <p>Boolean, whether clipping to [0,255] should be performed if outside of range.</p> <code>aug_mirror_p</code> <code>float</code> <p>Probability of mirroring application if activated. Default=0.5.</p> <code>aug_rotate_p</code> <code>float</code> <p>Probability of rotation application if activated. Default=0.5.</p> <code>aug_scale_p</code> <code>float</code> <p>Probability of scaling application if activated. Default=0.5.</p> <code>aug_elasticTransform_p</code> <code>float</code> <p>Probability of elastic deformation application if activated. Default=0.5.</p> <code>aug_gaussianNoise_p</code> <code>float</code> <p>Probability of Gaussian noise application if activated. Default=0.5.</p> <code>aug_brightness_p</code> <code>float</code> <p>Probability of brightness application if activated. Default=0.5.</p> <code>aug_contrast_p</code> <code>float</code> <p>Probability of contrast application if activated. Default=0.5.</p> <code>aug_gamma_p</code> <code>float</code> <p>Probability of gamma application if activated. Default=0.5.</p> Source code in <code>aucmedi/data_processing/augmentation/aug_batchgenerators.py</code> <pre><code>def __init__(self, image_shape, mirror=False, rotate=True, scale=True,\n             elastic_transform=False, gaussian_noise=True,\n             brightness=True, contrast=True, gamma=True):\n    \"\"\" Initialization function for the Batchgenerators Augmentation interface.\n\n    With boolean switches, it is possible to selected desired augmentation techniques.\n    Recommended augmentation configurations are defined as class variables.\n    Of course, these configs can be adjusted if needed.\n\n    Args:\n        image_shape (tuple of int):     Target shape of image, which will be passed to the neural network model.\n        mirror (bool):                  Boolean, whether mirroring should be performed as data augmentation.\n        rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n        scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n        elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n        gaussian_noise (bool):          Boolean, whether Gaussian noise should be added as data augmentation.\n        brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n        contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n        gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n\n    !!! warning\n        If class variables (attributes) are modified, the internal augmentation operator\n        has to be rebuild via the following call:\n\n        ```python\n        # initialize\n        aug = BatchgeneratorsAugmentation(model.meta_input, mirror=True)\n\n        # set probability to 100% = always\n        aug.aug_mirror_p = 1.0\n        # rebuild\n        aug.build()\n        ```\n\n    Attributes:\n        refine (bool):                  Boolean, whether clipping to [0,255] should be performed if outside of range.\n        aug_mirror_p (float):           Probability of mirroring application if activated. Default=0.5.\n        aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n        aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n        aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n        aug_gaussianNoise_p (float):    Probability of Gaussian noise application if activated. Default=0.5.\n        aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n        aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n        aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n    \"\"\"\n    # Cache class variables\n    self.image_shape = image_shape\n    self.aug_mirror = mirror\n    self.aug_rotate = rotate\n    self.aug_scale = scale\n    self.aug_elasticTransform = elastic_transform\n    self.aug_gaussianNoise = gaussian_noise\n    self.aug_brightness = brightness\n    self.aug_contrast = contrast\n    self.aug_gamma = gamma\n    # Build augmentation operator\n    self.build()\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_batchgenerators/#aucmedi.data_processing.augmentation.aug_batchgenerators.BatchgeneratorsAugmentation.apply","title":"<code>apply(image)</code>","text":"<p>Performs image augmentation with defined configuration on an image.</p> <p>This internal function is called in the DataGenerator during batch generation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>An image encoded as NumPy array with shape (z, y, x, channels).</p> required <p>Returns:</p> Name Type Description <code>aug_image</code> <code>numpy.ndarray</code> <p>An augmented / transformed image.</p> Source code in <code>aucmedi/data_processing/augmentation/aug_batchgenerators.py</code> <pre><code>def apply(self, image):\n    \"\"\" Performs image augmentation with defined configuration on an image.\n\n    This **internal** function is called in the DataGenerator during batch generation.\n\n    Args:\n        image (numpy.ndarray):          An image encoded as NumPy array with shape (z, y, x, channels).\n    Returns:\n        aug_image (numpy.ndarray):      An augmented / transformed image.\n    \"\"\"\n    # Convert image to batchgenerators format (float32, channel first and with batch axis)\n    image_bg = image.astype(np.float32)\n    image_bg = np.expand_dims(image_bg, axis=0)\n    image_bg = np.moveaxis(image_bg, -1, 1)\n    # Perform image augmentation\n    aug_image = self.operator(data=image_bg)[\"data\"]\n    # Remove batch axis and return to channel last\n    aug_image = np.moveaxis(aug_image, 1, -1)\n    aug_image = np.squeeze(aug_image, axis=0)\n    # Perform clipping if image is out of grayscale/RGB encodings\n    if self.refine and (np.min(aug_image) &lt; 0 or np.max(aug_image) &gt; 255):\n        aug_image = np.clip(aug_image, a_min=0, a_max=255)\n    # Return augmented image\n    return aug_image\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_batchgenerators/#aucmedi.data_processing.augmentation.aug_batchgenerators.BatchgeneratorsAugmentation.build","title":"<code>build()</code>","text":"<p>Builds the batchgenerators augmentator by initializing  all transformations.</p> <p>The activated transformation and their configurations are defined as class variables.</p> <p>-&gt; Builds a new self.operator</p> Source code in <code>aucmedi/data_processing/augmentation/aug_batchgenerators.py</code> <pre><code>def build(self):\n    \"\"\" Builds the batchgenerators augmentator by initializing  all transformations.\n\n    The activated transformation and their configurations are defined as\n    class variables.\n\n    -&gt; Builds a new self.operator\n    \"\"\"\n    # Initialize transform list\n    transforms = []\n    # Fill transform list\n    if self.aug_mirror:\n        tf = MirrorTransform(axes=self.aug_mirror_axes,\n                             p_per_sample=self.aug_mirror_p)\n        transforms.append(tf)\n    if self.aug_contrast:\n        tf = ContrastAugmentationTransform(\n                             self.aug_contrast_range,\n                             preserve_range=self.aug_contrast_preserverange,\n                             per_channel=self.aug_contrast_per_channel,\n                             p_per_sample=self.aug_contrast_p)\n        transforms.append(tf)\n    if self.aug_brightness:\n        tf = BrightnessMultiplicativeTransform(\n                             self.aug_brightness_range,\n                             per_channel=self.aug_brightness_per_channel,\n                             p_per_sample=self.aug_brightness_p)\n        transforms.append(tf)\n    if self.aug_gaussianNoise:\n        tf = GaussianNoiseTransform(self.aug_gaussianNoise_range,\n                                    p_per_sample=self.aug_gaussianNoise_p)\n        transforms.append(tf)\n    if self.aug_gamma:\n        tf = GammaTransform(self.aug_gamma_range,\n                            invert_image=False,\n                            per_channel=self.aug_gamma_per_channel,\n                            retain_stats=True,\n                            p_per_sample=self.aug_gamma_p)\n        transforms.append(tf)\n    if self.aug_rotate or self.aug_scale or self.aug_elasticTransform:\n        tf = SpatialTransform(self.image_shape,\n                              [i // 2 for i in self.image_shape],\n                              do_elastic_deform=self.aug_elasticTransform,\n                              alpha=self.aug_elasticTransform_alpha,\n                              sigma=self.aug_elasticTransform_sigma,\n                              do_rotation=self.aug_rotate,\n                              angle_x=self.aug_rotate_angleX,\n                              angle_y=self.aug_rotate_angleY,\n                              angle_z=self.aug_rotate_angleZ,\n                              do_scale=self.aug_scale,\n                              scale=self.aug_scale_range,\n                              border_mode_data='constant',\n                              border_cval_data=0,\n                              border_mode_seg='constant',\n                              border_cval_seg=0,\n                              order_data=3, order_seg=0,\n                              p_el_per_sample=self.aug_elasticTransform_p,\n                              p_rot_per_sample=self.aug_rotate_p,\n                              p_scale_per_sample=self.aug_scale_p,\n                              random_crop=False)\n        transforms.append(tf)\n\n    # Compose transforms\n    self.operator = Compose(transforms)\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_image/","title":"Aug image","text":""},{"location":"reference/data_processing/augmentation/aug_image/#aucmedi.data_processing.augmentation.aug_image.ImageAugmentation","title":"<code>ImageAugmentation</code>","text":"<p>The Image Augmentation class performs diverse augmentation methods on given     numpy array. The class acts as an easy to use function/interface for applying     all types of augmentations with just one function call.</p> <p>The class can be configured beforehand by selecting desired augmentation techniques and method ranges or strength. Afterwards, the class is passed to the DataGenerator which utilizes it during batch generation.</p> <p>The specific configurations of selected methods can be adjusted by class variables.</p> Build on top of the library <p>Albumentations - https://github.com/albumentations-team/albumentations</p> Source code in <code>aucmedi/data_processing/augmentation/aug_image.py</code> <pre><code>class ImageAugmentation():\n    \"\"\" The Image Augmentation class performs diverse augmentation methods on given\n        numpy array. The class acts as an easy to use function/interface for applying\n        all types of augmentations with just one function call.\n\n    The class can be configured beforehand by selecting desired augmentation techniques\n    and method ranges or strength.\n    Afterwards, the class is passed to the [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator]\n    which utilizes it during batch generation.\n\n    The specific configurations of selected methods can be adjusted by class variables.\n\n    ???+ abstract \"Build on top of the library\"\n        Albumentations - https://github.com/albumentations-team/albumentations\n    \"\"\"\n    #-----------------------------------------------------#\n    #              Augmentation Configuration             #\n    #-----------------------------------------------------#\n    # Define augmentation operator\n    operator = None\n    # Option for augmentation refinement (padding, cropping and clipping)\n    refine = True\n    # Augmentation: Flip\n    aug_flip = False\n    aug_flip_p = 0.5\n    # Augmentation: 90 degree rotate\n    aug_rotate = False\n    aug_rotate_p = 0.5\n    # Augmentation: Brightness\n    aug_brightness = False\n    aug_brightness_p = 0.5\n    aug_brightness_limits = (-0.1, 0.1)\n    # Augmentation: Contrast\n    aug_contrast = False\n    aug_contrast_p = 0.5\n    aug_contrast_limits = (-0.1, 0.1)\n    # Augmentation: Saturation shift\n    aug_saturation = False\n    aug_saturation_p = 0.5\n    aug_saturation_limits = 0.1\n    # Augmentation: Hue shift\n    aug_hue = False\n    aug_hue_p = 0.5\n    aug_hue_limits = (-0.1, 0.1)\n    # Augmentation: Scale\n    aug_scale = False\n    aug_scale_p = 0.5\n    aug_scale_limits = (0.9, 1.1)\n    # Augmentation: Crop\n    aug_crop = False\n    aug_crop_p = 0.5\n    aug_crop_shape = (244, 244)\n    # Augmentation: Grid Distortion\n    aug_gridDistortion = False\n    aug_gridDistortion_p = 0.5\n    # Augmentation: Image Compression (JPEG)\n    aug_compression = False\n    aug_compression_p = 0.5\n    aug_compression_limits = (90, 100)\n    # Augmentation: Gaussian Noise\n    aug_gaussianNoise = False\n    aug_gaussianNoise_p = 0.5\n    # Augmentation: Gaussian Blur\n    aug_gaussianBlur = False\n    aug_gaussianBlur_p = 0.5\n    # Augmentation: Downscale\n    aug_downscaling = False\n    aug_downscaling_p = 0.5\n    aug_downscaling_effect = 0.10\n    # Augmentation: Gamma\n    aug_gamma = False\n    aug_gamma_p = 0.5\n    aug_gamma_limit = (90, 110)\n    # Augmentation: Elastic Transformation\n    aug_elasticTransform = False\n    aug_elasticTransform_p = 0.5\n\n    #-----------------------------------------------------#\n    #                    Initialization                   #\n    #-----------------------------------------------------#\n    def __init__(self, flip=True, rotate=True, brightness=True, contrast=True,\n                 saturation=True, hue=True, scale=True, crop=False,\n                 grid_distortion=False, compression=False, gaussian_noise=False,\n                 gaussian_blur=False, downscaling=False, gamma=False,\n                 elastic_transform=False):\n        \"\"\" Initialization function for the Image Augmentation interface.\n\n        With boolean switches, it is possible to selected desired augmentation techniques.\n        Recommended augmentation configurations are defined as class variables.\n        Of course, these configs can be adjusted if needed.\n\n        Args:\n            flip (bool):                    Boolean, whether flipping should be performed as data augmentation.\n            rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n            brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n            contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n            saturation (bool):              Boolean, whether saturation changes should be added as data augmentation.\n            hue (bool):                     Boolean, whether hue changes should be added as data augmentation.\n            scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n            crop (bool):                    Boolean, whether scaling cropping be performed as data augmentation.\n            grid_distortion (bool):         Boolean, whether grid_distortion should be performed as data augmentation.\n            compression (bool):             Boolean, whether compression should be performed as data augmentation.\n            gaussian_noise (bool):          Boolean, whether gaussian noise should be added as data augmentation.\n            gaussian_blur (bool):           Boolean, whether gaussian blur should be added as data augmentation.\n            downscaling (bool):             Boolean, whether downscaling should be added as data augmentation.\n            gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n            elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n\n        !!! warning\n            If class variables (attributes) are modified, the internal augmentation operator\n            has to be rebuilt via the following call:\n\n            ```python\n            # initialize\n            aug = ImageAugmentation(flip=True)\n\n            # set probability to 100% = always\n            aug.aug_flip_p = 1.0\n            # rebuild\n            aug.build()\n            ```\n\n        Attributes:\n            refine (bool):                  Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.\n            aug_flip_p (float):             Probability of flipping application if activated. Default=0.5.\n            aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n            aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n            aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n            aug_saturation_p (float):       Probability of saturation application if activated. Default=0.5.\n            aug_hue_p (float):              Probability of hue application if activated. Default=0.5.\n            aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n            aug_crop_p (float):             Probability of crop application if activated. Default=0.5.\n            aug_grid_distortion_p (float):  Probability of grid_distortion application if activated. Default=0.5.\n            aug_compression_p (float):      Probability of compression application if activated. Default=0.5.\n            aug_gaussianNoise_p (float):    Probability of gaussian noise application if activated. Default=0.5.\n            aug_gaussianBlur_p (float):     Probability of gaussian blur application if activated. Default=0.5.\n            aug_downscaling_p (float):      Probability of downscaling application if activated. Default=0.5.\n            aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n            aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n        \"\"\"\n        # Cache class variables\n        self.aug_flip = flip\n        self.aug_rotate = rotate\n        self.aug_brightness = brightness\n        self.aug_contrast = contrast\n        self.aug_scale = scale\n        self.aug_crop = crop\n        self.aug_saturation = saturation\n        self.aug_hue = hue\n        self.aug_compression = compression\n        self.aug_gaussianNoise = gaussian_noise\n        self.aug_gaussianBlur= gaussian_blur\n        self.aug_downscaling = downscaling\n        self.aug_gamma = gamma\n        self.aug_gridDistortion = grid_distortion\n        self.aug_elasticTransform = elastic_transform\n        # Build augmentation operator\n        self.build()\n\n    #-----------------------------------------------------#\n    #                Albumentations Builder               #\n    #-----------------------------------------------------#\n    def build(self):\n        \"\"\" Builds the albumenations augmentator by initializing  all transformations.\n\n        The activated transformation and their configurations are defined as\n        class variables.\n\n        -&gt; Builds a new self.operator\n        \"\"\"\n        # Initialize transform list\n        transforms = []\n        # Fill transform list\n        if self.aug_flip:\n            tf_horizontal = ai.HorizontalFlip(p=self.aug_flip_p)\n            tf_vertical = ai.VerticalFlip(p=self.aug_flip_p)\n            transforms.append(tf_horizontal)\n            transforms.append(tf_vertical)\n        if self.aug_rotate:\n            tf = ai.RandomRotate90(p=self.aug_rotate_p)\n            transforms.append(tf)\n        if self.aug_brightness:\n            tf = ai.RandomBrightnessContrast(brightness_limit=self.aug_brightness_limits,\n                                             contrast_limit=0,\n                                             p=self.aug_brightness_p)\n            transforms.append(tf)\n        if self.aug_contrast:\n            tf = ai.RandomBrightnessContrast(contrast_limit=self.aug_contrast_limits,\n                                             brightness_limit=0,\n                                             p=self.aug_contrast_p)\n            transforms.append(tf)\n        if self.aug_saturation:\n            tf = ai.ColorJitter(brightness=0, contrast=0, hue=0,\n                                saturation=self.aug_saturation_limits,\n                                p=self.aug_saturation_p)\n            transforms.append(tf)\n        if self.aug_hue:\n            tf = ai.ColorJitter(brightness=0, contrast=0, saturation=0,\n                                hue=self.aug_hue_limits,\n                                p=self.aug_hue_p)\n            transforms.append(tf)\n        if self.aug_scale:\n            tf = ai.RandomScale(scale_limit=self.aug_scale_limits,\n                                p=self.aug_scale_p)\n            transforms.append(tf)\n        if self.aug_crop:\n            tf = ai.RandomCrop(width=self.aug_crop_shape[0],\n                               height=self.aug_crop_shape[1],\n                               p=self.aug_crop_p)\n            transforms.append(tf)\n        if self.aug_gridDistortion:\n            tf = ai.GridDistortion(p=self.aug_gridDistortion_p)\n            transforms.append(tf)\n        if self.aug_compression:\n            tf = ai.ImageCompression(quality_lower=self.aug_compression_limits[0],\n                                     quality_upper=self.aug_compression_limits[1],\n                                     p=self.aug_compression_p)\n            transforms.append(tf)\n        if self.aug_gaussianNoise:\n            tf = ai.GaussNoise(p=self.aug_gaussianNoise_p)\n            transforms.append(tf)\n        if self.aug_gaussianBlur:\n            tf = ai.GlassBlur(p=self.aug_gaussianBlur_p)\n            transforms.append(tf)\n        if self.aug_downscaling:\n            tf = ai.Downscale(scale_min=self.aug_downscaling_effect,\n                              scale_max=self.aug_downscaling_effect,\n                              p=self.aug_downscaling_p)\n            transforms.append(tf)\n        if self.aug_gamma:\n            tf = ai.RandomGamma(gamma_limit=self.aug_gamma_limit,\n                                p=self.aug_gamma_p)\n            transforms.append(tf)\n        if self.aug_elasticTransform:\n            tf = ai.ElasticTransform(p=self.aug_elasticTransform_p)\n            transforms.append(tf)\n\n        # Compose transforms\n        self.operator = Compose(transforms)\n\n    #-----------------------------------------------------#\n    #                 Perform Augmentation                #\n    #-----------------------------------------------------#\n    def apply(self, image):\n        \"\"\" Performs image augmentation with defined configuration on an image.\n\n        This **internal** function is called in the DataGenerator during batch generation.\n\n        Args:\n            image (numpy.ndarray):          An image encoded as NumPy array with shape (x, y, channels).\n        Returns:\n            aug_image (numpy.ndarray):      An augmented / transformed image.\n        \"\"\"\n        # Verify that image is in grayscale/RGB encoding\n        if np.min(image) &lt; 0 or np.max(image) &gt; 255:\n            warnings.warn(\"Image Augmentation: A value of the image is lower than 0 or higher than 255.\",\n                          \"Albumentations expects images to be in grayscale/RGB!\",\n                          np.min(image), np.max(image))\n        # Cache image shape\n        org_shape = image.shape\n        # Perform image augmentation\n        aug_image = self.operator(image=image)[\"image\"]\n        # Perform padding &amp; cropping if image shape changed\n        if self.refine and aug_image.shape != org_shape:\n            aug_image = ai.PadIfNeeded(min_height=org_shape[0], min_width=org_shape[1], border_mode=cv2.BORDER_REPLICATE, value=0)(image=aug_image)[\"image\"]\n\n            aug_image = ai.RandomCrop(height=org_shape[0], width=org_shape[1])(image=aug_image)[\"image\"]\n        # Perform clipping if image is out of grayscale/RGB encodings\n        if self.refine and (np.min(aug_image) &lt; 0 or np.max(aug_image) &gt; 255):\n            aug_image = np.clip(aug_image, a_min=0, a_max=255)\n        # Return augmented image\n        return aug_image\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_image/#aucmedi.data_processing.augmentation.aug_image.ImageAugmentation.__init__","title":"<code>__init__(flip=True, rotate=True, brightness=True, contrast=True, saturation=True, hue=True, scale=True, crop=False, grid_distortion=False, compression=False, gaussian_noise=False, gaussian_blur=False, downscaling=False, gamma=False, elastic_transform=False)</code>","text":"<p>Initialization function for the Image Augmentation interface.</p> <p>With boolean switches, it is possible to selected desired augmentation techniques. Recommended augmentation configurations are defined as class variables. Of course, these configs can be adjusted if needed.</p> <p>Parameters:</p> Name Type Description Default <code>flip</code> <code>bool</code> <p>Boolean, whether flipping should be performed as data augmentation.</p> <code>True</code> <code>rotate</code> <code>bool</code> <p>Boolean, whether rotations should be performed as data augmentation.</p> <code>True</code> <code>brightness</code> <code>bool</code> <p>Boolean, whether brightness changes should be added as data augmentation.</p> <code>True</code> <code>contrast</code> <code>bool</code> <p>Boolean, whether contrast changes should be added as data augmentation.</p> <code>True</code> <code>saturation</code> <code>bool</code> <p>Boolean, whether saturation changes should be added as data augmentation.</p> <code>True</code> <code>hue</code> <code>bool</code> <p>Boolean, whether hue changes should be added as data augmentation.</p> <code>True</code> <code>scale</code> <code>bool</code> <p>Boolean, whether scaling should be performed as data augmentation.</p> <code>True</code> <code>crop</code> <code>bool</code> <p>Boolean, whether scaling cropping be performed as data augmentation.</p> <code>False</code> <code>grid_distortion</code> <code>bool</code> <p>Boolean, whether grid_distortion should be performed as data augmentation.</p> <code>False</code> <code>compression</code> <code>bool</code> <p>Boolean, whether compression should be performed as data augmentation.</p> <code>False</code> <code>gaussian_noise</code> <code>bool</code> <p>Boolean, whether gaussian noise should be added as data augmentation.</p> <code>False</code> <code>gaussian_blur</code> <code>bool</code> <p>Boolean, whether gaussian blur should be added as data augmentation.</p> <code>False</code> <code>downscaling</code> <code>bool</code> <p>Boolean, whether downscaling should be added as data augmentation.</p> <code>False</code> <code>gamma</code> <code>bool</code> <p>Boolean, whether gamma changes should be added as data augmentation.</p> <code>False</code> <code>elastic_transform</code> <code>bool</code> <p>Boolean, whether elastic deformation should be performed as data augmentation.</p> <code>False</code> <p>Warning</p> <p>If class variables (attributes) are modified, the internal augmentation operator has to be rebuilt via the following call:</p> <pre><code># initialize\naug = ImageAugmentation(flip=True)\n\n# set probability to 100% = always\naug.aug_flip_p = 1.0\n# rebuild\naug.build()\n</code></pre> <p>Attributes:</p> Name Type Description <code>refine</code> <code>bool</code> <p>Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.</p> <code>aug_flip_p</code> <code>float</code> <p>Probability of flipping application if activated. Default=0.5.</p> <code>aug_rotate_p</code> <code>float</code> <p>Probability of rotation application if activated. Default=0.5.</p> <code>aug_brightness_p</code> <code>float</code> <p>Probability of brightness application if activated. Default=0.5.</p> <code>aug_contrast_p</code> <code>float</code> <p>Probability of contrast application if activated. Default=0.5.</p> <code>aug_saturation_p</code> <code>float</code> <p>Probability of saturation application if activated. Default=0.5.</p> <code>aug_hue_p</code> <code>float</code> <p>Probability of hue application if activated. Default=0.5.</p> <code>aug_scale_p</code> <code>float</code> <p>Probability of scaling application if activated. Default=0.5.</p> <code>aug_crop_p</code> <code>float</code> <p>Probability of crop application if activated. Default=0.5.</p> <code>aug_grid_distortion_p</code> <code>float</code> <p>Probability of grid_distortion application if activated. Default=0.5.</p> <code>aug_compression_p</code> <code>float</code> <p>Probability of compression application if activated. Default=0.5.</p> <code>aug_gaussianNoise_p</code> <code>float</code> <p>Probability of gaussian noise application if activated. Default=0.5.</p> <code>aug_gaussianBlur_p</code> <code>float</code> <p>Probability of gaussian blur application if activated. Default=0.5.</p> <code>aug_downscaling_p</code> <code>float</code> <p>Probability of downscaling application if activated. Default=0.5.</p> <code>aug_gamma_p</code> <code>float</code> <p>Probability of gamma application if activated. Default=0.5.</p> <code>aug_elasticTransform_p</code> <code>float</code> <p>Probability of elastic deformation application if activated. Default=0.5.</p> Source code in <code>aucmedi/data_processing/augmentation/aug_image.py</code> <pre><code>def __init__(self, flip=True, rotate=True, brightness=True, contrast=True,\n             saturation=True, hue=True, scale=True, crop=False,\n             grid_distortion=False, compression=False, gaussian_noise=False,\n             gaussian_blur=False, downscaling=False, gamma=False,\n             elastic_transform=False):\n    \"\"\" Initialization function for the Image Augmentation interface.\n\n    With boolean switches, it is possible to selected desired augmentation techniques.\n    Recommended augmentation configurations are defined as class variables.\n    Of course, these configs can be adjusted if needed.\n\n    Args:\n        flip (bool):                    Boolean, whether flipping should be performed as data augmentation.\n        rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n        brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n        contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n        saturation (bool):              Boolean, whether saturation changes should be added as data augmentation.\n        hue (bool):                     Boolean, whether hue changes should be added as data augmentation.\n        scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n        crop (bool):                    Boolean, whether scaling cropping be performed as data augmentation.\n        grid_distortion (bool):         Boolean, whether grid_distortion should be performed as data augmentation.\n        compression (bool):             Boolean, whether compression should be performed as data augmentation.\n        gaussian_noise (bool):          Boolean, whether gaussian noise should be added as data augmentation.\n        gaussian_blur (bool):           Boolean, whether gaussian blur should be added as data augmentation.\n        downscaling (bool):             Boolean, whether downscaling should be added as data augmentation.\n        gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n        elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n\n    !!! warning\n        If class variables (attributes) are modified, the internal augmentation operator\n        has to be rebuilt via the following call:\n\n        ```python\n        # initialize\n        aug = ImageAugmentation(flip=True)\n\n        # set probability to 100% = always\n        aug.aug_flip_p = 1.0\n        # rebuild\n        aug.build()\n        ```\n\n    Attributes:\n        refine (bool):                  Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.\n        aug_flip_p (float):             Probability of flipping application if activated. Default=0.5.\n        aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n        aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n        aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n        aug_saturation_p (float):       Probability of saturation application if activated. Default=0.5.\n        aug_hue_p (float):              Probability of hue application if activated. Default=0.5.\n        aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n        aug_crop_p (float):             Probability of crop application if activated. Default=0.5.\n        aug_grid_distortion_p (float):  Probability of grid_distortion application if activated. Default=0.5.\n        aug_compression_p (float):      Probability of compression application if activated. Default=0.5.\n        aug_gaussianNoise_p (float):    Probability of gaussian noise application if activated. Default=0.5.\n        aug_gaussianBlur_p (float):     Probability of gaussian blur application if activated. Default=0.5.\n        aug_downscaling_p (float):      Probability of downscaling application if activated. Default=0.5.\n        aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n        aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n    \"\"\"\n    # Cache class variables\n    self.aug_flip = flip\n    self.aug_rotate = rotate\n    self.aug_brightness = brightness\n    self.aug_contrast = contrast\n    self.aug_scale = scale\n    self.aug_crop = crop\n    self.aug_saturation = saturation\n    self.aug_hue = hue\n    self.aug_compression = compression\n    self.aug_gaussianNoise = gaussian_noise\n    self.aug_gaussianBlur= gaussian_blur\n    self.aug_downscaling = downscaling\n    self.aug_gamma = gamma\n    self.aug_gridDistortion = grid_distortion\n    self.aug_elasticTransform = elastic_transform\n    # Build augmentation operator\n    self.build()\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_image/#aucmedi.data_processing.augmentation.aug_image.ImageAugmentation.apply","title":"<code>apply(image)</code>","text":"<p>Performs image augmentation with defined configuration on an image.</p> <p>This internal function is called in the DataGenerator during batch generation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>An image encoded as NumPy array with shape (x, y, channels).</p> required <p>Returns:</p> Name Type Description <code>aug_image</code> <code>numpy.ndarray</code> <p>An augmented / transformed image.</p> Source code in <code>aucmedi/data_processing/augmentation/aug_image.py</code> <pre><code>def apply(self, image):\n    \"\"\" Performs image augmentation with defined configuration on an image.\n\n    This **internal** function is called in the DataGenerator during batch generation.\n\n    Args:\n        image (numpy.ndarray):          An image encoded as NumPy array with shape (x, y, channels).\n    Returns:\n        aug_image (numpy.ndarray):      An augmented / transformed image.\n    \"\"\"\n    # Verify that image is in grayscale/RGB encoding\n    if np.min(image) &lt; 0 or np.max(image) &gt; 255:\n        warnings.warn(\"Image Augmentation: A value of the image is lower than 0 or higher than 255.\",\n                      \"Albumentations expects images to be in grayscale/RGB!\",\n                      np.min(image), np.max(image))\n    # Cache image shape\n    org_shape = image.shape\n    # Perform image augmentation\n    aug_image = self.operator(image=image)[\"image\"]\n    # Perform padding &amp; cropping if image shape changed\n    if self.refine and aug_image.shape != org_shape:\n        aug_image = ai.PadIfNeeded(min_height=org_shape[0], min_width=org_shape[1], border_mode=cv2.BORDER_REPLICATE, value=0)(image=aug_image)[\"image\"]\n\n        aug_image = ai.RandomCrop(height=org_shape[0], width=org_shape[1])(image=aug_image)[\"image\"]\n    # Perform clipping if image is out of grayscale/RGB encodings\n    if self.refine and (np.min(aug_image) &lt; 0 or np.max(aug_image) &gt; 255):\n        aug_image = np.clip(aug_image, a_min=0, a_max=255)\n    # Return augmented image\n    return aug_image\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_image/#aucmedi.data_processing.augmentation.aug_image.ImageAugmentation.build","title":"<code>build()</code>","text":"<p>Builds the albumenations augmentator by initializing  all transformations.</p> <p>The activated transformation and their configurations are defined as class variables.</p> <p>-&gt; Builds a new self.operator</p> Source code in <code>aucmedi/data_processing/augmentation/aug_image.py</code> <pre><code>def build(self):\n    \"\"\" Builds the albumenations augmentator by initializing  all transformations.\n\n    The activated transformation and their configurations are defined as\n    class variables.\n\n    -&gt; Builds a new self.operator\n    \"\"\"\n    # Initialize transform list\n    transforms = []\n    # Fill transform list\n    if self.aug_flip:\n        tf_horizontal = ai.HorizontalFlip(p=self.aug_flip_p)\n        tf_vertical = ai.VerticalFlip(p=self.aug_flip_p)\n        transforms.append(tf_horizontal)\n        transforms.append(tf_vertical)\n    if self.aug_rotate:\n        tf = ai.RandomRotate90(p=self.aug_rotate_p)\n        transforms.append(tf)\n    if self.aug_brightness:\n        tf = ai.RandomBrightnessContrast(brightness_limit=self.aug_brightness_limits,\n                                         contrast_limit=0,\n                                         p=self.aug_brightness_p)\n        transforms.append(tf)\n    if self.aug_contrast:\n        tf = ai.RandomBrightnessContrast(contrast_limit=self.aug_contrast_limits,\n                                         brightness_limit=0,\n                                         p=self.aug_contrast_p)\n        transforms.append(tf)\n    if self.aug_saturation:\n        tf = ai.ColorJitter(brightness=0, contrast=0, hue=0,\n                            saturation=self.aug_saturation_limits,\n                            p=self.aug_saturation_p)\n        transforms.append(tf)\n    if self.aug_hue:\n        tf = ai.ColorJitter(brightness=0, contrast=0, saturation=0,\n                            hue=self.aug_hue_limits,\n                            p=self.aug_hue_p)\n        transforms.append(tf)\n    if self.aug_scale:\n        tf = ai.RandomScale(scale_limit=self.aug_scale_limits,\n                            p=self.aug_scale_p)\n        transforms.append(tf)\n    if self.aug_crop:\n        tf = ai.RandomCrop(width=self.aug_crop_shape[0],\n                           height=self.aug_crop_shape[1],\n                           p=self.aug_crop_p)\n        transforms.append(tf)\n    if self.aug_gridDistortion:\n        tf = ai.GridDistortion(p=self.aug_gridDistortion_p)\n        transforms.append(tf)\n    if self.aug_compression:\n        tf = ai.ImageCompression(quality_lower=self.aug_compression_limits[0],\n                                 quality_upper=self.aug_compression_limits[1],\n                                 p=self.aug_compression_p)\n        transforms.append(tf)\n    if self.aug_gaussianNoise:\n        tf = ai.GaussNoise(p=self.aug_gaussianNoise_p)\n        transforms.append(tf)\n    if self.aug_gaussianBlur:\n        tf = ai.GlassBlur(p=self.aug_gaussianBlur_p)\n        transforms.append(tf)\n    if self.aug_downscaling:\n        tf = ai.Downscale(scale_min=self.aug_downscaling_effect,\n                          scale_max=self.aug_downscaling_effect,\n                          p=self.aug_downscaling_p)\n        transforms.append(tf)\n    if self.aug_gamma:\n        tf = ai.RandomGamma(gamma_limit=self.aug_gamma_limit,\n                            p=self.aug_gamma_p)\n        transforms.append(tf)\n    if self.aug_elasticTransform:\n        tf = ai.ElasticTransform(p=self.aug_elasticTransform_p)\n        transforms.append(tf)\n\n    # Compose transforms\n    self.operator = Compose(transforms)\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_volume/","title":"Aug volume","text":""},{"location":"reference/data_processing/augmentation/aug_volume/#aucmedi.data_processing.augmentation.aug_volume.VolumeAugmentation","title":"<code>VolumeAugmentation</code>","text":"<p>The Volume Augmentation class performs diverse augmentation methods on given     numpy array. The class acts as an easy to use function/interface for applying     all types of augmentations with just one function call.</p> <p>The class can be configured beforehand by selecting desired augmentation techniques and method ranges or strength. Afterwards, the class is passed to the DataGenerator which utilizes it during batch generation.</p> <p>The specific configurations of selected methods can be adjusted by class variables.</p> Build on top of the library <p>Volumentations based on albumentations.</p> <pre><code>volumentations: Originally developed by @ashawkey and @ZFTurbo.\n                Initially inspired by albumentations library for augmentation of 2D images.\n                https://github.com/muellerdo/volumentations\n                https://github.com/ZFTurbo/volumentations\n                https://github.com/ashawkey/volumentations\nalbumentations: https://github.com/albumentations-team/albumentations\n</code></pre> <p>The Volumentations package was further continued by us to ensure ongoing development and support.</p> <p>For more details, please read the README under: https://github.com/muellerdo/volumentations</p> Source code in <code>aucmedi/data_processing/augmentation/aug_volume.py</code> <pre><code>class VolumeAugmentation():\n    \"\"\" The Volume Augmentation class performs diverse augmentation methods on given\n        numpy array. The class acts as an easy to use function/interface for applying\n        all types of augmentations with just one function call.\n\n    The class can be configured beforehand by selecting desired augmentation techniques\n    and method ranges or strength.\n    Afterwards, the class is passed to the [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator]\n    which utilizes it during batch generation.\n\n    The specific configurations of selected methods can be adjusted by class variables.\n\n    ???+ abstract \"Build on top of the library\"\n        Volumentations based on albumentations.\n\n        ```\n        volumentations: Originally developed by @ashawkey and @ZFTurbo.\n                        Initially inspired by albumentations library for augmentation of 2D images.\n                        https://github.com/muellerdo/volumentations\n                        https://github.com/ZFTurbo/volumentations\n                        https://github.com/ashawkey/volumentations\n        albumentations: https://github.com/albumentations-team/albumentations\n        ```\n\n        The Volumentations package was further continued by us to ensure ongoing development and support.\n\n        For more details, please read the README under:\n        https://github.com/muellerdo/volumentations\n    \"\"\"\n    #-----------------------------------------------------#\n    #              Augmentation Configuration             #\n    #-----------------------------------------------------#\n    # Define augmentation operator\n    operator = None\n    # Option for augmentation refinement (padding, cropping and clipping)\n    refine = True\n    # Augmentation: Flip\n    aug_flip = False\n    aug_flip_p = 0.5\n    # Augmentation: 90 degree rotate\n    aug_rotate = False\n    aug_rotate_p = 0.5\n    # Augmentation: Brightness\n    aug_brightness = False\n    aug_brightness_p = 0.5\n    aug_brightness_limits = 0.1\n    # Augmentation: Contrast\n    aug_contrast = False\n    aug_contrast_p = 0.5\n    aug_contrast_limits = 0.1\n    # Augmentation: Saturation shift\n    aug_saturation = False\n    aug_saturation_p = 0.5\n    aug_saturation_limits = 0.1\n    # Augmentation: Hue shift\n    aug_hue = False\n    aug_hue_p = 0.5\n    aug_hue_limits = 0.1\n    # Augmentation: Scale\n    aug_scale = False\n    aug_scale_p = 0.5\n    aug_scale_limits = (0.9, 1.1)\n    # Augmentation: Crop\n    aug_crop = False\n    aug_crop_p = 0.5\n    aug_crop_shape = (64, 64, 64)\n    # Augmentation: Grid Distortion\n    aug_gridDistortion = False\n    aug_gridDistortion_p = 0.5\n    # Augmentation: Image Compression (JPEG)\n    aug_compression = False\n    aug_compression_p = 0.5\n    aug_compression_limits = (90, 100)\n    # Augmentation: Gaussian Noise\n    aug_gaussianNoise = False\n    aug_gaussianNoise_p = 0.5\n    # Augmentation: Gaussian Blur\n    aug_gaussianBlur = False\n    aug_gaussianBlur_p = 0.5\n    # Augmentation: Downscale\n    aug_downscaling = False\n    aug_downscaling_p = 0.5\n    aug_downscaling_effect = 0.25\n    # Augmentation: Gamma\n    aug_gamma = False\n    aug_gamma_p = 0.5\n    aug_gamma_limit = (90, 110)\n    # Augmentation: Elastic Transformation\n    aug_elasticTransform = False\n    aug_elasticTransform_p = 0.5\n\n    #-----------------------------------------------------#\n    #                    Initialization                   #\n    #-----------------------------------------------------#\n    def __init__(self, flip=True, rotate=True, brightness=True, contrast=True,\n                 saturation=True, hue=True, scale=True, crop=False,\n                 grid_distortion=False, compression=False, gaussian_noise=False,\n                 gaussian_blur=False, downscaling=False, gamma=False,\n                 elastic_transform=False):\n        \"\"\" Initialization function for the Volume Augmentation interface.\n\n        With boolean switches, it is possible to selected desired augmentation techniques.\n        Recommended augmentation configurations are defined as class variables.\n        Of course, these configs can be adjusted if needed.\n\n        Args:\n            flip (bool):                    Boolean, whether flipping should be performed as data augmentation.\n            rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n            brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n            contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n            saturation (bool):              Boolean, whether saturation changes should be added as data augmentation.\n            hue (bool):                     Boolean, whether hue changes should be added as data augmentation.\n            scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n            crop (bool):                    Boolean, whether scaling cropping be performed as data augmentation.\n            grid_distortion (bool):         Boolean, whether grid_distortion should be performed as data augmentation.\n            compression (bool):             Boolean, whether compression should be performed as data augmentation.\n            gaussian_noise (bool):          Boolean, whether gaussian noise should be added as data augmentation.\n            gaussian_blur (bool):           Boolean, whether gaussian blur should be added as data augmentation.\n            downscaling (bool):             Boolean, whether downscaling should be added as data augmentation.\n            gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n            elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n\n        !!! warning\n            If class variables (attributes) are modified, the internal augmentation operator\n            has to be rebuild via the following call:\n\n            ```python\n            # initialize\n            aug = VolumeAugmentation(flip=True)\n\n            # set probability to 100% = always\n            aug.aug_flip_p = 1.0\n            # rebuild\n            aug.build()\n            ```\n\n        Attributes:\n            refine (bool):                  Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.\n            aug_flip_p (float):             Probability of flipping application if activated. Default=0.5.\n            aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n            aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n            aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n            aug_saturation_p (float):       Probability of saturation application if activated. Default=0.5.\n            aug_hue_p (float):              Probability of hue application if activated. Default=0.5.\n            aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n            aug_crop_p (float):             Probability of crop application if activated. Default=0.5.\n            aug_grid_distortion_p (float):  Probability of grid_distortion application if activated. Default=0.5.\n            aug_compression_p (float):      Probability of compression application if activated. Default=0.5.\n            aug_gaussianNoise_p (float):    Probability of gaussian noise application if activated. Default=0.5.\n            aug_gaussianBlur_p (float):     Probability of gaussian blur application if activated. Default=0.5.\n            aug_downscaling_p (float):      Probability of downscaling application if activated. Default=0.5.\n            aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n            aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n        \"\"\"\n        # Cache class variables\n        self.aug_flip = flip\n        self.aug_rotate = rotate\n        self.aug_brightness = brightness\n        self.aug_contrast = contrast\n        self.aug_scale = scale\n        self.aug_crop = crop\n        self.aug_saturation = saturation\n        self.aug_hue = hue\n        self.aug_compression = compression\n        self.aug_gaussianNoise = gaussian_noise\n        self.aug_gaussianBlur= gaussian_blur\n        self.aug_downscaling = downscaling\n        self.aug_gamma = gamma\n        self.aug_gridDistortion = grid_distortion\n        self.aug_elasticTransform = elastic_transform\n        # Build augmentation operator\n        self.build()\n\n    #-----------------------------------------------------#\n    #                Albumentations Builder               #\n    #-----------------------------------------------------#\n    def build(self):\n        \"\"\" Builds the albumenations augmentator by initializing  all transformations.\n\n        The activated transformation and their configurations are defined as\n        class variables.\n\n        -&gt; Builds a new self.operator\n        \"\"\"\n        # Initialize transform list\n        transforms = []\n        # Fill transform list\n        if self.aug_flip:\n            tf = ai.Flip(p=self.aug_flip_p)\n            transforms.append(tf)\n        if self.aug_rotate:\n            tf = ai.RandomRotate90(p=self.aug_rotate_p)\n            transforms.append(tf)\n        if self.aug_brightness:\n            tf = ai.ColorJitter(brightness=self.aug_brightness_limits,\n                                contrast=0, hue=0, saturation=0,\n                                p=self.aug_brightness_p)\n            transforms.append(tf)\n        if self.aug_contrast:\n            tf = ai.ColorJitter(brightness=0, contrast=self.aug_contrast_limits,\n                                hue=0, saturation=0,\n                                p=self.aug_contrast_p)\n            transforms.append(tf)\n        if self.aug_saturation:\n            tf = ai.ColorJitter(brightness=0, contrast=0, hue=0,\n                                saturation=self.aug_saturation_limits,\n                                p=self.aug_saturation_p)\n            transforms.append(tf)\n        if self.aug_hue:\n            tf = ai.ColorJitter(brightness=0, contrast=0, saturation=0,\n                                hue=self.aug_hue_limits,\n                                p=self.aug_hue_p)\n            transforms.append(tf)\n        if self.aug_scale:\n            tf = ai.RandomScale(scale_limit=self.aug_scale_limits,\n                                p=self.aug_scale_p)\n            transforms.append(tf)\n        if self.aug_crop:\n            tf = ai.RandomCrop(self.aug_crop_shape,\n                               p=self.aug_crop_p)\n            transforms.append(tf)\n        if self.aug_gridDistortion:\n            tf = ai.GridDistortion(p=self.aug_gridDistortion_p)\n            transforms.append(tf)\n        if self.aug_compression:\n            tf = ai.ImageCompression(quality_lower=self.aug_compression_limits[0],\n                                     quality_upper=self.aug_compression_limits[1],\n                                     p=self.aug_compression_p)\n            transforms.append(tf)\n        if self.aug_gaussianNoise:\n            tf = ai.GaussianNoise(p=self.aug_gaussianNoise_p)\n            transforms.append(tf)\n        if self.aug_gaussianBlur:\n            tf = ai.GlassBlur(p=self.aug_gaussianBlur_p)\n            transforms.append(tf)\n        if self.aug_downscaling:\n            tf = ai.Downscale(scale_min=self.aug_downscaling_effect,\n                              scale_max=self.aug_downscaling_effect,\n                              p=self.aug_downscaling_p)\n            transforms.append(tf)\n        if self.aug_gamma:\n            tf = ai.RandomGamma(gamma_limit=self.aug_gamma_limit,\n                                p=self.aug_gamma_p)\n            transforms.append(tf)\n        if self.aug_elasticTransform:\n            tf = ai.ElasticTransform(p=self.aug_elasticTransform_p)\n            transforms.append(tf)\n\n        # Compose transforms\n        self.operator = Compose(transforms)\n\n    #-----------------------------------------------------#\n    #                 Perform Augmentation                #\n    #-----------------------------------------------------#\n    \"\"\" Performs image augmentation with defined configuration on an image.\n\n    This **internal** function is called in the DataGenerator during batch generation.\n\n    Args:\n        image (numpy.ndarray):          An image encoded as NumPy array with shape (z, y, x, channels).\n    Returns:\n        aug_image (numpy.ndarray):      An augmented / transformed image.\n    \"\"\"\n    def apply(self, image):\n        # Verify that image is in grayscale/RGB encoding\n        if np.min(image) &lt; 0 or np.max(image) &gt; 255:\n            warnings.warn(\"Image Augmentation: A value of the image is lower than 0 or higher than 255.\",\n                          \"Volumentations expects images to be in grayscale/RGB!\",\n                          np.min(image), np.max(image))\n        # Cache image shape\n        org_shape = image.shape\n        # Perform image augmentation\n        aug_image = self.operator(image=image)[\"image\"]\n        # Perform padding &amp; cropping if image shape changed\n        if self.refine and aug_image.shape != org_shape:\n            aug_image = ai.pad(aug_image, new_shape=org_shape)\n            offset = (random.random(), random.random(), random.random())\n            aug_image = ai.random_crop(aug_image,\n                                       org_shape[0], org_shape[1], org_shape[2],\n                                       offset[0], offset[1], offset[2])\n        # Perform clipping if image is out of grayscale/RGB encodings\n        if self.refine and (np.min(aug_image) &lt; 0 or np.max(aug_image) &gt; 255):\n            aug_image = np.clip(aug_image, a_min=0, a_max=255)\n        # Return augmented image\n        return aug_image\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_volume/#aucmedi.data_processing.augmentation.aug_volume.VolumeAugmentation.__init__","title":"<code>__init__(flip=True, rotate=True, brightness=True, contrast=True, saturation=True, hue=True, scale=True, crop=False, grid_distortion=False, compression=False, gaussian_noise=False, gaussian_blur=False, downscaling=False, gamma=False, elastic_transform=False)</code>","text":"<p>Initialization function for the Volume Augmentation interface.</p> <p>With boolean switches, it is possible to selected desired augmentation techniques. Recommended augmentation configurations are defined as class variables. Of course, these configs can be adjusted if needed.</p> <p>Parameters:</p> Name Type Description Default <code>flip</code> <code>bool</code> <p>Boolean, whether flipping should be performed as data augmentation.</p> <code>True</code> <code>rotate</code> <code>bool</code> <p>Boolean, whether rotations should be performed as data augmentation.</p> <code>True</code> <code>brightness</code> <code>bool</code> <p>Boolean, whether brightness changes should be added as data augmentation.</p> <code>True</code> <code>contrast</code> <code>bool</code> <p>Boolean, whether contrast changes should be added as data augmentation.</p> <code>True</code> <code>saturation</code> <code>bool</code> <p>Boolean, whether saturation changes should be added as data augmentation.</p> <code>True</code> <code>hue</code> <code>bool</code> <p>Boolean, whether hue changes should be added as data augmentation.</p> <code>True</code> <code>scale</code> <code>bool</code> <p>Boolean, whether scaling should be performed as data augmentation.</p> <code>True</code> <code>crop</code> <code>bool</code> <p>Boolean, whether scaling cropping be performed as data augmentation.</p> <code>False</code> <code>grid_distortion</code> <code>bool</code> <p>Boolean, whether grid_distortion should be performed as data augmentation.</p> <code>False</code> <code>compression</code> <code>bool</code> <p>Boolean, whether compression should be performed as data augmentation.</p> <code>False</code> <code>gaussian_noise</code> <code>bool</code> <p>Boolean, whether gaussian noise should be added as data augmentation.</p> <code>False</code> <code>gaussian_blur</code> <code>bool</code> <p>Boolean, whether gaussian blur should be added as data augmentation.</p> <code>False</code> <code>downscaling</code> <code>bool</code> <p>Boolean, whether downscaling should be added as data augmentation.</p> <code>False</code> <code>gamma</code> <code>bool</code> <p>Boolean, whether gamma changes should be added as data augmentation.</p> <code>False</code> <code>elastic_transform</code> <code>bool</code> <p>Boolean, whether elastic deformation should be performed as data augmentation.</p> <code>False</code> <p>Warning</p> <p>If class variables (attributes) are modified, the internal augmentation operator has to be rebuild via the following call:</p> <pre><code># initialize\naug = VolumeAugmentation(flip=True)\n\n# set probability to 100% = always\naug.aug_flip_p = 1.0\n# rebuild\naug.build()\n</code></pre> <p>Attributes:</p> Name Type Description <code>refine</code> <code>bool</code> <p>Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.</p> <code>aug_flip_p</code> <code>float</code> <p>Probability of flipping application if activated. Default=0.5.</p> <code>aug_rotate_p</code> <code>float</code> <p>Probability of rotation application if activated. Default=0.5.</p> <code>aug_brightness_p</code> <code>float</code> <p>Probability of brightness application if activated. Default=0.5.</p> <code>aug_contrast_p</code> <code>float</code> <p>Probability of contrast application if activated. Default=0.5.</p> <code>aug_saturation_p</code> <code>float</code> <p>Probability of saturation application if activated. Default=0.5.</p> <code>aug_hue_p</code> <code>float</code> <p>Probability of hue application if activated. Default=0.5.</p> <code>aug_scale_p</code> <code>float</code> <p>Probability of scaling application if activated. Default=0.5.</p> <code>aug_crop_p</code> <code>float</code> <p>Probability of crop application if activated. Default=0.5.</p> <code>aug_grid_distortion_p</code> <code>float</code> <p>Probability of grid_distortion application if activated. Default=0.5.</p> <code>aug_compression_p</code> <code>float</code> <p>Probability of compression application if activated. Default=0.5.</p> <code>aug_gaussianNoise_p</code> <code>float</code> <p>Probability of gaussian noise application if activated. Default=0.5.</p> <code>aug_gaussianBlur_p</code> <code>float</code> <p>Probability of gaussian blur application if activated. Default=0.5.</p> <code>aug_downscaling_p</code> <code>float</code> <p>Probability of downscaling application if activated. Default=0.5.</p> <code>aug_gamma_p</code> <code>float</code> <p>Probability of gamma application if activated. Default=0.5.</p> <code>aug_elasticTransform_p</code> <code>float</code> <p>Probability of elastic deformation application if activated. Default=0.5.</p> Source code in <code>aucmedi/data_processing/augmentation/aug_volume.py</code> <pre><code>def __init__(self, flip=True, rotate=True, brightness=True, contrast=True,\n             saturation=True, hue=True, scale=True, crop=False,\n             grid_distortion=False, compression=False, gaussian_noise=False,\n             gaussian_blur=False, downscaling=False, gamma=False,\n             elastic_transform=False):\n    \"\"\" Initialization function for the Volume Augmentation interface.\n\n    With boolean switches, it is possible to selected desired augmentation techniques.\n    Recommended augmentation configurations are defined as class variables.\n    Of course, these configs can be adjusted if needed.\n\n    Args:\n        flip (bool):                    Boolean, whether flipping should be performed as data augmentation.\n        rotate (bool):                  Boolean, whether rotations should be performed as data augmentation.\n        brightness (bool):              Boolean, whether brightness changes should be added as data augmentation.\n        contrast (bool):                Boolean, whether contrast changes should be added as data augmentation.\n        saturation (bool):              Boolean, whether saturation changes should be added as data augmentation.\n        hue (bool):                     Boolean, whether hue changes should be added as data augmentation.\n        scale (bool):                   Boolean, whether scaling should be performed as data augmentation.\n        crop (bool):                    Boolean, whether scaling cropping be performed as data augmentation.\n        grid_distortion (bool):         Boolean, whether grid_distortion should be performed as data augmentation.\n        compression (bool):             Boolean, whether compression should be performed as data augmentation.\n        gaussian_noise (bool):          Boolean, whether gaussian noise should be added as data augmentation.\n        gaussian_blur (bool):           Boolean, whether gaussian blur should be added as data augmentation.\n        downscaling (bool):             Boolean, whether downscaling should be added as data augmentation.\n        gamma (bool):                   Boolean, whether gamma changes should be added as data augmentation.\n        elastic_transform (bool):       Boolean, whether elastic deformation should be performed as data augmentation.\n\n    !!! warning\n        If class variables (attributes) are modified, the internal augmentation operator\n        has to be rebuild via the following call:\n\n        ```python\n        # initialize\n        aug = VolumeAugmentation(flip=True)\n\n        # set probability to 100% = always\n        aug.aug_flip_p = 1.0\n        # rebuild\n        aug.build()\n        ```\n\n    Attributes:\n        refine (bool):                  Boolean, whether clipping to [0,255] and padding/cropping should be performed if outside of range.\n        aug_flip_p (float):             Probability of flipping application if activated. Default=0.5.\n        aug_rotate_p (float):           Probability of rotation application if activated. Default=0.5.\n        aug_brightness_p (float):       Probability of brightness application if activated. Default=0.5.\n        aug_contrast_p (float):         Probability of contrast application if activated. Default=0.5.\n        aug_saturation_p (float):       Probability of saturation application if activated. Default=0.5.\n        aug_hue_p (float):              Probability of hue application if activated. Default=0.5.\n        aug_scale_p (float):            Probability of scaling application if activated. Default=0.5.\n        aug_crop_p (float):             Probability of crop application if activated. Default=0.5.\n        aug_grid_distortion_p (float):  Probability of grid_distortion application if activated. Default=0.5.\n        aug_compression_p (float):      Probability of compression application if activated. Default=0.5.\n        aug_gaussianNoise_p (float):    Probability of gaussian noise application if activated. Default=0.5.\n        aug_gaussianBlur_p (float):     Probability of gaussian blur application if activated. Default=0.5.\n        aug_downscaling_p (float):      Probability of downscaling application if activated. Default=0.5.\n        aug_gamma_p (float):            Probability of gamma application if activated. Default=0.5.\n        aug_elasticTransform_p (float): Probability of elastic deformation application if activated. Default=0.5.\n    \"\"\"\n    # Cache class variables\n    self.aug_flip = flip\n    self.aug_rotate = rotate\n    self.aug_brightness = brightness\n    self.aug_contrast = contrast\n    self.aug_scale = scale\n    self.aug_crop = crop\n    self.aug_saturation = saturation\n    self.aug_hue = hue\n    self.aug_compression = compression\n    self.aug_gaussianNoise = gaussian_noise\n    self.aug_gaussianBlur= gaussian_blur\n    self.aug_downscaling = downscaling\n    self.aug_gamma = gamma\n    self.aug_gridDistortion = grid_distortion\n    self.aug_elasticTransform = elastic_transform\n    # Build augmentation operator\n    self.build()\n</code></pre>"},{"location":"reference/data_processing/augmentation/aug_volume/#aucmedi.data_processing.augmentation.aug_volume.VolumeAugmentation.build","title":"<code>build()</code>","text":"<p>Builds the albumenations augmentator by initializing  all transformations.</p> <p>The activated transformation and their configurations are defined as class variables.</p> <p>-&gt; Builds a new self.operator</p> Source code in <code>aucmedi/data_processing/augmentation/aug_volume.py</code> <pre><code>def build(self):\n    \"\"\" Builds the albumenations augmentator by initializing  all transformations.\n\n    The activated transformation and their configurations are defined as\n    class variables.\n\n    -&gt; Builds a new self.operator\n    \"\"\"\n    # Initialize transform list\n    transforms = []\n    # Fill transform list\n    if self.aug_flip:\n        tf = ai.Flip(p=self.aug_flip_p)\n        transforms.append(tf)\n    if self.aug_rotate:\n        tf = ai.RandomRotate90(p=self.aug_rotate_p)\n        transforms.append(tf)\n    if self.aug_brightness:\n        tf = ai.ColorJitter(brightness=self.aug_brightness_limits,\n                            contrast=0, hue=0, saturation=0,\n                            p=self.aug_brightness_p)\n        transforms.append(tf)\n    if self.aug_contrast:\n        tf = ai.ColorJitter(brightness=0, contrast=self.aug_contrast_limits,\n                            hue=0, saturation=0,\n                            p=self.aug_contrast_p)\n        transforms.append(tf)\n    if self.aug_saturation:\n        tf = ai.ColorJitter(brightness=0, contrast=0, hue=0,\n                            saturation=self.aug_saturation_limits,\n                            p=self.aug_saturation_p)\n        transforms.append(tf)\n    if self.aug_hue:\n        tf = ai.ColorJitter(brightness=0, contrast=0, saturation=0,\n                            hue=self.aug_hue_limits,\n                            p=self.aug_hue_p)\n        transforms.append(tf)\n    if self.aug_scale:\n        tf = ai.RandomScale(scale_limit=self.aug_scale_limits,\n                            p=self.aug_scale_p)\n        transforms.append(tf)\n    if self.aug_crop:\n        tf = ai.RandomCrop(self.aug_crop_shape,\n                           p=self.aug_crop_p)\n        transforms.append(tf)\n    if self.aug_gridDistortion:\n        tf = ai.GridDistortion(p=self.aug_gridDistortion_p)\n        transforms.append(tf)\n    if self.aug_compression:\n        tf = ai.ImageCompression(quality_lower=self.aug_compression_limits[0],\n                                 quality_upper=self.aug_compression_limits[1],\n                                 p=self.aug_compression_p)\n        transforms.append(tf)\n    if self.aug_gaussianNoise:\n        tf = ai.GaussianNoise(p=self.aug_gaussianNoise_p)\n        transforms.append(tf)\n    if self.aug_gaussianBlur:\n        tf = ai.GlassBlur(p=self.aug_gaussianBlur_p)\n        transforms.append(tf)\n    if self.aug_downscaling:\n        tf = ai.Downscale(scale_min=self.aug_downscaling_effect,\n                          scale_max=self.aug_downscaling_effect,\n                          p=self.aug_downscaling_p)\n        transforms.append(tf)\n    if self.aug_gamma:\n        tf = ai.RandomGamma(gamma_limit=self.aug_gamma_limit,\n                            p=self.aug_gamma_p)\n        transforms.append(tf)\n    if self.aug_elasticTransform:\n        tf = ai.ElasticTransform(p=self.aug_elasticTransform_p)\n        transforms.append(tf)\n\n    # Compose transforms\n    self.operator = Compose(transforms)\n</code></pre>"},{"location":"reference/data_processing/io_interfaces/","title":"Io interfaces","text":"<p>The IO Interfaces of AUCMEDI allow extracting information from datasets in different     structures or formats.</p> <p>These interfaces are called internally via the input_interface.</p> Format Interfaces Interface Internal Function Description <code>\"csv\"</code> io_csv() Storing class annotations in a CSV file. <code>\"directory\"</code> io_directory() Storing class annotations in subdirectories. <code>\"json\"</code> io_json() Storing class annotations in a JSON file."},{"location":"reference/data_processing/io_interfaces/io_csv/","title":"Io csv","text":""},{"location":"reference/data_processing/io_interfaces/io_csv/#aucmedi.data_processing.io_interfaces.io_csv.csv_loader","title":"<code>csv_loader(path_data, path_imagedir, allowed_image_formats, training=True, ohe=True, ohe_range=None, col_sample='SAMPLE', col_class='CLASS')</code>","text":"<p>Data Input Interface for loading a dataset via a CSV and an image directory.</p> <p>This internal function allows simple parsing of class annotations encoded in a CSV, and can be called via the input_interface() function by passing <code>\"csv\"</code> as parameter <code>interface</code>.</p> Input Formats <pre><code>CSV Format 1:\n   - Name Column: \"SAMPLE\" -&gt; String Value\n   - Class Column: \"CLASS\" -&gt; Sparse Categorical Classes (String/Integers)\n   - Optional Meta Columns possible\n\nCSV Format 2:\n   - Name Column: \"SAMPLE\"\n   - One-Hot Encoded Class Columns:\n       -&gt; If OHE parameter provides list of column names -&gt; use these\n       -&gt; Else try to use all other columns as OHE columns\n   - Optional Meta Columns only possible if OHE parameter provided\n</code></pre> <p>Expected structure: <pre><code>dataset/\n    images_dir/                 # path_imagedir = \"dataset/images_dir\"\n        sample001.png\n        sample002.png\n        ...\n        sample350.png\n    annotations.csv             # path_data = \"dataset/annotations.csv\"\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path_data</code> <code>str</code> <p>Path to the csv file.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>allowed_image_formats</code> <code>list of str</code> <p>List of allowed imaging formats. (provided by IO_Interface)</p> required <code>training</code> <code>bool</code> <p>Boolean option whether annotation data is available.</p> <code>True</code> <code>ohe</code> <code>bool</code> <p>Boolean option whether annotation data is sparse categorical or one-hot encoded.</p> <code>True</code> <code>ohe_range</code> <code>list of str</code> <p>List of column name values if annotation encoded in OHE. Example: [\"classA\", \"classB\", \"classC\"]</p> <code>None</code> <code>col_sample</code> <code>str</code> <p>Index column name for the sample name column. Default: 'SAMPLE'</p> <code>'SAMPLE'</code> <code>col_class</code> <code>str</code> <p>Index column name for the sparse categorical classes column. Default: 'CLASS'</p> <code>'CLASS'</code> <p>Returns:</p> Name Type Description <code>index_list</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Required in DataGenerator as <code>samples</code>.</p> <code>class_ohe</code> <code>numpy.ndarray</code> <p>Classification list as One-Hot encoding. Required in DataGenerator as <code>labels</code>.</p> <code>class_n</code> <code>int</code> <p>Number of classes. Required in NeuralNetwork for Architecture design as <code>n_labels</code>.</p> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for later prediction storage or evaluation.</p> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading. Required in DataGenerator.</p> Source code in <code>aucmedi/data_processing/io_interfaces/io_csv.py</code> <pre><code>def csv_loader(path_data, path_imagedir, allowed_image_formats,\n               training=True, ohe=True, ohe_range=None,\n               col_sample=\"SAMPLE\", col_class=\"CLASS\"):\n    \"\"\" Data Input Interface for loading a dataset via a CSV and an image directory.\n\n    This **internal** function allows simple parsing of class annotations encoded in a CSV,\n    and can be called via the [input_interface()][aucmedi.data_processing.io_data.input_interface]\n    function by passing `\"csv\"` as parameter `interface`.\n\n    ???+ info \"Input Formats\"\n        ```\n        CSV Format 1:\n           - Name Column: \"SAMPLE\" -&gt; String Value\n           - Class Column: \"CLASS\" -&gt; Sparse Categorical Classes (String/Integers)\n           - Optional Meta Columns possible\n\n        CSV Format 2:\n           - Name Column: \"SAMPLE\"\n           - One-Hot Encoded Class Columns:\n               -&gt; If OHE parameter provides list of column names -&gt; use these\n               -&gt; Else try to use all other columns as OHE columns\n           - Optional Meta Columns only possible if OHE parameter provided\n        ```\n\n    **Expected structure:**\n    ```\n    dataset/\n        images_dir/                 # path_imagedir = \"dataset/images_dir\"\n            sample001.png\n            sample002.png\n            ...\n            sample350.png\n        annotations.csv             # path_data = \"dataset/annotations.csv\"\n    ```\n\n    Args:\n        path_data (str):                        Path to the csv file.\n        path_imagedir (str):                    Path to the directory containing the images.\n        allowed_image_formats (list of str):    List of allowed imaging formats. (provided by IO_Interface)\n        training (bool):                        Boolean option whether annotation data is available.\n        ohe (bool):                             Boolean option whether annotation data is sparse categorical or one-hot encoded.\n        ohe_range (list of str):                List of column name values if annotation encoded in OHE. Example: [\"classA\", \"classB\", \"classC\"]\n        col_sample (str):                       Index column name for the sample name column. Default: 'SAMPLE'\n        col_class (str):                        Index column name for the sparse categorical classes column. Default: 'CLASS'\n\n    Returns:\n        index_list (list of str):               List of sample/index encoded as Strings. Required in DataGenerator as `samples`.\n        class_ohe (numpy.ndarray):              Classification list as One-Hot encoding. Required in DataGenerator as `labels`.\n        class_n (int):                          Number of classes. Required in NeuralNetwork for Architecture design as `n_labels`.\n        class_names (list of str):              List of names for corresponding classes. Used for later prediction storage or evaluation.\n        image_format (str):                     Image format to add at the end of the sample index for image loading. Required in DataGenerator.\n    \"\"\"\n    # Load CSV file\n    dt = pd.read_csv(path_data, sep=\",\", header=0)\n    # Check if image index column exist and parse it\n    if col_sample in dt.columns : index_list = dt[col_sample].tolist()\n    else : raise Exception(\"Sample column (\" + str(col_sample) + \\\n                           \") not available in CSV file!\", path_data)\n    # Ensure index list to contain strings\n    index_list = [str(index) for index in index_list]\n    # Identify image format by peaking first image\n    image_format = None\n    for file in os.listdir(path_imagedir):\n        format = file.split(\".\")[-1]\n        if format.lower() in allowed_image_formats or \\\n           format.upper() in allowed_image_formats:\n           image_format = format\n           break\n    # Raise Exception if image format is unknown\n    if image_format is None:\n        raise Exception(\"Unknown image format.\", path_imagedir)\n    # Check if image ending is already in sample name by peaking first one\n    if index_list[0].endswith(\".\" + image_format) : image_format = None\n    # Verify if all images are existing\n    for sample in index_list:\n        # Obtain image file path\n        if image_format : img_file = sample + \".\" + image_format\n        else : img_file = sample\n        path_img = os.path.join(path_imagedir, img_file)\n        # Check existance\n        if not os.path.exists(path_img):\n            raise Exception(\"Image does not exist / not accessible!\",\n                            'Sample: \"' + sample + '\"', path_img)\n\n    # If CSV is for inference (no annotation data) -&gt; return parsing\n    if not training : return index_list, None, None, None, image_format\n\n    # Try parsing with a sparse categorical class format (CSV Format 1)\n    if not ohe:\n        # Verify if provided classification column in in dataframe\n        if col_class not in dt.columns:\n            raise Exception(\"Provided classification column not in dataset!\")\n        # Obtain class information\n        classes_sparse = dt[col_class].tolist()\n        class_names = np.unique(classes_sparse).tolist()\n        class_n = len(class_names)\n        # Parse sparse categorical annotations to One-Hot Encoding\n        class_ohe = pd.get_dummies(classes_sparse).to_numpy()\n    # Try parsing one-hot encoded format (CSV Format 2)\n    else:\n        # Identify OHE columns\n        if ohe_range is None : ohe_columns = dt.loc[:, dt.columns != col_sample]\n        else : ohe_columns = dt.loc[:, ohe_range]\n        # Parse information\n        class_names = list(ohe_columns.columns)\n        class_n = len(class_names)\n        class_ohe = ohe_columns.to_numpy()\n\n    # Validate if number of samples and number of annotations match\n    if len(index_list) != len(class_ohe):\n        raise Exception(\"Numbers of samples and annotations do not match!\",\n                        len(index_list), len(class_ohe))\n    # Return parsed CSV data\n    return index_list, class_ohe, class_n, class_names, image_format\n</code></pre>"},{"location":"reference/data_processing/io_interfaces/io_directory/","title":"Io directory","text":""},{"location":"reference/data_processing/io_interfaces/io_directory/#aucmedi.data_processing.io_interfaces.io_directory.directory_loader","title":"<code>directory_loader(path_imagedir, allowed_image_formats, training=True)</code>","text":"<p>Data Input Interface for loading a dataset in a directory-based structure.</p> <p>This internal function allows simple parsing of class annotations encoded in subdirectories.</p> Input Formats <pre><code>Format Directory - Training:\n    - Class annotations are encoded via subdirectories\n    - Images are provided in subdirectories\n\nFormat Directory - Testing:\n    - All images are provided in the directory\n    - No class annotations\n</code></pre> <p>Expected structure for training: <pre><code>images_dir/                     # path_imagedir = \"dataset/images_dir\"\n    class_A/\n        sample001.png\n        sample002.png\n        ...\n        sample050.png\n    class_B/                    # Directory / class names can be any String\n        sample051.png           # like \"diabetes\", \"cancer\", ...\n        sample052.png\n        ...\n        sample100.png\n    ...\n    class_C/\n        sample101.png           # Sample names (indicies) should be unique!\n        sample102.png\n        ...\n        sample150.png\n</code></pre></p> <p>Expected structure for testing: <pre><code>images_dir/                     # path_imagedir = \"dataset/images_dir\"\n    sample001.png\n    sample002.png\n    ...\n    sample100.png\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images or the subdirectories.</p> required <code>allowed_image_formats</code> <code>list of str</code> <p>List of allowed imaging formats. (provided by IO_Interface)</p> required <code>training</code> <code>bool</code> <p>Boolean option whether annotation data is available.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>index_list</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Required in DataGenerator as <code>samples</code>.</p> <code>class_ohe</code> <code>numpy.ndarray</code> <p>Classification list as One-Hot encoding. Required in DataGenerator as <code>labels</code>.</p> <code>class_n</code> <code>int</code> <p>Number of classes. Required in NeuralNetwork for Architecture design as <code>n_labels</code>.</p> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for later prediction storage or evaluation.</p> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading. Required in DataGenerator.</p> Source code in <code>aucmedi/data_processing/io_interfaces/io_directory.py</code> <pre><code>def directory_loader(path_imagedir, allowed_image_formats, training=True):\n    \"\"\" Data Input Interface for loading a dataset in a directory-based structure.\n\n    This **internal** function allows simple parsing of class annotations encoded in subdirectories.\n\n    ???+ info \"Input Formats\"\n        ```\n        Format Directory - Training:\n            - Class annotations are encoded via subdirectories\n            - Images are provided in subdirectories\n\n        Format Directory - Testing:\n            - All images are provided in the directory\n            - No class annotations\n        ```\n\n    **Expected structure for training:**\n    ```\n    images_dir/                     # path_imagedir = \"dataset/images_dir\"\n        class_A/\n            sample001.png\n            sample002.png\n            ...\n            sample050.png\n        class_B/                    # Directory / class names can be any String\n            sample051.png           # like \"diabetes\", \"cancer\", ...\n            sample052.png\n            ...\n            sample100.png\n        ...\n        class_C/\n            sample101.png           # Sample names (indicies) should be unique!\n            sample102.png\n            ...\n            sample150.png\n    ```\n\n    **Expected structure for testing:**\n    ```\n    images_dir/                     # path_imagedir = \"dataset/images_dir\"\n        sample001.png\n        sample002.png\n        ...\n        sample100.png\n    ```\n\n    Args:\n        path_imagedir (str):                    Path to the directory containing the images or the subdirectories.\n        allowed_image_formats (list of str):    List of allowed imaging formats. (provided by IO_Interface)\n        training (bool):                        Boolean option whether annotation data is available.\n\n    Returns:\n        index_list (list of str):               List of sample/index encoded as Strings. Required in DataGenerator as `samples`.\n        class_ohe (numpy.ndarray):              Classification list as One-Hot encoding. Required in DataGenerator as `labels`.\n        class_n (int):                          Number of classes. Required in NeuralNetwork for Architecture design as `n_labels`.\n        class_names (list of str):              List of names for corresponding classes. Used for later prediction storage or evaluation.\n        image_format (str):                     Image format to add at the end of the sample index for image loading. Required in DataGenerator.\n    \"\"\"\n    # Initialize some variables\n    image_format = None\n    index_list = []\n    # Format - including class annotations encoded via subdirectories\n    if training:\n        class_names = []\n        classes_sparse = []\n        # Iterate over subdirectories\n        for c, subdirectory in enumerate(sorted(os.listdir(path_imagedir))):\n            # Skip items which are not a directory (metadata)\n            if not os.path.isdir(os.path.join(path_imagedir, subdirectory)):\n                continue\n            class_names.append(subdirectory)\n            # Iterate over each sample\n            path_sd = os.path.join(path_imagedir, subdirectory)\n            for file in sorted(os.listdir(path_sd)):\n                sample = os.path.join(subdirectory, file)\n                index_list.append(sample)\n                classes_sparse.append(c)\n        # Parse sparse categorical annotations to One-Hot Encoding\n        class_n = len(class_names)\n        class_ohe = pd.get_dummies(classes_sparse).to_numpy()\n        # Return parsing\n        return index_list, class_ohe, class_n, class_names, image_format\n    # Format - excluding class annotations -&gt; only testing images\n    else:\n        # Iterate over all images\n        for file in sorted(os.listdir(path_imagedir)):\n            # Identify image format by peaking first image\n            if image_format is None:\n                format = file.split(\".\")[-1]\n                if format.lower() in allowed_image_formats or \\\n                   format.upper() in allowed_image_formats:\n                   image_format = format\n            # Add sample to list\n            index_list.append(file[:-(len(format)+1)])\n        # Raise Exception if image format is unknown\n        if image_format is None:\n            raise Exception(\"Unknown image format.\", path_imagedir)\n        # Return parsing\n        return index_list, None, None, None, image_format\n</code></pre>"},{"location":"reference/data_processing/io_interfaces/io_json/","title":"Io json","text":""},{"location":"reference/data_processing/io_interfaces/io_json/#aucmedi.data_processing.io_interfaces.io_json.json_loader","title":"<code>json_loader(path_data, path_imagedir, allowed_image_formats, training=True, ohe=True)</code>","text":"<p>Data Input Interface for loading a dataset via a JSON and an image directory.</p> <p>This internal function allows simple parsing of class annotations encoded in a JSON.</p> Input Formats <pre><code>Format Sparse:\n    - Name Index (key) : Class (value)\n\nFormat One-Hot Encoded:\n    - Name Index (key) : List consisting of binary integers.\n</code></pre> <p>Expected structure: <pre><code>dataset/\n    images_dir/                 # path_imagedir = \"dataset/images_dir\"\n        sample001.png\n        sample002.png\n        ...\n        sample350.png\n    annotations.json            # path_data = \"dataset/annotations.json\"\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path_data</code> <code>str</code> <p>Path to the json file.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>allowed_image_formats</code> <code>list of str</code> <p>List of allowed imaging formats. (provided by IO_Interface)</p> required <code>training</code> <code>bool</code> <p>Boolean option whether annotation data is available.</p> <code>True</code> <code>ohe</code> <code>bool</code> <p>Boolean option whether annotation data is sparse categorical or one-hot encoded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>index_list</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Required in DataGenerator as <code>samples</code>.</p> <code>class_ohe</code> <code>numpy.ndarray</code> <p>Classification list as One-Hot encoding. Required in DataGenerator as <code>labels</code>.</p> <code>class_n</code> <code>int</code> <p>Number of classes. Required in NeuralNetwork for Architecture design as <code>n_labels</code>.</p> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for later prediction storage or evaluation.</p> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading. Required in DataGenerator.</p> Source code in <code>aucmedi/data_processing/io_interfaces/io_json.py</code> <pre><code>def json_loader(path_data, path_imagedir, allowed_image_formats, training=True,\n                ohe=True):\n    \"\"\" Data Input Interface for loading a dataset via a JSON and an image directory.\n\n    This **internal** function allows simple parsing of class annotations encoded in a JSON.\n\n    ???+ info \"Input Formats\"\n        ```\n        Format Sparse:\n            - Name Index (key) : Class (value)\n\n        Format One-Hot Encoded:\n            - Name Index (key) : List consisting of binary integers.\n        ```\n\n    **Expected structure:**\n    ```\n    dataset/\n        images_dir/                 # path_imagedir = \"dataset/images_dir\"\n            sample001.png\n            sample002.png\n            ...\n            sample350.png\n        annotations.json            # path_data = \"dataset/annotations.json\"\n    ```\n\n    Args:\n        path_data (str):                        Path to the json file.\n        path_imagedir (str):                    Path to the directory containing the images.\n        allowed_image_formats (list of str):    List of allowed imaging formats. (provided by IO_Interface)\n        training (bool):                        Boolean option whether annotation data is available.\n        ohe (bool):                             Boolean option whether annotation data is sparse categorical or one-hot encoded.\n\n    Returns:\n        index_list (list of str):               List of sample/index encoded as Strings. Required in DataGenerator as `samples`.\n        class_ohe (numpy.ndarray):              Classification list as One-Hot encoding. Required in DataGenerator as `labels`.\n        class_n (int):                          Number of classes. Required in NeuralNetwork for Architecture design as `n_labels`.\n        class_names (list of str):              List of names for corresponding classes. Used for later prediction storage or evaluation.\n        image_format (str):                     Image format to add at the end of the sample index for image loading. Required in DataGenerator.\n    \"\"\"\n    # Load JSON file\n    with open(path_data, \"r\") as json_reader:\n        dt_json = json.load(json_reader)\n    # Identify image format by peaking first image\n    image_format = None\n    for file in os.listdir(path_imagedir):\n        format = file.split(\".\")[-1]\n        if format.lower() in allowed_image_formats or \\\n           format.upper() in allowed_image_formats:\n           image_format = format\n           break\n    # Raise Exception if image format is unknown\n    if image_format is None:\n        raise Exception(\"Unknown image format.\", path_imagedir)\n\n    # Verify if all images are existing\n    lever = True\n    for sample in dt_json:\n        if sample == \"legend\" : continue\n        # Check if image ending is already in sample name by peaking first one\n        if lever:\n            lever = False\n            if sample.endswith(\".\" + image_format) : image_format = None\n        # Obtain image file path\n        if image_format : img_file = sample + \".\" + image_format\n        else : img_file = sample\n        path_img = os.path.join(path_imagedir, img_file)\n        # Check existance\n        if not os.path.exists(path_img):\n            raise Exception(\"Image does not exist / not accessible!\",\n                            'Sample: \"' + sample + '\"', path_img)\n\n    # If JSON is for inference (no annotation data)\n    if not training:\n        # Ensure index list to contain strings\n        if \"legend\" in dt_json : del dt_json[\"legend\"]\n        index_list = [str(x) for x in dt_json]\n        # -&gt; return parsing\n        return index_list, None, None, None, image_format\n\n    # Try parsing with a sparse categorical class format\n    if not ohe:\n        # Parse class name information\n        if \"legend\" in dt_json:\n            class_names = dt_json[\"legend\"]\n            del dt_json[\"legend\"]\n        else : class_names = None\n        # Obtain class information and index list\n        index_list = []\n        classes_sparse = []\n        for sample in dt_json:\n            index_list.append(str(sample))\n            classes_sparse.append(dt_json[sample])\n        if class_names is None : class_names = np.unique(classes_sparse).tolist()\n        class_n = len(class_names)\n        # Parse sparse categorical annotations to One-Hot Encoding\n        class_ohe = pd.get_dummies(classes_sparse).to_numpy()\n    # Try parsing one-hot encoded format\n    else:\n        # Parse information\n        if \"legend\" in dt_json:\n            class_names = dt_json[\"legend\"]\n            del dt_json[\"legend\"]\n            class_n = len(class_names)\n        else:\n            class_names = None\n            class_n = None\n        # Obtain class information and index list\n        index_list = []\n        class_data = []\n        for sample in dt_json:\n            index_list.append(str(sample))\n            class_data.append(dt_json[sample])\n        class_ohe = np.array(class_data)\n        # Verify number of class annotation\n        if class_n is None : class_ohe.shape[1]\n\n    # Validate if number of samples and number of annotations match\n    if len(index_list) != len(class_ohe):\n        raise Exception(\"Numbers of samples and annotations do not match!\",\n                        len(index_list), len(class_ohe))\n\n    # Return parsed JSON data\n    return index_list, class_ohe, class_n, class_names, image_format\n</code></pre>"},{"location":"reference/data_processing/io_loader/","title":"Io loader","text":"<p>The IO Loader functions of AUCMEDI allow loading samples from datasets in different file formats.</p> <p>These functions are called internally via the DataGenerator.</p> <p>IO_loader Functions</p> Interface Description image_loader() Image Loader for image loading via Pillow. sitk_loader() SimpleITK Loader for loading NIfTI (nii) or Metafile (mha) formats. numpy_loader() NumPy Loader for image loading of .npy files. cache_loader() Cache Loader for passing already loaded images. <p>Parameters defined in <code>**kwargs</code> are passed down to IO_loader functions.</p> Example <pre><code># Import required libraries\nfrom aucmedi import *\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize DataGenerator with by default using image_loader\ndata_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                         image_format=image_format, resize=None)\n\n# Initialize DataGenerator with manually selected image_loader\nfrom aucmedi.data_processing.io_loader import image_loader\ndata_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                         image_format=image_format, resize=None,\n                         loader=image_loader)\n</code></pre>"},{"location":"reference/data_processing/io_loader/cache_loader/","title":"Cache loader","text":""},{"location":"reference/data_processing/io_loader/cache_loader/#aucmedi.data_processing.io_loader.cache_loader.cache_loader","title":"<code>cache_loader(sample, path_imagedir=None, image_format=None, grayscale=False, two_dim=True, cache=None, **kwargs)</code>","text":"<p>Cache Loader for passing already loaded images within the AUCMEDI pipeline.</p> <p>The Cache Loader is an IO_loader function, which have to be passed to the  DataGenerator.</p> <p>The complete data management happens in the memory. Thus, for multiple images or common data set sizes, this is NOT recommended!</p> <p>Warning</p> <p>This functions requires to pass a Dictionary to the parameter <code>cache</code>!</p> <p>Dictionary structure: key=index as String; value=Image as NumPy array  e.g. cache = {\"my_index_001\": my_image}</p> Example <pre><code># Import required libraries\nfrom aucmedi import *\nfrom aucmedi.data_processing.io_loader import cache_loader\n\n# Encode information as dictionary\ncache = {\"sample_a\": image_a,\n         \"sample_b\": image_b,\n         \"sample_n\": image_n}\n\n# Obtain meta data\nmy_labels = [[1,0], [0,1], [1,0]]      # one-hot encoded annotation matrix\nsample_list = cache.keys()\n\n# Initialize DataGenerator\ndata_gen = DataGenerator(sample_list, None, labels=my_labels,\n                         resize=None, grayscale=False, two_dim=True,\n                         loader=cache_loader, cache=cache)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Sample name/index of an image.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> <code>None</code> <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading.</p> <code>None</code> <code>grayscale</code> <code>bool</code> <p>Boolean, whether images are grayscale or RGB.</p> <code>False</code> <code>two_dim</code> <code>bool</code> <p>Boolean, whether image is 2D or 3D.</p> <code>True</code> <code>cache</code> <code>dict</code> <p>A Python dictionary containing one or multiple images.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the sample loader.</p> <code>{}</code> Source code in <code>aucmedi/data_processing/io_loader/cache_loader.py</code> <pre><code>def cache_loader(sample, path_imagedir=None, image_format=None,\n                 grayscale=False, two_dim=True, cache=None, **kwargs):\n    \"\"\" Cache Loader for passing already loaded images within the AUCMEDI pipeline.\n\n    The Cache Loader is an IO_loader function, which have to be passed to the \n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    The complete data management happens in the memory.\n    Thus, for multiple images or common data set sizes, this is NOT recommended!\n\n    !!! warning\n        This functions requires to pass a Dictionary to the parameter `cache`!\n\n    Dictionary structure: key=index as String; value=Image as NumPy array &lt;br&gt;\n    e.g. cache = {\"my_index_001\": my_image}\n\n    ???+ example\n        ```python\n        # Import required libraries\n        from aucmedi import *\n        from aucmedi.data_processing.io_loader import cache_loader\n\n        # Encode information as dictionary\n        cache = {\"sample_a\": image_a,\n                 \"sample_b\": image_b,\n                 \"sample_n\": image_n}\n\n        # Obtain meta data\n        my_labels = [[1,0], [0,1], [1,0]]      # one-hot encoded annotation matrix\n        sample_list = cache.keys()\n\n        # Initialize DataGenerator\n        data_gen = DataGenerator(sample_list, None, labels=my_labels,\n                                 resize=None, grayscale=False, two_dim=True,\n                                 loader=cache_loader, cache=cache)\n        ```\n\n    Args:\n        sample (str):               Sample name/index of an image.\n        path_imagedir (str):        Path to the directory containing the images.\n        image_format (str):         Image format to add at the end of the sample index for image loading.\n        grayscale (bool):           Boolean, whether images are grayscale or RGB.\n        two_dim (bool):             Boolean, whether image is 2D or 3D.\n        cache (dict):               A Python dictionary containing one or multiple images.\n        **kwargs (dict):            Additional parameters for the sample loader.\n    \"\"\"\n    # Verify if a cache is provided\n    if cache is None or type(cache) is not dict:\n        raise TypeError(\"No dictionary was provided to cache_loader()!\")\n    # Obtain image from cache\n    img = cache[sample]\n    # Verify image shape for grayscale &amp; 2D\n    if grayscale and two_dim:\n        # Add channel axis and return image\n        if len(img.shape) == 2:\n            return np.reshape(img, img.shape + (1,))\n        # Just return image\n        elif len(img.shape) == 3 and img.shape[-1] == 1:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 2D &amp; Grayscale: Expected either 2D \" + \\\n                             \"without channel axis or 3D with single channel\" + \\\n                             \" axis, but got:\", img.shape, len(img.shape))\n    # Verify image shape for grayscale &amp; 3D\n    elif grayscale and not two_dim:\n        # Add channel axis and return image\n        if len(img.shape) == 3:\n            return np.reshape(img, img.shape + (1,))\n        # Just return image\n        elif len(img.shape) == 4 and img.shape[-1] == 1:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 3D &amp; Grayscale: Expected either 3D \" + \\\n                             \"without channel axis or 4D with single channel\" + \\\n                             \" axis, but got:\", img.shape, len(img.shape))\n    # Verify image shape for rgb &amp; 2D\n    elif not grayscale and two_dim:\n        # Just return image\n        if len(img.shape) == 3 and img.shape[-1] == 3:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 2D &amp; RGB: Expected 3D array \" + \\\n                             \"including a single channel axis, but got:\",\n                             img.shape, len(img.shape))\n    # Verify image shape for rgb &amp; 3D\n    elif not grayscale and not two_dim:\n        # Just return image\n        if len(img.shape) == 4 and img.shape[-1] == 3:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 3D &amp; RGB: Expected 4D array \" + \\\n                             \"including a single channel axis, but got:\",\n                             img.shape, len(img.shape))\n</code></pre>"},{"location":"reference/data_processing/io_loader/image_loader/","title":"Image loader","text":""},{"location":"reference/data_processing/io_loader/image_loader/#aucmedi.data_processing.io_loader.image_loader.image_loader","title":"<code>image_loader(sample, path_imagedir, image_format=None, grayscale=False, **kwargs)</code>","text":"<p>Image Loader for image loading within the AUCMEDI pipeline.</p> <p>The Image Loader is an IO_loader function, which have to be passed to the DataGenerator.</p> Info <p>The Image Loader utilizes Pillow for image loading:  https://github.com/python-pillow/Pillow</p> Example <pre><code># Import required libraries\nfrom aucmedi import *\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize DataGenerator with by default using image_loader\ndata_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                         image_format=image_format, resize=None)\n\n# Initialize DataGenerator with manually selected image_loader\nfrom aucmedi.data_processing.io_loader import image_loader\ndata_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                         image_format=image_format, resize=None,\n                         loader=image_loader)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Sample name/index of an image.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading.</p> <code>None</code> <code>grayscale</code> <code>bool</code> <p>Boolean, whether images are grayscale or RGB.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the sample loader.</p> <code>{}</code> Source code in <code>aucmedi/data_processing/io_loader/image_loader.py</code> <pre><code>def image_loader(sample, path_imagedir, image_format=None, grayscale=False,\n                 **kwargs):\n    \"\"\" Image Loader for image loading within the AUCMEDI pipeline.\n\n    The Image Loader is an IO_loader function, which have to be passed to the\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    ???+ info\n        The Image Loader utilizes Pillow for image loading: &lt;br&gt;\n        https://github.com/python-pillow/Pillow\n\n    ???+ example\n        ```python\n        # Import required libraries\n        from aucmedi import *\n\n        # Initialize input data reader\n        ds = input_interface(interface=\"csv\",\n                             path_imagedir=\"dataset/images/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Initialize DataGenerator with by default using image_loader\n        data_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                                 image_format=image_format, resize=None)\n\n        # Initialize DataGenerator with manually selected image_loader\n        from aucmedi.data_processing.io_loader import image_loader\n        data_gen = DataGenerator(samples, \"dataset/images/\", labels=class_ohe,\n                                 image_format=image_format, resize=None,\n                                 loader=image_loader)\n        ```\n\n    Args:\n        sample (str):               Sample name/index of an image.\n        path_imagedir (str):        Path to the directory containing the images.\n        image_format (str):         Image format to add at the end of the sample index for image loading.\n        grayscale (bool):           Boolean, whether images are grayscale or RGB.\n        **kwargs (dict):            Additional parameters for the sample loader.\n    \"\"\"\n    # Get image path\n    if image_format : img_file = sample + \".\" + image_format\n    else : img_file = sample\n    path_img = os.path.join(path_imagedir, img_file)\n    # Load image via the PIL package\n    img_raw = Image.open(path_img)\n    # Convert image to grayscale or rgb\n    if grayscale : img_converted = img_raw.convert('LA')\n    else : img_converted = img_raw.convert('RGB')\n    # Convert image to NumPy\n    img = np.asarray(img_converted)\n    # Perform additional preprocessing if grayscale image\n    if grayscale:\n        # Remove maximum value and keep only intensity\n        img = img[:,:,0]\n        # Reshape image to create a single channel\n        img = np.reshape(img, img.shape + (1,))\n    # Return image\n    return img\n</code></pre>"},{"location":"reference/data_processing/io_loader/numpy_loader/","title":"Numpy loader","text":""},{"location":"reference/data_processing/io_loader/numpy_loader/#aucmedi.data_processing.io_loader.numpy_loader.numpy_loader","title":"<code>numpy_loader(sample, path_imagedir, image_format=None, grayscale=False, two_dim=True, **kwargs)</code>","text":"<p>NumPy Loader for image loading within the AUCMEDI pipeline.</p> <p>The NumPy Loader is an IO_loader function, which have to be passed to the DataGenerator.</p> <p>The NumPy load function <code>np.load(path_img, allow_pickle=True)</code> is used.</p> Example <pre><code># Import required libraries\nfrom aucmedi import *\nfrom aucmedi.data_processing.io_loader import numpy_loader\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",\n                     path_imagedir=\"dataset/npy_files/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize DataGenerator with numpy_loader\ndata_gen = DataGenerator(samples, \"dataset/npy_files/\", labels=class_ohe,\n                         image_format=image_format, resize=None,\n                         grayscale=True, two_dim=False,\n                         loader=numpy_loader)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Sample name/index of an image.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading.</p> <code>None</code> <code>grayscale</code> <code>bool</code> <p>Boolean, whether images are grayscale or RGB.</p> <code>False</code> <code>two_dim</code> <code>bool</code> <p>Boolean, whether image is 2D or 3D.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the sample loader.</p> <code>{}</code> Source code in <code>aucmedi/data_processing/io_loader/numpy_loader.py</code> <pre><code>def numpy_loader(sample, path_imagedir, image_format=None, grayscale=False,\n                 two_dim=True, **kwargs):\n    \"\"\" NumPy Loader for image loading within the AUCMEDI pipeline.\n\n    The NumPy Loader is an IO_loader function, which have to be passed to the\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    The NumPy load function `np.load(path_img, allow_pickle=True)` is used.\n\n    ???+ example\n        ```python\n        # Import required libraries\n        from aucmedi import *\n        from aucmedi.data_processing.io_loader import numpy_loader\n\n        # Initialize input data reader\n        ds = input_interface(interface=\"csv\",\n                             path_imagedir=\"dataset/npy_files/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Initialize DataGenerator with numpy_loader\n        data_gen = DataGenerator(samples, \"dataset/npy_files/\", labels=class_ohe,\n                                 image_format=image_format, resize=None,\n                                 grayscale=True, two_dim=False,\n                                 loader=numpy_loader)\n        ```\n\n    Args:\n        sample (str):               Sample name/index of an image.\n        path_imagedir (str):        Path to the directory containing the images.\n        image_format (str):         Image format to add at the end of the sample index for image loading.\n        grayscale (bool):           Boolean, whether images are grayscale or RGB.\n        two_dim (bool):             Boolean, whether image is 2D or 3D.\n        **kwargs (dict):            Additional parameters for the sample loader.\n    \"\"\"\n    # Get image path\n    if image_format : img_file = sample + \".\" + image_format\n    else : img_file = sample\n    path_img = os.path.join(path_imagedir, img_file)\n    # Load image via the NumPy package\n    img = np.load(path_img, allow_pickle=True)\n    # Verify image shape for grayscale &amp; 2D\n    if grayscale and two_dim:\n        # Add channel axis and return image\n        if len(img.shape) == 2:\n            return np.reshape(img, img.shape + (1,))\n        # Just return image\n        elif len(img.shape) == 3 and img.shape[-1] == 1:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 2D &amp; Grayscale: Expected either 2D \" + \\\n                             \"without channel axis or 3D with single channel\" + \\\n                             \" axis, but got:\", img.shape, len(img.shape))\n    # Verify image shape for grayscale &amp; 3D\n    elif grayscale and not two_dim:\n        # Add channel axis and return image\n        if len(img.shape) == 3:\n            return np.reshape(img, img.shape + (1,))\n        # Just return image\n        elif len(img.shape) == 4 and img.shape[-1] == 1:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 3D &amp; Grayscale: Expected either 3D \" + \\\n                             \"without channel axis or 4D with single channel\" + \\\n                             \" axis, but got:\", img.shape, len(img.shape))\n    # Verify image shape for rgb &amp; 2D\n    elif not grayscale and two_dim:\n        # Just return image\n        if len(img.shape) == 3 and img.shape[-1] == 3:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 2D &amp; RGB: Expected 3D array \" + \\\n                             \"including a single channel axis, but got:\",\n                             img.shape, len(img.shape))\n    # Verify image shape for rgb &amp; 3D\n    elif not grayscale and not two_dim:\n        # Just return image\n        if len(img.shape) == 4 and img.shape[-1] == 3:\n            return img\n        # Throw Exception\n        else:\n            raise ValueError(\"Parameter 3D &amp; RGB: Expected 4D array \" + \\\n                             \"including a single channel axis, but got:\",\n                             img.shape, len(img.shape))\n</code></pre>"},{"location":"reference/data_processing/io_loader/sitk_loader/","title":"Sitk loader","text":""},{"location":"reference/data_processing/io_loader/sitk_loader/#aucmedi.data_processing.io_loader.sitk_loader.sitk_loader","title":"<code>sitk_loader(sample, path_imagedir, image_format=None, grayscale=True, resampling=(1.0, 1.0, 1.0), outside_value=0, **kwargs)</code>","text":"<p>SimpleITK Loader for loading of CT/MRI scans in NIfTI (nii) or Metafile (mha) format within the AUCMEDI pipeline.</p> <p>The SimpleITK Loader is an IO_loader function, which have to be passed to the DataGenerator.</p> <p>This loader is intended to load only 3D volumes with annotated voxel spacings.</p> <p>By default, volumes are normalized to voxel spacing 1.0 x 1.0 x 1.0.  You can define a custom voxel spacing for the loader, by passing a tuple of spacings as parameter 'resampling'. </p> Info <p>The SimpleITK Loader utilizes SimpleITK for sample loading:  https://simpleitk.readthedocs.io/en/master/IO.html</p> Example <pre><code># Import required libraries\nfrom aucmedi import *\nfrom aucmedi.data_processing.io_loader import sitk_loader\n\n# Initialize input data reader\nds = input_interface(interface=\"csv\",\n                    path_imagedir=\"dataset/nii_files/\",\n                    path_data=\"dataset/annotations.csv\",\n                    ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize DataGenerator with sitk_loader\ndata_gen = DataGenerator(samples, \"dataset/nii_files/\", labels=class_ohe,\n                        image_format=image_format, resize=None,\n                        grayscale=True, resampling=(2.10, 1.48, 1.48),\n                        loader=sitk_loader)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Sample name/index of an image.</p> required <code>path_imagedir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>image_format</code> <code>str</code> <p>Image format to add at the end of the sample index for image loading.</p> <code>None</code> <code>grayscale</code> <code>bool</code> <p>Boolean, whether images are grayscale or RGB.</p> <code>True</code> <code>resampling</code> <code>tuple of float</code> <p>Tuple of 3x floats with z,y,x mapping encoding voxel spacing.                         If passing <code>None</code>, no normalization will be performed.</p> <code>(1.0, 1.0, 1.0)</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for the sample loader.</p> <code>{}</code> Source code in <code>aucmedi/data_processing/io_loader/sitk_loader.py</code> <pre><code>def sitk_loader(sample, path_imagedir, image_format=None, grayscale=True,\n                resampling=(1.0, 1.0, 1.0), outside_value=0, **kwargs):\n    \"\"\" SimpleITK Loader for loading of CT/MRI scans in NIfTI (nii) or Metafile (mha) format within the AUCMEDI pipeline.\n\n    The SimpleITK Loader is an IO_loader function, which have to be passed to the\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    This loader is intended to load only 3D volumes with annotated voxel spacings.\n\n    By default, volumes are normalized to voxel spacing 1.0 x 1.0 x 1.0. &lt;br&gt;\n    You can define a custom voxel spacing for the loader, by passing a tuple of spacings as parameter 'resampling'. &lt;br&gt;\n\n    ???+ info\n        The SimpleITK Loader utilizes SimpleITK for sample loading: &lt;br&gt;\n        https://simpleitk.readthedocs.io/en/master/IO.html\n\n    ???+ example\n        ```python\n        # Import required libraries\n        from aucmedi import *\n        from aucmedi.data_processing.io_loader import sitk_loader\n\n        # Initialize input data reader\n        ds = input_interface(interface=\"csv\",\n                            path_imagedir=\"dataset/nii_files/\",\n                            path_data=\"dataset/annotations.csv\",\n                            ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Initialize DataGenerator with sitk_loader\n        data_gen = DataGenerator(samples, \"dataset/nii_files/\", labels=class_ohe,\n                                image_format=image_format, resize=None,\n                                grayscale=True, resampling=(2.10, 1.48, 1.48),\n                                loader=sitk_loader)\n        ```\n\n    Args:\n        sample (str):               Sample name/index of an image.\n        path_imagedir (str):        Path to the directory containing the images.\n        image_format (str):         Image format to add at the end of the sample index for image loading.\n        grayscale (bool):           Boolean, whether images are grayscale or RGB.\n        resampling (tuple of float):Tuple of 3x floats with z,y,x mapping encoding voxel spacing.\n                                    If passing `None`, no normalization will be performed.\n        **kwargs (dict):            Additional parameters for the sample loader.\n    \"\"\"\n    # Get image path\n    if image_format : img_file = sample + \".\" + image_format\n    else : img_file = sample\n    path_img = os.path.join(path_imagedir, img_file)\n    # Load image via the SimpleITK package\n    sample_itk = sitk.ReadImage(path_img)\n    # Perform resampling\n    if resampling is not None:\n        # Extract information from sample\n        shape = sample_itk.GetSize()\n        spacing = sample_itk.GetSpacing()\n        # Reverse resampling spacing to sITK mapping (z,y,x -&gt; x,y,z)\n        new_spacing = resampling[::-1]\n        # Estimate output shape after resampling\n        output_shape = []\n        for t in zip(shape, spacing, new_spacing):\n            s = int(t[0] * t[1] / t[2])\n            output_shape.append(s)\n        output_shape = tuple(output_shape)\n        # Perform resampling via sITK\n        sample_itk_resampled = sitk.Resample(sample_itk,\n                                             output_shape,\n                                             sitk.Transform(),\n                                             sitk.sitkLinear,\n                                             sample_itk.GetOrigin(),\n                                             new_spacing,\n                                             sample_itk.GetDirection(),\n                                             outside_value)\n    # Skip resampling if None\n    else : sample_itk_resampled = sample_itk\n    # Convert to NumPy\n    img = sitk.GetArrayFromImage(sample_itk_resampled)\n    # Add single channel axis\n    if len(img.shape) == 3 : img = np.expand_dims(img, axis=-1)\n    # Return image\n    return img\n</code></pre>"},{"location":"reference/data_processing/subfunctions/","title":"Subfunctions","text":"<p>Library of implemented Subfunctions in AUCMEDI.</p> <p>A Subfunction is a preprocessing method, which is automatically applied on all samples if provided to a DataGenerator.</p> <p>Image preprocessing is defined as a method or technique which modify the image before passing it to the neural network model. The aim of preprocessing methods is to extensively increase performance due to simplification of information. Common preprocessing methods range from intensity value normalization to image resizing.</p> Info <p>The DataGenerator applies the list of Subfunctions sequentially on the data set.</p> <p>The Subfunctions Resize and Standardize are integrated into the DataGenerator base due to its requirement in any medical image classification pipeline.</p> Example <pre><code># Import Subfunctions\nfrom aucmedi.data_processing.subfunctions import *\n\n# Select desired Subfunctions\nsf_crop = Crop(shape=(128, 164), mode=\"center\")\nsf_padding = Padding(mode=\"square\")\n# Pack them into a list\nsf_list = [sf_crop, sf_padding]                 # Subfunctions will be applied in provided list order\n\n# Pass the list to the DataGenerator\ntrain_gen = DataGenerator(samples=index_list,\n                          path_imagedir=\"my_images/\",\n                          labels=class_ohe,\n                          resize=(512,512),                # Call the integrated resize Subfunction\n                          standardize_mode=\"grayscale\",    # Call the integrated standardize Subfunction\n                          subfunctions=sf_list)            # Pass desired Subfunctions\n</code></pre> <p>Subfunctions are based on the abstract base class Subfunction_Base, which allow simple integration of custom preprocessing methods.</p>"},{"location":"reference/data_processing/subfunctions/chromer/","title":"Chromer","text":""},{"location":"reference/data_processing/subfunctions/chromer/#aucmedi.data_processing.subfunctions.chromer.Chromer","title":"<code>Chromer</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Subfunction class which which can be used for color format transforming.</p> <pre><code>Transforms:\n- grayscale -&gt;  RGB\n- RGB       -&gt;  grayscale\n</code></pre> <p>Possible target formats: <code>[\"rgb\", \"grayscale\"]</code></p> <p>Typical use case is converting a grayscale to RGB in order to utilize transfer learning weights based on ImageNet.</p> Source code in <code>aucmedi/data_processing/subfunctions/chromer.py</code> <pre><code>class Chromer(Subfunction_Base):\n    \"\"\" A Subfunction class which which can be used for color format transforming.\n\n    ```\n    Transforms:\n    - grayscale -&gt;  RGB\n    - RGB       -&gt;  grayscale\n\n    ```\n\n    Possible target formats: `[\"rgb\", \"grayscale\"]`\n\n    Typical use case is converting a grayscale to RGB in order to utilize\n    transfer learning weights based on ImageNet.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, target=\"rgb\"):\n        \"\"\" Initialization function for creating a Chromer Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            target (str):               Transformation mode for desired target format.\n        \"\"\"\n        # Verify target format\n        if target not in [\"grayscale\", \"rgb\"]:\n            raise ValueError(\"Unknown target format for Chromer Subfunction\",\n                             target, \"Possibles target formats are: ['grayscale', 'rgb']\")\n        # Cache target format\n        self.target = target\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Verify that image is in correct format\n        if self.target == \"rgb\" and (image.shape[-1] != 1 or \\\n                                     np.max(image) &gt; 255 or \\\n                                     np.min(image) &lt; 0):\n            raise ValueError(\"Subfunction Chromer: Image is not in grayscale format!\",\n                             \"Ensure that it is grayscale normalized and has\",\n                             \"a single channel.\")\n        elif self.target == \"grayscale\" and (image.shape[-1] != 3 or \\\n                                             np.max(image) &gt; 255 or \\\n                                             np.min(image) &lt; 0):\n            raise ValueError(\"Subfunction Chromer: Image is not in RGB format!\",\n                             \"Ensure that it is normalized [0,255] and has 3 channels.\")\n        # Run grayscale -&gt; RGB\n        if self.target == \"rgb\":\n            image_chromed = np.concatenate((image,)*3, axis=-1)\n        # Run RGB -&gt; grayscale\n        else:\n            # Get color intensity values\n            r = np.take(image, indices=0, axis=-1)\n            g = np.take(image, indices=1, axis=-1)\n            b = np.take(image, indices=2, axis=-1)\n            # Compute grayscale image\n            image_chromed = 0.299 * r + 0.587 * g + 0.114 * b\n            # Add channel axis back\n            image_chromed = np.expand_dims(image_chromed, axis=-1)\n        # Return chromed image\n        return image_chromed\n</code></pre>"},{"location":"reference/data_processing/subfunctions/chromer/#aucmedi.data_processing.subfunctions.chromer.Chromer.__init__","title":"<code>__init__(target='rgb')</code>","text":"<p>Initialization function for creating a Chromer Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>Transformation mode for desired target format.</p> <code>'rgb'</code> Source code in <code>aucmedi/data_processing/subfunctions/chromer.py</code> <pre><code>def __init__(self, target=\"rgb\"):\n    \"\"\" Initialization function for creating a Chromer Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        target (str):               Transformation mode for desired target format.\n    \"\"\"\n    # Verify target format\n    if target not in [\"grayscale\", \"rgb\"]:\n        raise ValueError(\"Unknown target format for Chromer Subfunction\",\n                         target, \"Possibles target formats are: ['grayscale', 'rgb']\")\n    # Cache target format\n    self.target = target\n</code></pre>"},{"location":"reference/data_processing/subfunctions/clip/","title":"Clip","text":""},{"location":"reference/data_processing/subfunctions/clip/#aucmedi.data_processing.subfunctions.clip.Clip","title":"<code>Clip</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Subfunction class which which can be used for clipping intensity pixel     values on a certain range.</p> <p>Typical use case is clipping Hounsfield Units (HU) in CT scans for focusing on tissue types of interest.</p> Source code in <code>aucmedi/data_processing/subfunctions/clip.py</code> <pre><code>class Clip(Subfunction_Base):\n    \"\"\" A Subfunction class which which can be used for clipping intensity pixel\n        values on a certain range.\n\n    Typical use case is clipping Hounsfield Units (HU) in CT scans for focusing\n    on tissue types of interest.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, min=None, max=None, per_channel=False):\n        \"\"\" Initialization function for creating a Clip Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            min (float or int or list):     Desired minimum value for clipping (if `None`, no lower limit is applied).\n                                            Also possible to pass a list of minimum values if `per_channel=True`.\n            max (float or int or list):     Desired maximum value for clipping (if `None`, no upper limit is applied).\n                                            Also possible to pass a list of maximum values if `per_channel=True`.\n            per_channel (bool):             Option if clipping should be applied per channel with different clipping ranges.\n        \"\"\"\n        self.min = min\n        self.max = max\n        self.per_channel = per_channel\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Perform clipping on all channels\n        if not self.per_channel:\n            image_clipped = np.clip(image, a_min=self.min, a_max=self.max)\n        # Perform clipping on each channel\n        else:\n            image_clipped = image.copy()\n            for c in range(0, image.shape[-1]):\n                # Identify minimum to clip\n                if self.min is not None and \\\n                    type(self.min) in [list, tuple, np.ndarray]:\n                    min = self.min[c]\n                else : min = self.min\n                # Identify maximum to clip\n                if self.max is not None and \\\n                    type(self.max) in [list, tuple, np.ndarray]:\n                    max = self.max[c]\n                else : max = self.max\n                # Perform clipping\n                image_clipped[..., c] = np.clip(image[...,c], \n                                                a_min=min, \n                                                a_max=max)\n        # Return clipped image\n        return image_clipped\n</code></pre>"},{"location":"reference/data_processing/subfunctions/clip/#aucmedi.data_processing.subfunctions.clip.Clip.__init__","title":"<code>__init__(min=None, max=None, per_channel=False)</code>","text":"<p>Initialization function for creating a Clip Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float or int or list</code> <p>Desired minimum value for clipping (if <code>None</code>, no lower limit is applied).                             Also possible to pass a list of minimum values if <code>per_channel=True</code>.</p> <code>None</code> <code>max</code> <code>float or int or list</code> <p>Desired maximum value for clipping (if <code>None</code>, no upper limit is applied).                             Also possible to pass a list of maximum values if <code>per_channel=True</code>.</p> <code>None</code> <code>per_channel</code> <code>bool</code> <p>Option if clipping should be applied per channel with different clipping ranges.</p> <code>False</code> Source code in <code>aucmedi/data_processing/subfunctions/clip.py</code> <pre><code>def __init__(self, min=None, max=None, per_channel=False):\n    \"\"\" Initialization function for creating a Clip Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        min (float or int or list):     Desired minimum value for clipping (if `None`, no lower limit is applied).\n                                        Also possible to pass a list of minimum values if `per_channel=True`.\n        max (float or int or list):     Desired maximum value for clipping (if `None`, no upper limit is applied).\n                                        Also possible to pass a list of maximum values if `per_channel=True`.\n        per_channel (bool):             Option if clipping should be applied per channel with different clipping ranges.\n    \"\"\"\n    self.min = min\n    self.max = max\n    self.per_channel = per_channel\n</code></pre>"},{"location":"reference/data_processing/subfunctions/color_constancy/","title":"Color constancy","text":""},{"location":"reference/data_processing/subfunctions/color_constancy/#aucmedi.data_processing.subfunctions.color_constancy.ColorConstancy","title":"<code>ColorConstancy</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Subfunction class which fixes the problem of Color Constancy in an image.</p> Warning <p>Can only be applied on RGB images.</p> <p>\"The paper Improving dermoscopy image classification using color constancy shows that using a color compensation technique to reduce the influence of the acquisition setup on the color features extracted from the images provides a improvement on the performance for skin cancer classification.\"</p> Cite <p>Description from: https://www.kaggle.com/apacheco/shades-of-gray-color-constancy</p> Reference - Implementation <p>https://github.com/nickshawn/Shades_of_Gray-color_constancy_transformation</p> Reference - Publication <p>Catarina Barata; M. Emre Celebi; Jorge S. Marques. 2014. Improving Dermoscopy Image Classification Using Color Constancy.  https://ieeexplore.ieee.org/abstract/document/6866131</p> Source code in <code>aucmedi/data_processing/subfunctions/color_constancy.py</code> <pre><code>class ColorConstancy(Subfunction_Base):\n    \"\"\"  A Subfunction class which fixes the problem of Color Constancy in an image.\n\n    ???+ warning\n        Can only be applied on RGB images.\n\n    \"The paper Improving dermoscopy image classification using color constancy shows\n    that using a color compensation technique to reduce the influence of the acquisition\n    setup on the color features extracted from the images provides a improvement on the\n    performance for skin cancer classification.\"\n\n    ???+ cite\n        Description from: https://www.kaggle.com/apacheco/shades-of-gray-color-constancy\n\n    ??? abstract \"Reference - Implementation\"\n        https://github.com/nickshawn/Shades_of_Gray-color_constancy_transformation\n\n    ??? abstract \"Reference - Publication\"\n        Catarina Barata; M. Emre Celebi; Jorge S. Marques. 2014.\n        Improving Dermoscopy Image Classification Using Color Constancy.\n        &lt;br&gt;\n        https://ieeexplore.ieee.org/abstract/document/6866131\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, power=6):\n        \"\"\" Initialization function for creating a ColorConstancy Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            power (int):            Exponent for the image.\n        \"\"\"\n        self.power = power\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Verify if image is RGB\n        if image.shape[-1] != 3:\n            raise ValueError(\"Image have to be RGB for Color Constancy application!\",\n                             \"Last axis of image is not 3 (RGB):\", image.shape)\n        # Apply color constancy filtering (Shades of Gray)\n        img = image.astype('float32')\n        img_power = np.power(img, self.power)\n        axes = tuple(range(len(image.shape[:-1])))\n        rgb_vec = np.power(np.mean(img_power, axes), 1/self.power)\n        rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n        rgb_vec = rgb_vec / rgb_norm\n        rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n        img_filtered = np.multiply(img, rgb_vec)\n        # Return filtered image\n        return img_filtered\n</code></pre>"},{"location":"reference/data_processing/subfunctions/color_constancy/#aucmedi.data_processing.subfunctions.color_constancy.ColorConstancy.__init__","title":"<code>__init__(power=6)</code>","text":"<p>Initialization function for creating a ColorConstancy Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>power</code> <code>int</code> <p>Exponent for the image.</p> <code>6</code> Source code in <code>aucmedi/data_processing/subfunctions/color_constancy.py</code> <pre><code>def __init__(self, power=6):\n    \"\"\" Initialization function for creating a ColorConstancy Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        power (int):            Exponent for the image.\n    \"\"\"\n    self.power = power\n</code></pre>"},{"location":"reference/data_processing/subfunctions/crop/","title":"Crop","text":""},{"location":"reference/data_processing/subfunctions/crop/#aucmedi.data_processing.subfunctions.crop.Crop","title":"<code>Crop</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Crop Subfunction class which center/randomly crops a desired shape from an image.</p> <p>List of valid modes for parameter \"mode\": <code>[\"center\", \"random\"]</code></p> 2D image <p>Shape have to be defined as tuple with x and y size: <code>Crop(shape=(224, 224))</code></p> <p>Cropping is done via albumentations CenterCrop and RandomCrop transform.  https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop   https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop  </p> 3D volume <p>Shape has to be defined as tuple with x, y and z size: <code>Crop(shape=(224, 224, 244))</code></p> <p>Cropping is done via volumentations CenterCrop and RandomCrop transform.  https://github.com/muellerdo/volumentations</p> Source code in <code>aucmedi/data_processing/subfunctions/crop.py</code> <pre><code>class Crop(Subfunction_Base):\n    \"\"\" A Crop Subfunction class which center/randomly crops a desired shape from an image.\n\n    List of valid modes for parameter \"mode\": `[\"center\", \"random\"]`\n\n    ???+ info \"2D image\"\n        Shape have to be defined as tuple with x and y size: `Crop(shape=(224, 224))`\n\n        Cropping is done via albumentations CenterCrop and RandomCrop transform. &lt;br&gt;\n        https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop  &lt;br&gt;\n        https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop  &lt;br&gt;\n\n    ???+ info \"3D volume\"\n        Shape has to be defined as tuple with x, y and z size: `Crop(shape=(224, 224, 244))`\n\n        Cropping is done via volumentations CenterCrop and RandomCrop transform. &lt;br&gt;\n        https://github.com/muellerdo/volumentations\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, shape=(224, 224), mode=\"center\"):\n        \"\"\" Initialization function for creating a Crop Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            shape (tuple of int):       Desired output shape after cropping.\n            mode (str):                 Selected mode for cropping.\n        \"\"\"\n        # Initialize parameter\n        params = {\"p\":1.0, \"always_apply\":True}\n        # Select augmentation module and add further parameter depending on dimension\n        if len(shape) == 2:\n            params[\"height\"] = shape[0]\n            params[\"width\"] = shape[1]\n            mod = albumentations\n        elif len(shape) == 3:\n            params[\"shape\"] = shape\n            mod = volumentations\n        else : raise ValueError(\"Shape for cropping has to be 2D or 3D!\", shape)\n        # Initialize cropping transform\n        if mode == \"center\":\n            self.aug_transform = mod.Compose([mod.CenterCrop(**params)])\n        elif mode == \"random\":\n            self.aug_transform = mod.Compose([mod.RandomCrop(**params)])\n        else : raise ValueError(\"Unknown mode for crop Subfunction\", mode,\n                                \"Possibles modes are: ['center', 'random']\")\n        # Cache shape\n        self.shape = shape\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Perform cropping on image\n        image_cropped = self.aug_transform(image=image)[\"image\"]\n        # Return cropped image\n        return image_cropped\n</code></pre>"},{"location":"reference/data_processing/subfunctions/crop/#aucmedi.data_processing.subfunctions.crop.Crop.__init__","title":"<code>__init__(shape=(224, 224), mode='center')</code>","text":"<p>Initialization function for creating a Crop Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple of int</code> <p>Desired output shape after cropping.</p> <code>(224, 224)</code> <code>mode</code> <code>str</code> <p>Selected mode for cropping.</p> <code>'center'</code> Source code in <code>aucmedi/data_processing/subfunctions/crop.py</code> <pre><code>def __init__(self, shape=(224, 224), mode=\"center\"):\n    \"\"\" Initialization function for creating a Crop Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        shape (tuple of int):       Desired output shape after cropping.\n        mode (str):                 Selected mode for cropping.\n    \"\"\"\n    # Initialize parameter\n    params = {\"p\":1.0, \"always_apply\":True}\n    # Select augmentation module and add further parameter depending on dimension\n    if len(shape) == 2:\n        params[\"height\"] = shape[0]\n        params[\"width\"] = shape[1]\n        mod = albumentations\n    elif len(shape) == 3:\n        params[\"shape\"] = shape\n        mod = volumentations\n    else : raise ValueError(\"Shape for cropping has to be 2D or 3D!\", shape)\n    # Initialize cropping transform\n    if mode == \"center\":\n        self.aug_transform = mod.Compose([mod.CenterCrop(**params)])\n    elif mode == \"random\":\n        self.aug_transform = mod.Compose([mod.RandomCrop(**params)])\n    else : raise ValueError(\"Unknown mode for crop Subfunction\", mode,\n                            \"Possibles modes are: ['center', 'random']\")\n    # Cache shape\n    self.shape = shape\n</code></pre>"},{"location":"reference/data_processing/subfunctions/padding/","title":"Padding","text":""},{"location":"reference/data_processing/subfunctions/padding/#aucmedi.data_processing.subfunctions.padding.Padding","title":"<code>Padding</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Padding Subfunction class which pads an images according to a desired shape.</p> <p>Standard application is to square images to keep original aspect ratio. If another mode as \"square\" is selected, than a shape and NumPy pad mode is required!</p> <p>Shape should be defined as tuple with x and y size:  <code>Padding(mode=\"edge\", shape=(224, 224))</code></p> <p>Padding is done via NumPy pad function which uses can be called with different modes like \"edge\" or \"minimum\".  https://numpy.org/doc/stable/reference/generated/numpy.pad.html</p> Source code in <code>aucmedi/data_processing/subfunctions/padding.py</code> <pre><code>class Padding(Subfunction_Base):\n    \"\"\" A Padding Subfunction class which pads an images according to a desired shape.\n\n    Standard application is to square images to keep original aspect ratio.\n    If another mode as \"square\" is selected, than a shape and NumPy pad mode is required!\n\n    Shape should be defined as tuple with x and y size: &lt;br&gt;\n    `Padding(mode=\"edge\", shape=(224, 224))`\n\n    Padding is done via NumPy pad function which uses can be called with different\n    modes like \"edge\" or \"minimum\". &lt;br&gt;\n    https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, mode=\"square\", shape=None):\n        \"\"\" Initialization function for creating a Padding Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            mode (str):                 Selected mode for image padding. If not `\"square\"`, then NumPy modes are used.\n            shape (tuple of int):       Minimum image shape for non-`\"square\"` modes.\n        \"\"\"\n        self.shape = shape\n        self.mode = mode\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Identify new shape\n        if self.mode == \"square\":\n            max_axis = max(image.shape[:-1])\n            new_shape = [max_axis for x in range(0, len(image.shape[:-1]))]\n        else:\n            new_shape = [max(self.shape[i],image.shape[i]) \\\n                         for i in range(0, len(image.shape[:-1]))]\n        # Compute padding width\n        ## Code inspiration from: https://github.com/MIC-DKFZ/batchgenerators/blob/master/batchgenerators/augmentations/utils.py\n        ## Leave a star for them if you are reading this. The MIC-DKFZ is doing some great work ;)\n        difference = new_shape - np.asarray(image.shape[0:-1])\n        pad_below = difference // 2\n        pad_above = difference // 2 + difference % 2\n        pad_list = list([list(i) for i in zip(pad_below, pad_above)]) + [[0, 0]]\n        # Identify correct NumPy pad mode\n        if self.mode == \"square\" : pad_mode = \"edge\"\n        else : pad_mode = self.mode\n        # Perform padding into desired shape\n        image_padded = np.pad(image, pad_list, mode=pad_mode)\n        # Return padded image\n        return image_padded\n</code></pre>"},{"location":"reference/data_processing/subfunctions/padding/#aucmedi.data_processing.subfunctions.padding.Padding.__init__","title":"<code>__init__(mode='square', shape=None)</code>","text":"<p>Initialization function for creating a Padding Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Selected mode for image padding. If not <code>\"square\"</code>, then NumPy modes are used.</p> <code>'square'</code> <code>shape</code> <code>tuple of int</code> <p>Minimum image shape for non-<code>\"square\"</code> modes.</p> <code>None</code> Source code in <code>aucmedi/data_processing/subfunctions/padding.py</code> <pre><code>def __init__(self, mode=\"square\", shape=None):\n    \"\"\" Initialization function for creating a Padding Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        mode (str):                 Selected mode for image padding. If not `\"square\"`, then NumPy modes are used.\n        shape (tuple of int):       Minimum image shape for non-`\"square\"` modes.\n    \"\"\"\n    self.shape = shape\n    self.mode = mode\n</code></pre>"},{"location":"reference/data_processing/subfunctions/resize/","title":"Resize","text":""},{"location":"reference/data_processing/subfunctions/resize/#aucmedi.data_processing.subfunctions.resize.Resize","title":"<code>Resize</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Resize Subfunction class which resizes an images according to a desired shape.</p> 2D image <p>Shape have to be defined as tuple with x and y size: <code>Resize(shape=(224, 224))</code></p> <p>Resizing is done via albumentations resize transform which uses bi-linear interpolation by default.  https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/</p> 3D volume <p>Shape has to be defined as tuple with x, y and z size: <code>Resize(shape=(128, 128, 128))</code></p> <p>Resizing is done via volumentations resize transform which uses bi-linear interpolation by default.  https://github.com/muellerdo/volumentations</p> Source code in <code>aucmedi/data_processing/subfunctions/resize.py</code> <pre><code>class Resize(Subfunction_Base):\n    \"\"\" A Resize Subfunction class which resizes an images according to a desired shape.\n\n    ???+ info \"2D image\"\n        Shape have to be defined as tuple with x and y size: `Resize(shape=(224, 224))`\n\n        Resizing is done via albumentations resize transform which uses bi-linear interpolation by default. &lt;br&gt;\n        https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/\n\n    ???+ info \"3D volume\"\n        Shape has to be defined as tuple with x, y and z size: `Resize(shape=(128, 128, 128))`\n\n        Resizing is done via volumentations resize transform which uses bi-linear interpolation by default. &lt;br&gt;\n        https://github.com/muellerdo/volumentations\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, shape=(224, 224), interpolation=1):\n        \"\"\" Initialization function for creating a Resize Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            shape (tuple of int):       Resizing shape consisting of a X and Y size. (optional Z size for Volumes).\n            interpolation (int):        Interpolation mode for resizing. Using encoding from cv2:\n                                        https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html\n        \"\"\"\n        # Initialize parameter\n        params = {\"p\":1.0, \"always_apply\":True, \"interpolation\":interpolation}\n        # Select augmentation module and add further parameter depending on dimension\n        if len(shape) == 2:\n            params[\"height\"] = shape[0]\n            params[\"width\"] = shape[1]\n            mod = albumentations\n        elif len(shape) == 3:\n            params[\"shape\"] = shape\n            mod = volumentations\n        else : raise ValueError(\"Shape for Resize has to be 2D or 3D!\", shape)\n        # Initialize resizing transform\n        self.aug_transform = mod.Compose([mod.Resize(**params)])\n        # Cache shape\n        self.shape = shape\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Perform resizing into desired shape\n        image_resized = self.aug_transform(image=image)[\"image\"]\n        # Return resized image\n        return image_resized\n</code></pre>"},{"location":"reference/data_processing/subfunctions/resize/#aucmedi.data_processing.subfunctions.resize.Resize.__init__","title":"<code>__init__(shape=(224, 224), interpolation=1)</code>","text":"<p>Initialization function for creating a Resize Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple of int</code> <p>Resizing shape consisting of a X and Y size. (optional Z size for Volumes).</p> <code>(224, 224)</code> <code>interpolation</code> <code>int</code> <p>Interpolation mode for resizing. Using encoding from cv2:                         https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html</p> <code>1</code> Source code in <code>aucmedi/data_processing/subfunctions/resize.py</code> <pre><code>def __init__(self, shape=(224, 224), interpolation=1):\n    \"\"\" Initialization function for creating a Resize Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        shape (tuple of int):       Resizing shape consisting of a X and Y size. (optional Z size for Volumes).\n        interpolation (int):        Interpolation mode for resizing. Using encoding from cv2:\n                                    https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html\n    \"\"\"\n    # Initialize parameter\n    params = {\"p\":1.0, \"always_apply\":True, \"interpolation\":interpolation}\n    # Select augmentation module and add further parameter depending on dimension\n    if len(shape) == 2:\n        params[\"height\"] = shape[0]\n        params[\"width\"] = shape[1]\n        mod = albumentations\n    elif len(shape) == 3:\n        params[\"shape\"] = shape\n        mod = volumentations\n    else : raise ValueError(\"Shape for Resize has to be 2D or 3D!\", shape)\n    # Initialize resizing transform\n    self.aug_transform = mod.Compose([mod.Resize(**params)])\n    # Cache shape\n    self.shape = shape\n</code></pre>"},{"location":"reference/data_processing/subfunctions/sf_base/","title":"Sf base","text":""},{"location":"reference/data_processing/subfunctions/sf_base/#aucmedi.data_processing.subfunctions.sf_base.Subfunction_Base","title":"<code>Subfunction_Base</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class for a Subfunction class.</p> <p>A child of this ABC can be used as a Subfunction and be passed to a DataGenerator.</p> <p>This class provides functionality for running the transform function, which preprocesses an image during the processing (batch preparation) of the DataGenerator.</p> Create a custom Subfunction <pre><code>from aucmedi.data_processing.subfunctions.sf_base import Subfunction_Base\n\nclass My_custom_Subfunction(Subfunction_Base):\n    def __init__(self):                 # you can pass here class variables\n        pass\n\n    def transform(self, image):\n        new_image = image + 1.0         # do some operation\n        return new_image                # return modified image\n</code></pre> Required Functions Function Description <code>__init__()</code> Object creation function. <code>transform()</code> Transform the image. Source code in <code>aucmedi/data_processing/subfunctions/sf_base.py</code> <pre><code>class Subfunction_Base(ABC):\n    \"\"\" An abstract base class for a Subfunction class.\n\n    A child of this ABC can be used as a [Subfunction][aucmedi.data_processing.subfunctions]\n    and be passed to a [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    This class provides functionality for running the transform function,\n    which preprocesses an image during the processing (batch preparation) of the DataGenerator.\n\n    ???+ example \"Create a custom Subfunction\"\n        ```python\n        from aucmedi.data_processing.subfunctions.sf_base import Subfunction_Base\n\n        class My_custom_Subfunction(Subfunction_Base):\n            def __init__(self):                 # you can pass here class variables\n                pass\n\n            def transform(self, image):\n                new_image = image + 1.0         # do some operation\n                return new_image                # return modified image\n        ```\n\n    ???+ info \"Required Functions\"\n        | Function            | Description                                |\n        | ------------------- | ------------------------------------------ |\n        | `__init__()`        | Object creation function.                  |\n        | `transform()`       | Transform the image.                       |\n\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    @abstractmethod\n    def __init__(self):\n        \"\"\" Functions which will be called during the Subfunction object creation.\n\n        This function can be used to pass variables and options in the Subfunction instance.\n        The are no mandatory required parameters for the initialization.\n        \"\"\"\n        pass\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    @abstractmethod\n    def transform(self, image):\n        \"\"\" Transform the image according to the subfunction during preprocessing (training + prediction).\n\n        It is required to return the transformed image object (as NumPy array).\n\n        Args:\n            image (numpy.ndarray):      Image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)\n\n        Returns:\n            image (numpy.ndarray):      Transformed image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)\n        \"\"\"\n        return image\n</code></pre>"},{"location":"reference/data_processing/subfunctions/sf_base/#aucmedi.data_processing.subfunctions.sf_base.Subfunction_Base.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Functions which will be called during the Subfunction object creation.</p> <p>This function can be used to pass variables and options in the Subfunction instance. The are no mandatory required parameters for the initialization.</p> Source code in <code>aucmedi/data_processing/subfunctions/sf_base.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n    \"\"\" Functions which will be called during the Subfunction object creation.\n\n    This function can be used to pass variables and options in the Subfunction instance.\n    The are no mandatory required parameters for the initialization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/data_processing/subfunctions/sf_base/#aucmedi.data_processing.subfunctions.sf_base.Subfunction_Base.transform","title":"<code>transform(image)</code>  <code>abstractmethod</code>","text":"<p>Transform the image according to the subfunction during preprocessing (training + prediction).</p> <p>It is required to return the transformed image object (as NumPy array).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)</p> required <p>Returns:</p> Name Type Description <code>image</code> <code>numpy.ndarray</code> <p>Transformed image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)</p> Source code in <code>aucmedi/data_processing/subfunctions/sf_base.py</code> <pre><code>@abstractmethod\ndef transform(self, image):\n    \"\"\" Transform the image according to the subfunction during preprocessing (training + prediction).\n\n    It is required to return the transformed image object (as NumPy array).\n\n    Args:\n        image (numpy.ndarray):      Image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)\n\n    Returns:\n        image (numpy.ndarray):      Transformed image encoded as NumPy matrix with 1 or 3 channels. (e.g. 224x224x3)\n    \"\"\"\n    return image\n</code></pre>"},{"location":"reference/data_processing/subfunctions/standardize/","title":"Standardize","text":""},{"location":"reference/data_processing/subfunctions/standardize/#aucmedi.data_processing.subfunctions.standardize.Standardize","title":"<code>Standardize</code>","text":"<p>         Bases: <code>Subfunction_Base</code></p> <p>A Standardization method which utilizes custom normalization functions and the Keras     preprocess_input() functionality in order to normalize intensity value ranges to be     suitable for neural networks.</p> <p>Default mode: <code>\"z-score\"</code></p> <p>Possible modes: <code>[\"z-score\", \"minmax\", \"grayscale\", \"tf\", \"caffe\", \"torch\"]</code></p> Mode Descriptions Mode Description <code>\"z-score\"</code> Sample-wise Z-score normalization (also called Z-transformation). <code>\"minmax\"</code> Sample-wise scaling to range [0,1]. <code>\"grayscale\"</code> Sample-wise scaling to grayscale range [0, 255]. <code>\"caffe\"</code> Will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling. (RGB encoding required!) <code>\"tf\"</code> Will scale pixels between -1 and 1, sample-wise. (Grayscale/RGB encoding required!) <code>\"torch\"</code> Will scale pixels between 0 and 1 and then will normalize each channel with respect to the ImageNet dataset. (RGB encoding required!) Reference - Implementation <p>Keras preprocess_input() for <code>\"tf\", \"caffe\", \"torch\"</code></p> <p>https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/preprocess_input</p> Source code in <code>aucmedi/data_processing/subfunctions/standardize.py</code> <pre><code>class Standardize(Subfunction_Base):\n    \"\"\" A Standardization method which utilizes custom normalization functions and the Keras\n        preprocess_input() functionality in order to normalize intensity value ranges to be\n        suitable for neural networks.\n\n    Default mode: `\"z-score\"`\n\n    Possible modes: `[\"z-score\", \"minmax\", \"grayscale\", \"tf\", \"caffe\", \"torch\"]`\n\n\n    ???+ info \"Mode Descriptions\"\n\n        | Mode                | Description                                                               |\n        | ------------------- | ------------------------------------------------------------------------- |\n        | `\"z-score\"`         | Sample-wise Z-score normalization (also called Z-transformation).         |\n        | `\"minmax\"`          | Sample-wise scaling to range [0,1].                                       |\n        | `\"grayscale\"`       | Sample-wise scaling to grayscale range [0, 255].                          |\n        | `\"caffe\"`           | Will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling. (RGB encoding required!) |\n        | `\"tf\"`              | Will scale pixels between -1 and 1, sample-wise. (Grayscale/RGB encoding required!) |\n        | `\"torch\"`           | Will scale pixels between 0 and 1 and then will normalize each channel with respect to the ImageNet dataset. (RGB encoding required!) |\n\n    ??? abstract \"Reference - Implementation\"\n        Keras preprocess_input() for `\"tf\", \"caffe\", \"torch\"`\n\n        https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/preprocess_input\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, mode=\"z-score\", per_channel=False, smooth=0.000001):\n        \"\"\" Initialization function for creating a Standardize Subfunction which can be passed to a\n            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            mode (str):             Selected mode which standardization/normalization technique should be applied.\n            per_channel (bool):     Option to apply standardization per channel instead of across complete image.\n            smooth (float):         Smoothing factor to avoid zero devisions (epsilon).\n        \"\"\"\n        # Verify mode existence\n        if mode not in [\"z-score\", \"minmax\", \"grayscale\", \"tf\", \"caffe\", \"torch\"]:\n            raise ValueError(\"Subfunction - Standardize: Unknown modus\", mode)\n        # Cache class variables\n        self.mode = mode\n        self.per_channel = per_channel\n        self.e = smooth\n\n    #---------------------------------------------#\n    #                Transformation               #\n    #---------------------------------------------#\n    def transform(self, image):\n        # Apply normalization per channel\n        if self.per_channel:\n            image_norm = image.copy()\n            for c in range(0, image.shape[-1]):\n                image_norm[..., c] = self.normalize(image[..., c])\n        # Apply normalization across complete image\n        else : image_norm = self.normalize(image)\n        # Return standardized image\n        return image_norm\n\n    #---------------------------------------------#\n    #      Internal Function: Normalization       #\n    #---------------------------------------------#\n    def normalize(self, image):\n        # Perform z-score normalization\n        if self.mode == \"z-score\":\n            # Compute mean and standard deviation\n            mean = np.mean(image)\n            std = np.std(image)\n            # Scaling\n            image_norm = (image - mean + self.e) / (std  + self.e)\n        # Perform MinMax normalization between [0,1]\n        elif self.mode == \"minmax\":\n            # Identify minimum and maximum\n            max_value = np.max(image)\n            min_value = np.min(image)\n            # Scaling\n            image_norm = (image - min_value + self.e) / \\\n                         (max_value - min_value + self.e)\n        elif self.mode == \"grayscale\":\n            # Identify minimum and maximum\n            max_value = np.max(image)\n            min_value = np.min(image)\n            # Scaling\n            image_scaled = (image - min_value + self.e) / \\\n                           (max_value - min_value + self.e)\n            image_norm = np.around(image_scaled * 255, decimals=0)\n        else:\n            # Verify if image is in [0,255] format\n            if np.min(image) &lt; 0 or np.max(image) &gt; 255:\n                raise ValueError(\"Subfunction Standardize: Image values are not in range [0,255]!\",\n                    \"Provided min/max values for image are:\", np.min(image), np.max(image),\n                    \"Ensure that all images are normalized to [0,255] before using the following modes:\",\n                    \"['tf', 'caffe', 'torch']\")\n            # Perform architecture standardization\n            image_norm = imagenet_utils.preprocess_input(image, mode=self.mode)\n        # Return normalized image\n        return image_norm\n</code></pre>"},{"location":"reference/data_processing/subfunctions/standardize/#aucmedi.data_processing.subfunctions.standardize.Standardize.__init__","title":"<code>__init__(mode='z-score', per_channel=False, smooth=1e-06)</code>","text":"<p>Initialization function for creating a Standardize Subfunction which can be passed to a     DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Selected mode which standardization/normalization technique should be applied.</p> <code>'z-score'</code> <code>per_channel</code> <code>bool</code> <p>Option to apply standardization per channel instead of across complete image.</p> <code>False</code> <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid zero devisions (epsilon).</p> <code>1e-06</code> Source code in <code>aucmedi/data_processing/subfunctions/standardize.py</code> <pre><code>def __init__(self, mode=\"z-score\", per_channel=False, smooth=0.000001):\n    \"\"\" Initialization function for creating a Standardize Subfunction which can be passed to a\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        mode (str):             Selected mode which standardization/normalization technique should be applied.\n        per_channel (bool):     Option to apply standardization per channel instead of across complete image.\n        smooth (float):         Smoothing factor to avoid zero devisions (epsilon).\n    \"\"\"\n    # Verify mode existence\n    if mode not in [\"z-score\", \"minmax\", \"grayscale\", \"tf\", \"caffe\", \"torch\"]:\n        raise ValueError(\"Subfunction - Standardize: Unknown modus\", mode)\n    # Cache class variables\n    self.mode = mode\n    self.per_channel = per_channel\n    self.e = smooth\n</code></pre>"},{"location":"reference/ensemble/","title":"Ensemble","text":"<p>State-of-the-art and high-performance medical image classification pipelines     are heavily utilizing Ensemble Learning strategies.</p> <p>The idea of Ensemble Learning is to assemble diverse models or multiple predictions and thus boost prediction performance.</p> <p>AUCMEDI currently supports the following Ensemble Learning techniques:</p> Technique Description Augmenting Inference Augmenting (test-time augmentation) function for augmenting unknown images for prediction. Bagging Cross-Validation based Bagging for equal models trained with different sampling. Stacking Ensemble of unequal models with a fitted Metalearner stacked on top of it. Composite Combination of Stacking and Bagging via cross-validation with a fitted Metalearner stacked on top of it. <p>Info</p> <p></p> <p>More information on performance impact of Ensemble Learning in medical image classification can be found here: </p> <p>Dominik M\u00fcller, I\u00f1aki Soto-Rey, Frank Kramer. (2022) An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. arXiv e-print: https://arxiv.org/abs/2201.11440</p>"},{"location":"reference/ensemble/augmenting/","title":"Augmenting","text":""},{"location":"reference/ensemble/augmenting/#aucmedi.ensemble.augmenting.predict_augmenting","title":"<code>predict_augmenting(model, prediction_generator, n_cycles=10, aggregate='mean')</code>","text":"<p>Inference Augmenting function for automatically augmenting unknown images for prediction.</p> <p>The predictions of the augmented images are aggregated via the provided Aggregate function.</p> Example <pre><code># Import libraries\nfrom aucmedi.ensemble import predict_augmenting\nfrom aucmedi import ImageAugmentation, DataGenerator\n\n# Initialize testing DataGenerator with desired Data Augmentation\ntest_aug = ImageAugmentation(flip=True, rotate=True, brightness=False, contrast=False))\ntest_gen = DataGenerator(samples_test, \"images_dir/\",\n                         data_aug=test_aug,\n                         resize=model.meta_input,\n                         standardize_mode=model.meta_standardize)\n\n# Compute predictions via Augmenting\npreds = predict_augmenting(model, test_gen, n_cycles=15, aggregate=\"majority_vote\")\n</code></pre> <p>The inclusion of the Aggregate function can be achieved in multiple ways:</p> <ul> <li>self-initialization with an AUCMEDI Aggregate function,</li> <li>use a string key to call an AUCMEDI Aggregate function by name, or</li> <li>implementing a custom Aggregate function by extending the AUCMEDI base class for Aggregate functions</li> </ul> <p>Info</p> <p>Description and list of implemented Aggregate functions can be found here: Aggregate</p> <p>The Data Augmentation class instance from the DataGenerator will be used for inference augmenting. It can either be predefined or remain <code>None</code>. If the <code>data_aug</code> is <code>None</code>, a Data Augmentation class instance is automatically created which applies rotation and flipping augmentations.</p> Warning <p>The passed DataGenerator will be re-initialized! This can result in redundant image preparation if <code>prepare_images=True</code>.</p> Reference for Ensemble Learning Techniques <p>Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022). An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. arXiv e-print: https://arxiv.org/abs/2201.11440</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>NeuralNetwork</code> <p>Instance of a AUCMEDI neural network class.</p> required <code>prediction_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for Augmenting based inference.</p> required <code>n_cycles</code> <code>int</code> <p>Number of image augmentations, which should be created per sample.</p> <code>10</code> <code>aggregate</code> <code>str or aggregate Function</code> <p>Aggregate function class instance or a string for an AUCMEDI Aggregate function.</p> <code>'mean'</code> Source code in <code>aucmedi/ensemble/augmenting.py</code> <pre><code>def predict_augmenting(model, prediction_generator, n_cycles=10, aggregate=\"mean\"):\n    \"\"\" Inference Augmenting function for automatically augmenting unknown images for prediction.\n\n    The predictions of the augmented images are aggregated via the provided Aggregate function.\n\n    ???+ example\n        ```python\n        # Import libraries\n        from aucmedi.ensemble import predict_augmenting\n        from aucmedi import ImageAugmentation, DataGenerator\n\n        # Initialize testing DataGenerator with desired Data Augmentation\n        test_aug = ImageAugmentation(flip=True, rotate=True, brightness=False, contrast=False))\n        test_gen = DataGenerator(samples_test, \"images_dir/\",\n                                 data_aug=test_aug,\n                                 resize=model.meta_input,\n                                 standardize_mode=model.meta_standardize)\n\n        # Compute predictions via Augmenting\n        preds = predict_augmenting(model, test_gen, n_cycles=15, aggregate=\"majority_vote\")\n        ```\n\n    The inclusion of the Aggregate function can be achieved in multiple ways:\n\n    - self-initialization with an AUCMEDI Aggregate function,\n    - use a string key to call an AUCMEDI Aggregate function by name, or\n    - implementing a custom Aggregate function by extending the [AUCMEDI base class for Aggregate functions][aucmedi.ensemble.aggregate.agg_base]\n\n    !!! info\n        Description and list of implemented Aggregate functions can be found here:\n        [Aggregate][aucmedi.ensemble.aggregate]\n\n    The Data Augmentation class instance from the DataGenerator will be used for inference augmenting.\n    It can either be predefined or remain `None`. If the `data_aug` is `None`, a Data Augmentation class\n    instance is automatically created which applies rotation and flipping augmentations.\n\n    ???+ warning\n        The passed DataGenerator will be re-initialized!\n        This can result in redundant image preparation if `prepare_images=True`.\n\n    ??? reference \"Reference for Ensemble Learning Techniques\"\n        Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022).\n        An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks.\n        arXiv e-print: [https://arxiv.org/abs/2201.11440](https://arxiv.org/abs/2201.11440)\n\n    Args:\n        model (NeuralNetwork):                 Instance of a AUCMEDI neural network class.\n        prediction_generator (DataGenerator):   A data generator which will be used for Augmenting based inference.\n        n_cycles (int):                         Number of image augmentations, which should be created per sample.\n        aggregate (str or aggregate Function):  Aggregate function class instance or a string for an AUCMEDI Aggregate function.\n    \"\"\"\n    # Initialize aggregate function if required\n    if isinstance(aggregate, str) and aggregate in aggregate_dict:\n        agg_fun = aggregate_dict[aggregate]()\n    else : agg_fun = aggregate\n\n    # Initialize image augmentation if none provided (only flip, rotate)\n    if prediction_generator.data_aug is None and len(model.input_shape) == 3:\n        data_aug = ImageAugmentation(flip=True, rotate=True, scale=False,\n                                     brightness=False, contrast=False,\n                                     saturation=False, hue=False, crop=False,\n                                     grid_distortion=False, compression=False,\n                                     gamma=False, gaussian_noise=False,\n                                     gaussian_blur=False, downscaling=False,\n                                     elastic_transform=False)\n    elif prediction_generator.data_aug is None and len(model.input_shape) == 4:\n        data_aug = VolumeAugmentation(flip=True, rotate=True, scale=False,\n                                      brightness=False, contrast=False,\n                                      saturation=False, hue=False, crop=False,\n                                      grid_distortion=False, compression=False,\n                                      gamma=False, gaussian_noise=False,\n                                      gaussian_blur=False, downscaling=False,\n                                      elastic_transform=False)\n    else : data_aug = prediction_generator.data_aug\n    # Multiply sample list for prediction according to number of cycles\n    samples_aug = np.repeat(prediction_generator.samples, n_cycles)\n\n    # Re-initialize DataGenerator for inference\n    aug_gen = DataGenerator(samples_aug,\n                            path_imagedir=prediction_generator.path_imagedir,\n                            labels=None,\n                            metadata=prediction_generator.metadata,\n                            batch_size=prediction_generator.batch_size,\n                            data_aug=data_aug,\n                            seed=prediction_generator.seed,\n                            subfunctions=prediction_generator.subfunctions,\n                            shuffle=False,\n                            standardize_mode=prediction_generator.standardize_mode,\n                            resize=prediction_generator.resize,\n                            grayscale=prediction_generator.grayscale,\n                            prepare_images=prediction_generator.prepare_images,\n                            sample_weights=None,\n                            image_format=prediction_generator.image_format,\n                            loader=prediction_generator.sample_loader,\n                            workers=prediction_generator.workers,\n                            **prediction_generator.kwargs)\n\n    # Compute predictions with provided model\n    preds_all = model.predict(aug_gen)\n\n    # Ensemble inferences via aggregate function\n    preds_ensembled = []\n    for i in range(0, len(prediction_generator.samples)):\n        # Identify subset for a single sample\n        j = i*n_cycles\n        subset = preds_all[j:j+n_cycles]\n        # Aggregate predictions\n        pred_sample = agg_fun.aggregate(subset)\n        # Add prediction to prediction list\n        preds_ensembled.append(pred_sample)\n    # Convert prediction list to NumPy\n    preds_ensembled = np.asarray(preds_ensembled)\n\n    # Return ensembled predictions\n    return preds_ensembled\n</code></pre>"},{"location":"reference/ensemble/bagging/","title":"Bagging","text":""},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging","title":"<code>Bagging</code>","text":"<p>A Bagging class providing functionality for cross-validation based ensemble learning.</p> <p>Homogeneous model ensembles can be defined as multiple models consisting of the same algorithm, hyperparameters, or architecture. The Bagging technique is based on improved training dataset sampling and a popular homogeneous ensemble learning technique. In contrast to a standard single training/validation split, which results in a single model, Bagging consists of training multiple models on randomly drawn subsets from the dataset.</p> <p>In AUCMEDI, a k-fold cross-validation is applied on the dataset resulting in k models.</p> Example <pre><code># Initialize NeuralNetwork model\nmodel = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\n\n# Initialize Bagging object for 3-fold cross-validation\nel = Bagging(model, k_fold=3)\n\n\n# Initialize training DataGenerator for complete training data\ndatagen = DataGenerator(samples_train, \"images_dir/\",\n                        labels=train_labels_ohe, batch_size=3,\n                        resize=model.meta_input,\n                        standardize_mode=model.meta_standardize)\n# Train models\nel.train(datagen, epochs=100)\n\n\n# Initialize testing DataGenerator for testing data\ntest_gen = DataGenerator(samples_test, \"images_dir/\",\n                         resize=model.meta_input,\n                         standardize_mode=model.meta_standardize)\n# Run Inference with majority vote aggregation\npreds = el.predict(test_gen, aggregate=\"majority_vote\")\n</code></pre> <p>Training Time Increase</p> <p>Bagging sequentially performs fitting processes for multiple models (commonly <code>k_fold=3</code> up to <code>k_fold=10</code>), which will drastically increase training time.</p> DataGenerator re-initialization <p>The passed DataGenerator for the train() and predict() function of the Bagging class will be re-initialized!</p> <p>This can result in redundant image preparation if <code>prepare_images=True</code>.</p> NeuralNetwork re-initialization <p>The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!</p> <p>Attention: Metrics are not passed to the processes due to pickling issues.</p> Technical Details <p>For the training and inference process, each model will create an individual process via the Python multiprocessing package.</p> <p>This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs, which is why more and more redundant data pile up with an increasing number of k-fold.</p> <p>Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next fold model.</p> Reference for Ensemble Learning Techniques <p>Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022). An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. arXiv e-print: https://arxiv.org/abs/2201.11440</p> Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>class Bagging:\n    \"\"\" A Bagging class providing functionality for cross-validation based ensemble learning.\n\n    Homogeneous model ensembles can be defined as multiple models consisting of the same algorithm, hyperparameters,\n    or architecture. The Bagging technique is based on improved training dataset sampling and a popular homogeneous\n    ensemble learning technique. In contrast to a standard single training/validation split, which results in a single\n    model, Bagging consists of training multiple models on randomly drawn subsets from the dataset.\n\n    In AUCMEDI, a k-fold cross-validation is applied on the dataset resulting in k models.\n\n    ???+ example\n        ```python\n        # Initialize NeuralNetwork model\n        model = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\n\n        # Initialize Bagging object for 3-fold cross-validation\n        el = Bagging(model, k_fold=3)\n\n\n        # Initialize training DataGenerator for complete training data\n        datagen = DataGenerator(samples_train, \"images_dir/\",\n                                labels=train_labels_ohe, batch_size=3,\n                                resize=model.meta_input,\n                                standardize_mode=model.meta_standardize)\n        # Train models\n        el.train(datagen, epochs=100)\n\n\n        # Initialize testing DataGenerator for testing data\n        test_gen = DataGenerator(samples_test, \"images_dir/\",\n                                 resize=model.meta_input,\n                                 standardize_mode=model.meta_standardize)\n        # Run Inference with majority vote aggregation\n        preds = el.predict(test_gen, aggregate=\"majority_vote\")\n        ```\n\n    !!! warning \"Training Time Increase\"\n        Bagging sequentially performs fitting processes for multiple models (commonly `k_fold=3` up to `k_fold=10`),\n        which will drastically increase training time.\n\n    ??? warning \"DataGenerator re-initialization\"\n        The passed DataGenerator for the train() and predict() function of the Bagging class will be re-initialized!\n\n        This can result in redundant image preparation if `prepare_images=True`.\n\n    ??? warning \"NeuralNetwork re-initialization\"\n        The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!\n\n        Attention: Metrics are not passed to the processes due to pickling issues.\n\n    ??? info \"Technical Details\"\n        For the training and inference process, each model will create an individual process via the Python multiprocessing package.\n\n        This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs,\n        which is why more and more redundant data pile up with an increasing number of k-fold.\n\n        Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next fold model.\n\n    ??? reference \"Reference for Ensemble Learning Techniques\"\n        Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022).\n        An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks.\n        arXiv e-print: [https://arxiv.org/abs/2201.11440](https://arxiv.org/abs/2201.11440)\n    \"\"\"\n    def __init__(self, model, k_fold=3):\n        \"\"\" Initialization function for creating a Bagging object.\n\n        Args:\n            model (NeuralNetwork):         Instance of an AUCMEDI neural network class.\n            k_fold (int):                   Number of folds (k) for the Cross-Validation. Must be at least 2.\n        \"\"\"\n        # Cache class variables\n        self.model_template = model\n        self.k_fold = k_fold\n        self.cache_dir = None\n\n        # Set multiprocessing method to spawn\n        mp.set_start_method(\"spawn\", force=True)\n\n    def train(self, training_generator, epochs=20, iterations=None,\n              callbacks=[], class_weights=None, transfer_learning=False):\n        \"\"\" Training function for the Bagging models which performs a k-fold cross-validation model fitting.\n\n        The training data will be sampled according to a k-fold cross-validation in which a validation\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator] will be automatically created.\n\n        It is also possible to pass custom Callback classes in order to obtain more information.\n\n        For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training (will be split according to k-fold sampling).\n            epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                    the complete data set.\n            iterations (int):                       Number of iterations (batches) in a single epoch.\n            callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n            class_weights (dictionary or list):     A list or dictionary of float values to handle class imbalance.\n            transfer_learning (bool):               Option whether a transfer learning training should be performed.\n\n        Returns:\n            history (dict):                   A history dictionary from a Keras history object which contains several logs.\n        \"\"\"\n        temp_dg = training_generator    # Template DataGenerator variable for faster access\n        history_bagging = {}            # Final history dictionary\n\n        # Create temporary model directory\n        self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                     suffix=\".bagging\")\n\n        # Obtain training data\n        x = training_generator.samples\n        y = training_generator.labels\n        m = training_generator.metadata\n\n        # Apply cross-validaton sampling\n        cv_sampling = sampling_kfold(x, y, m, n_splits=self.k_fold,\n                                     stratified=True, iterative=True)\n\n        # Sequentially iterate over all folds\n        for i, fold in enumerate(cv_sampling):\n            # Pack data into a tuple\n            if len(fold) == 4:\n                (train_x, train_y, test_x, test_y) = fold\n                data = (train_x, train_y, None, test_x, test_y, None)\n            else : data = fold\n\n            # Create model specific callback list\n            callbacks_model = callbacks.copy()\n            # Extend Callback list\n            cb_mc = ModelCheckpoint(os.path.join(self.cache_dir.name,\n                                                 \"cv_\" + str(i) + \\\n                                                 \".model.keras\"),\n                                    monitor=\"val_loss\", verbose=1,\n                                    save_best_only=True, mode=\"min\")\n            cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                                 \"cv_\" + str(i) + \\\n                                                 \".logs.csv\"),\n                              separator=',', append=True)\n            callbacks_model.extend([cb_mc, cb_cl])\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_template.n_labels,\n                \"channels\": self.model_template.channels,\n                \"input_shape\": self.model_template.input_shape,\n                \"architecture\": self.model_template.architecture,\n                \"pretrained_weights\": self.model_template.pretrained_weights,\n                \"loss\": self.model_template.loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_template.activation_output,\n                \"fcl_dropout\": self.model_template.fcl_dropout,\n                \"meta_variables\": self.model_template.meta_variables,\n                \"learning_rate\": self.model_template.learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": temp_dg.standardize_mode,\n                             \"resize\": temp_dg.resize,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Gather training parameters\n            parameters_training = {\"epochs\": epochs,\n                                   \"iterations\": iterations,\n                                   \"callbacks\": callbacks_model,\n                                   \"class_weights\": class_weights,\n                                   \"transfer_learning\": transfer_learning\n            }\n\n            # Start training process\n            process_queue = mp.Queue()\n            process_train = mp.Process(target=__training_process__,\n                                       args=(process_queue,\n                                             model_paras,\n                                             data,\n                                             datagen_paras,\n                                             parameters_training))\n            process_train.start()\n            process_train.join()\n            cv_history = process_queue.get()\n            # Combine logged history objects\n            hcv = {\"cv_\" + str(i) + \".\" + k: v for k, v in cv_history.items()}\n            history_bagging = {**history_bagging, **hcv}\n\n        # Return Bagging history object\n        return history_bagging\n\n    def predict(self, prediction_generator, aggregate=\"mean\",\n                return_ensemble=False):\n        \"\"\" Prediction function for the Bagging models.\n\n        The fitted models will predict classifications for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        The inclusion of the Aggregate function can be achieved in multiple ways:\n\n        - self-initialization with an AUCMEDI Aggregate function,\n        - use a string key to call an AUCMEDI Aggregate function by name, or\n        - implementing a custom Aggregate function by extending the [AUCMEDI base class for Aggregate functions][aucmedi.ensemble.aggregate.agg_base]\n\n        !!! info\n            Description and list of implemented Aggregate functions can be found here:\n            [Aggregate][aucmedi.ensemble.aggregate]\n\n        Args:\n            prediction_generator (DataGenerator):   A data generator which will be used for inference.\n            aggregate (str or aggregate Function):  Aggregate function class instance or a string for an AUCMEDI Aggregate function.\n            return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n        Returns:\n            preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n            ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                    Shape (n_models, n_samples, n_labels).\n        \"\"\"\n        # Verify if there is a linked cache dictionary\n        con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n                   os.path.exists(self.cache_dir.name))\n        con_var = (self.cache_dir is not None and \\\n                   not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n                   and os.path.exists(self.cache_dir))\n        if not con_tmp and not con_var:\n            raise FileNotFoundError(\"Bagging does not have a valid model cache directory!\")\n\n        # Initialize aggregate function if required\n        if isinstance(aggregate, str) and aggregate in aggregate_dict:\n            agg_fun = aggregate_dict[aggregate]()\n        else : agg_fun = aggregate\n\n        # Initialize some variables\n        temp_dg = prediction_generator\n        preds_ensemble = []\n        preds_final = []\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"samples\": temp_dg.samples,\n                         \"metadata\": temp_dg.metadata,\n                         \"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": temp_dg.standardize_mode,\n                         \"resize\": temp_dg.resize,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Identify path to model directory\n        if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            path_model_dir = self.cache_dir.name\n        else : path_model_dir = self.cache_dir\n\n        # Sequentially iterate over all fold models\n        for i in range(self.k_fold):\n            # Identify path to fitted model\n            path_model = os.path.join(path_model_dir,\n                                      \"cv_\" + str(i) + \".model.keras\")\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_template.n_labels,\n                \"channels\": self.model_template.channels,\n                \"input_shape\": self.model_template.input_shape,\n                \"architecture\": self.model_template.architecture,\n                \"pretrained_weights\": self.model_template.pretrained_weights,\n                \"loss\": self.model_template.loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_template.activation_output,\n                \"fcl_dropout\": self.model_template.fcl_dropout,\n                \"meta_variables\": self.model_template.meta_variables,\n                \"learning_rate\": self.model_template.learning_rate,\n            }\n\n            # Start inference process for fold i\n            process_queue = mp.Queue()\n            process_pred = mp.Process(target=__prediction_process__,\n                                      args=(process_queue,\n                                            model_paras,\n                                            path_model,\n                                            datagen_paras))\n            process_pred.start()\n            process_pred.join()\n            preds = process_queue.get()\n\n            # Append to prediction ensemble\n            preds_ensemble.append(preds)\n\n        # Aggregate predictions\n        preds_ensemble = np.array(preds_ensemble)\n        for i in range(0, len(temp_dg.samples)):\n            pred_sample = agg_fun.aggregate(preds_ensemble[:,i,:])\n            preds_final.append(pred_sample)\n\n        # Convert prediction list to NumPy\n        preds_final = np.asarray(preds_final)\n\n        # Return ensembled predictions\n        if return_ensemble : return preds_final, preds_ensemble\n        else : return preds_final\n\n    # Dump model to file\n    def dump(self, directory_path):\n        \"\"\" Store temporary Bagging model directory permanently to disk at desired location.\n\n        If the model directory is a provided path which is already persistent on the disk,\n        the directory is copied in order to keep original data persistent.\n\n        Args:\n            directory_path (str):       Path to store the model directory on disk.\n        \"\"\"\n        if self.cache_dir is None:\n            raise FileNotFoundError(\"Bagging does not have a valid model cache directory!\")\n        elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            shutil.copytree(self.cache_dir.name, directory_path,\n                            dirs_exist_ok=True)\n            self.cache_dir.cleanup()\n            self.cache_dir = directory_path\n        else:\n            shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n            self.cache_dir = directory_path\n\n    # Load model from file\n    def load(self, directory_path):\n        \"\"\" Load a Bagging model directory which can be used for aggregated inference.\n\n        Args:\n            directory_path (str):       Input path, from which the Bagging models will be loaded.\n        \"\"\"\n        # Check directory existence\n        if not os.path.exists(directory_path):\n            raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                    directory_path)\n        # Check model existence\n        for i in range(self.k_fold):\n            path_model = os.path.join(directory_path,\n                                      \"cv_\" + str(i) + \".model.keras\")\n            if not os.path.exists(path_model):\n                raise FileNotFoundError(\"Bagging model for fold \" + str(i) + \\\n                                        \" does not exist!\", path_model)\n        # Update model directory\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging.__init__","title":"<code>__init__(model, k_fold=3)</code>","text":"<p>Initialization function for creating a Bagging object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>NeuralNetwork</code> <p>Instance of an AUCMEDI neural network class.</p> required <code>k_fold</code> <code>int</code> <p>Number of folds (k) for the Cross-Validation. Must be at least 2.</p> <code>3</code> Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>def __init__(self, model, k_fold=3):\n    \"\"\" Initialization function for creating a Bagging object.\n\n    Args:\n        model (NeuralNetwork):         Instance of an AUCMEDI neural network class.\n        k_fold (int):                   Number of folds (k) for the Cross-Validation. Must be at least 2.\n    \"\"\"\n    # Cache class variables\n    self.model_template = model\n    self.k_fold = k_fold\n    self.cache_dir = None\n\n    # Set multiprocessing method to spawn\n    mp.set_start_method(\"spawn\", force=True)\n</code></pre>"},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging.dump","title":"<code>dump(directory_path)</code>","text":"<p>Store temporary Bagging model directory permanently to disk at desired location.</p> <p>If the model directory is a provided path which is already persistent on the disk, the directory is copied in order to keep original data persistent.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Path to store the model directory on disk.</p> required Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>def dump(self, directory_path):\n    \"\"\" Store temporary Bagging model directory permanently to disk at desired location.\n\n    If the model directory is a provided path which is already persistent on the disk,\n    the directory is copied in order to keep original data persistent.\n\n    Args:\n        directory_path (str):       Path to store the model directory on disk.\n    \"\"\"\n    if self.cache_dir is None:\n        raise FileNotFoundError(\"Bagging does not have a valid model cache directory!\")\n    elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        shutil.copytree(self.cache_dir.name, directory_path,\n                        dirs_exist_ok=True)\n        self.cache_dir.cleanup()\n        self.cache_dir = directory_path\n    else:\n        shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging.load","title":"<code>load(directory_path)</code>","text":"<p>Load a Bagging model directory which can be used for aggregated inference.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Input path, from which the Bagging models will be loaded.</p> required Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>def load(self, directory_path):\n    \"\"\" Load a Bagging model directory which can be used for aggregated inference.\n\n    Args:\n        directory_path (str):       Input path, from which the Bagging models will be loaded.\n    \"\"\"\n    # Check directory existence\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                directory_path)\n    # Check model existence\n    for i in range(self.k_fold):\n        path_model = os.path.join(directory_path,\n                                  \"cv_\" + str(i) + \".model.keras\")\n        if not os.path.exists(path_model):\n            raise FileNotFoundError(\"Bagging model for fold \" + str(i) + \\\n                                    \" does not exist!\", path_model)\n    # Update model directory\n    self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging.predict","title":"<code>predict(prediction_generator, aggregate='mean', return_ensemble=False)</code>","text":"<p>Prediction function for the Bagging models.</p> <p>The fitted models will predict classifications for the provided DataGenerator.</p> <p>The inclusion of the Aggregate function can be achieved in multiple ways:</p> <ul> <li>self-initialization with an AUCMEDI Aggregate function,</li> <li>use a string key to call an AUCMEDI Aggregate function by name, or</li> <li>implementing a custom Aggregate function by extending the AUCMEDI base class for Aggregate functions</li> </ul> <p>Info</p> <p>Description and list of implemented Aggregate functions can be found here: Aggregate</p> <p>Parameters:</p> Name Type Description Default <code>prediction_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for inference.</p> required <code>aggregate</code> <code>str or aggregate Function</code> <p>Aggregate function class instance or a string for an AUCMEDI Aggregate function.</p> <code>'mean'</code> <code>return_ensemble</code> <code>bool</code> <p>Option, whether gathered ensemble of predictions should be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels).</p> <code>ensemble</code> <code>numpy.ndarray</code> <p>Optional ensemble of predictions: Will be only passed if <code>return_ensemble=True</code>.                                     Shape (n_models, n_samples, n_labels).</p> Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>def predict(self, prediction_generator, aggregate=\"mean\",\n            return_ensemble=False):\n    \"\"\" Prediction function for the Bagging models.\n\n    The fitted models will predict classifications for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    The inclusion of the Aggregate function can be achieved in multiple ways:\n\n    - self-initialization with an AUCMEDI Aggregate function,\n    - use a string key to call an AUCMEDI Aggregate function by name, or\n    - implementing a custom Aggregate function by extending the [AUCMEDI base class for Aggregate functions][aucmedi.ensemble.aggregate.agg_base]\n\n    !!! info\n        Description and list of implemented Aggregate functions can be found here:\n        [Aggregate][aucmedi.ensemble.aggregate]\n\n    Args:\n        prediction_generator (DataGenerator):   A data generator which will be used for inference.\n        aggregate (str or aggregate Function):  Aggregate function class instance or a string for an AUCMEDI Aggregate function.\n        return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n    Returns:\n        preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n        ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                Shape (n_models, n_samples, n_labels).\n    \"\"\"\n    # Verify if there is a linked cache dictionary\n    con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n               os.path.exists(self.cache_dir.name))\n    con_var = (self.cache_dir is not None and \\\n               not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n               and os.path.exists(self.cache_dir))\n    if not con_tmp and not con_var:\n        raise FileNotFoundError(\"Bagging does not have a valid model cache directory!\")\n\n    # Initialize aggregate function if required\n    if isinstance(aggregate, str) and aggregate in aggregate_dict:\n        agg_fun = aggregate_dict[aggregate]()\n    else : agg_fun = aggregate\n\n    # Initialize some variables\n    temp_dg = prediction_generator\n    preds_ensemble = []\n    preds_final = []\n\n    # Gather DataGenerator parameters\n    datagen_paras = {\"samples\": temp_dg.samples,\n                     \"metadata\": temp_dg.metadata,\n                     \"path_imagedir\": temp_dg.path_imagedir,\n                     \"batch_size\": temp_dg.batch_size,\n                     \"data_aug\": temp_dg.data_aug,\n                     \"seed\": temp_dg.seed,\n                     \"subfunctions\": temp_dg.subfunctions,\n                     \"shuffle\": temp_dg.shuffle,\n                     \"standardize_mode\": temp_dg.standardize_mode,\n                     \"resize\": temp_dg.resize,\n                     \"grayscale\": temp_dg.grayscale,\n                     \"prepare_images\": temp_dg.prepare_images,\n                     \"sample_weights\": temp_dg.sample_weights,\n                     \"image_format\": temp_dg.image_format,\n                     \"loader\": temp_dg.sample_loader,\n                     \"workers\": temp_dg.workers,\n                     \"kwargs\": temp_dg.kwargs\n    }\n\n    # Identify path to model directory\n    if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        path_model_dir = self.cache_dir.name\n    else : path_model_dir = self.cache_dir\n\n    # Sequentially iterate over all fold models\n    for i in range(self.k_fold):\n        # Identify path to fitted model\n        path_model = os.path.join(path_model_dir,\n                                  \"cv_\" + str(i) + \".model.keras\")\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_template.n_labels,\n            \"channels\": self.model_template.channels,\n            \"input_shape\": self.model_template.input_shape,\n            \"architecture\": self.model_template.architecture,\n            \"pretrained_weights\": self.model_template.pretrained_weights,\n            \"loss\": self.model_template.loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_template.activation_output,\n            \"fcl_dropout\": self.model_template.fcl_dropout,\n            \"meta_variables\": self.model_template.meta_variables,\n            \"learning_rate\": self.model_template.learning_rate,\n        }\n\n        # Start inference process for fold i\n        process_queue = mp.Queue()\n        process_pred = mp.Process(target=__prediction_process__,\n                                  args=(process_queue,\n                                        model_paras,\n                                        path_model,\n                                        datagen_paras))\n        process_pred.start()\n        process_pred.join()\n        preds = process_queue.get()\n\n        # Append to prediction ensemble\n        preds_ensemble.append(preds)\n\n    # Aggregate predictions\n    preds_ensemble = np.array(preds_ensemble)\n    for i in range(0, len(temp_dg.samples)):\n        pred_sample = agg_fun.aggregate(preds_ensemble[:,i,:])\n        preds_final.append(pred_sample)\n\n    # Convert prediction list to NumPy\n    preds_final = np.asarray(preds_final)\n\n    # Return ensembled predictions\n    if return_ensemble : return preds_final, preds_ensemble\n    else : return preds_final\n</code></pre>"},{"location":"reference/ensemble/bagging/#aucmedi.ensemble.bagging.Bagging.train","title":"<code>train(training_generator, epochs=20, iterations=None, callbacks=[], class_weights=None, transfer_learning=False)</code>","text":"<p>Training function for the Bagging models which performs a k-fold cross-validation model fitting.</p> <p>The training data will be sampled according to a k-fold cross-validation in which a validation DataGenerator will be automatically created.</p> <p>It is also possible to pass custom Callback classes in order to obtain more information.</p> <p>For more information on the fitting process, check out NeuralNetwork.train().</p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training (will be split according to k-fold sampling).</p> required <code>epochs</code> <code>int</code> <p>Number of epochs. A single epoch is defined as one iteration through                                     the complete data set.</p> <code>20</code> <code>iterations</code> <code>int</code> <p>Number of iterations (batches) in a single epoch.</p> <code>None</code> <code>callbacks</code> <code>list of Callback classes</code> <p>A list of Callback classes for custom evaluation.</p> <code>[]</code> <code>class_weights</code> <code>dictionary or list</code> <p>A list or dictionary of float values to handle class imbalance.</p> <code>None</code> <code>transfer_learning</code> <code>bool</code> <p>Option whether a transfer learning training should be performed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> Source code in <code>aucmedi/ensemble/bagging.py</code> <pre><code>def train(self, training_generator, epochs=20, iterations=None,\n          callbacks=[], class_weights=None, transfer_learning=False):\n    \"\"\" Training function for the Bagging models which performs a k-fold cross-validation model fitting.\n\n    The training data will be sampled according to a k-fold cross-validation in which a validation\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator] will be automatically created.\n\n    It is also possible to pass custom Callback classes in order to obtain more information.\n\n    For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training (will be split according to k-fold sampling).\n        epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                the complete data set.\n        iterations (int):                       Number of iterations (batches) in a single epoch.\n        callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n        class_weights (dictionary or list):     A list or dictionary of float values to handle class imbalance.\n        transfer_learning (bool):               Option whether a transfer learning training should be performed.\n\n    Returns:\n        history (dict):                   A history dictionary from a Keras history object which contains several logs.\n    \"\"\"\n    temp_dg = training_generator    # Template DataGenerator variable for faster access\n    history_bagging = {}            # Final history dictionary\n\n    # Create temporary model directory\n    self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                 suffix=\".bagging\")\n\n    # Obtain training data\n    x = training_generator.samples\n    y = training_generator.labels\n    m = training_generator.metadata\n\n    # Apply cross-validaton sampling\n    cv_sampling = sampling_kfold(x, y, m, n_splits=self.k_fold,\n                                 stratified=True, iterative=True)\n\n    # Sequentially iterate over all folds\n    for i, fold in enumerate(cv_sampling):\n        # Pack data into a tuple\n        if len(fold) == 4:\n            (train_x, train_y, test_x, test_y) = fold\n            data = (train_x, train_y, None, test_x, test_y, None)\n        else : data = fold\n\n        # Create model specific callback list\n        callbacks_model = callbacks.copy()\n        # Extend Callback list\n        cb_mc = ModelCheckpoint(os.path.join(self.cache_dir.name,\n                                             \"cv_\" + str(i) + \\\n                                             \".model.keras\"),\n                                monitor=\"val_loss\", verbose=1,\n                                save_best_only=True, mode=\"min\")\n        cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                             \"cv_\" + str(i) + \\\n                                             \".logs.csv\"),\n                          separator=',', append=True)\n        callbacks_model.extend([cb_mc, cb_cl])\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_template.n_labels,\n            \"channels\": self.model_template.channels,\n            \"input_shape\": self.model_template.input_shape,\n            \"architecture\": self.model_template.architecture,\n            \"pretrained_weights\": self.model_template.pretrained_weights,\n            \"loss\": self.model_template.loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_template.activation_output,\n            \"fcl_dropout\": self.model_template.fcl_dropout,\n            \"meta_variables\": self.model_template.meta_variables,\n            \"learning_rate\": self.model_template.learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": temp_dg.standardize_mode,\n                         \"resize\": temp_dg.resize,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Gather training parameters\n        parameters_training = {\"epochs\": epochs,\n                               \"iterations\": iterations,\n                               \"callbacks\": callbacks_model,\n                               \"class_weights\": class_weights,\n                               \"transfer_learning\": transfer_learning\n        }\n\n        # Start training process\n        process_queue = mp.Queue()\n        process_train = mp.Process(target=__training_process__,\n                                   args=(process_queue,\n                                         model_paras,\n                                         data,\n                                         datagen_paras,\n                                         parameters_training))\n        process_train.start()\n        process_train.join()\n        cv_history = process_queue.get()\n        # Combine logged history objects\n        hcv = {\"cv_\" + str(i) + \".\" + k: v for k, v in cv_history.items()}\n        history_bagging = {**history_bagging, **hcv}\n\n    # Return Bagging history object\n    return history_bagging\n</code></pre>"},{"location":"reference/ensemble/composite/","title":"Composite","text":""},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite","title":"<code>Composite</code>","text":"<p>A Composite class providing functionality for cross-validation and metalearner based ensemble learning.</p> <p>The Composite strategy combines the homogeneous Bagging and the heterogeneous Stacking technique.</p> <p>If a metalearner is selected, a percentage sampling split is applied. For an aggregate function, this is not done. The remaining training data is sampled via a cross-validation. For each fold, a different model is trained returning into a heterogenous ensemble. Predictions for this heterogenous ensemble are combined with the fitted metalearner model or an aggregate function.</p> <p>Instead of utilizing the fixed parameters of the DataGenerator, default paramters for Resizing and Standardize of the associated models are used (if <code>fixed_datagenerator=True</code>).</p> Example <pre><code># Initialize some NeuralNetwork models\nmodel_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\nmodel_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\nmodel_c = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.EfficientNetB1\")\n\n# Initialize Composite object\nel = Composite(model_list=[model_a, model_b, model_c],\n               metalearner=\"logistic_regression\", k_fold=3)\n\n# Initialize training DataGenerator for complete training data\ndatagen = DataGenerator(samples_train, \"images_dir/\",\n                        labels=train_labels_ohe, batch_size=3,\n                        resize=None, standardize_mode=None)\n# Train neural network and metalearner models\nel.train(datagen, epochs=100)\n\n# Initialize testing DataGenerator for testing data\ntest_gen = DataGenerator(samples_test, \"images_dir/\",\n                         resize=None, standardize_mode=None)\n# Run Inference\npreds = el.predict(test_gen)\n</code></pre> <p>Training Time Increase</p> <p>Composite sequentially performs fitting processes for multiple models, which will drastically increase training time.</p> DataGenerator re-initialization <p>The passed DataGenerator for the train() and predict() function of the Composite class will be re-initialized!</p> <p>This can result in redundant image preparation if <code>prepare_images=True</code>.</p> <p>Furthermore, the parameters <code>resize</code> and <code>standardize_mode</code> are automatically re-initialized with NeuralNetwork model specific values (<code>model.meta_standardize</code> for <code>standardize_mode</code> and <code>model.meta_input</code> for <code>input_shape</code>).</p> <p>If desired (but not recommended!), it is possible to modify the meta variables of the NeuralNetwork model as follows: <pre><code># For input_shape\nmodel_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\",\n                        input_shape=(64,64))\n# For standardize_mode\nmodel_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\nmodel_b.meta_standardize = \"torch\"\n</code></pre></p> NeuralNetwork re-initialization <p>The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!</p> <p>Attention: Metrics are not passed to the processes due to pickling issues.</p> Technical Details <p>For the training and inference process, each model will create an individual process via the Python multiprocessing package.</p> <p>This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs, which is why more and more redundant data pile up with an increasing number of models.</p> <p>Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next model.</p> Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>class Composite:\n    \"\"\" A Composite class providing functionality for cross-validation and metalearner based ensemble learning.\n\n    The Composite strategy combines the homogeneous [Bagging][aucmedi.ensemble.Bagging] and the heterogeneous\n    [Stacking][aucmedi.ensemble.Stacking] technique.\n\n    If a metalearner is selected, a percentage sampling split is applied. For an aggregate function, this is not done.\n    The remaining training data is sampled via a cross-validation. For each fold, a different model is trained\n    returning into a heterogenous ensemble.\n    Predictions for this heterogenous ensemble are combined with the fitted metalearner model or an aggregate function.\n\n    Instead of utilizing the fixed parameters of the [DataGenerator][aucmedi.data_processing.data_generator],\n    default paramters for Resizing and Standardize of the associated models are used (if `fixed_datagenerator=True`).\n\n    ???+ example\n        ```python\n        # Initialize some NeuralNetwork models\n        model_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\n        model_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\n        model_c = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.EfficientNetB1\")\n\n        # Initialize Composite object\n        el = Composite(model_list=[model_a, model_b, model_c],\n                       metalearner=\"logistic_regression\", k_fold=3)\n\n        # Initialize training DataGenerator for complete training data\n        datagen = DataGenerator(samples_train, \"images_dir/\",\n                                labels=train_labels_ohe, batch_size=3,\n                                resize=None, standardize_mode=None)\n        # Train neural network and metalearner models\n        el.train(datagen, epochs=100)\n\n        # Initialize testing DataGenerator for testing data\n        test_gen = DataGenerator(samples_test, \"images_dir/\",\n                                 resize=None, standardize_mode=None)\n        # Run Inference\n        preds = el.predict(test_gen)\n        ```\n\n    !!! warning \"Training Time Increase\"\n        Composite sequentially performs fitting processes for multiple models, which will drastically increase training time.\n\n    ??? warning \"DataGenerator re-initialization\"\n        The passed DataGenerator for the train() and predict() function of the Composite class will be re-initialized!\n\n        This can result in redundant image preparation if `prepare_images=True`.\n\n        Furthermore, the parameters `resize` and `standardize_mode` are automatically re-initialized with\n        NeuralNetwork model specific values (`model.meta_standardize` for `standardize_mode` and\n        `model.meta_input` for `input_shape`).\n\n        If desired (but not recommended!), it is possible to modify the meta variables of the NeuralNetwork model as follows:\n        ```python\n        # For input_shape\n        model_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\",\n                                input_shape=(64,64))\n        # For standardize_mode\n        model_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\n        model_b.meta_standardize = \"torch\"\n        ```\n\n    ??? warning \"NeuralNetwork re-initialization\"\n        The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!\n\n        Attention: Metrics are not passed to the processes due to pickling issues.\n\n    ??? info \"Technical Details\"\n        For the training and inference process, each model will create an individual process via the Python multiprocessing package.\n\n        This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs,\n        which is why more and more redundant data pile up with an increasing number of models.\n\n        Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next model.\n    \"\"\"\n    def __init__(self, model_list, metalearner=\"logistic_regression\",\n                 k_fold=3, sampling=[0.85, 0.15], fixed_datagenerator=False):\n        \"\"\" Initialization function for creating a Composite object.\n\n        Args:\n            model_list (list of NeuralNetwork):         List of instances of AUCMEDI neural network class.\n                                                        The number of models (`len(model_list)`) have to be equal to `k_fold`.\n            metalearner (str, Metalearner or Aggregate):Metalearner class instance / a string for an AUCMEDI Metalearner,\n                                                        or Aggregate function / a string for an AUCMEDI Aggregate function.\n            k_fold (int):                               Number of folds (k) for the Cross-Validation. Must be at least 2.\n            sampling (list of float):                   List of percentage values with split sizes. Should be 2x percentage values\n                                                        for heterogenous metalearner (must sum up to 1.0).\n            fixed_datagenerator (bool):                 Boolean, whether using fixed parameters of passed DataGenerator or\n                                                        using default architecture paramters for Resizing and Standardize.\n        \"\"\"\n        # Cache class variables\n        self.model_list = model_list\n        self.metalearner = metalearner\n        self.sampling = sampling\n        self.k_fold = k_fold\n        self.fixed_datagenerator = fixed_datagenerator\n        self.sampling_seed = 0\n        self.cache_dir = None\n\n        # Initialize Metalearner\n        if isinstance(metalearner, str) and metalearner in metalearner_dict:\n            self.ml_model = metalearner_dict[metalearner]()\n        elif isinstance(metalearner, str) and metalearner in aggregate_dict:\n            self.ml_model = aggregate_dict[metalearner]()\n        elif isinstance(metalearner, Metalearner_Base) or \\\n             isinstance(metalearner, Aggregate_Base):\n            self.ml_model = metalearner\n        else : raise TypeError(\"Unknown type of Metalearner (neither known \" + \\\n                               \"ensembler nor Aggregate or Metalearner class)!\")\n\n        # Verify model list length\n        if k_fold != len(model_list):\n            raise ValueError(\"Length of model_list and k_fold has to be equal!\")\n\n        # Set multiprocessing method to spawn\n        mp.set_start_method(\"spawn\", force=True)\n\n    def train(self, training_generator, epochs=20, iterations=None,\n              callbacks=[], class_weights=None, transfer_learning=False,\n              metalearner_fitting=True):\n        \"\"\" Training function for fitting the provided NeuralNetwork models.\n\n        The training data will be sampled according to a percentage split in which\n        [DataGenerators][aucmedi.data_processing.data_generator.DataGenerator] for model training\n        and metalearner training if a metalearner is provided. Else all data is used as model\n        training subset. The model training subset is furthermore sampled via cross-validation.\n\n        It is also possible to pass custom Callback classes in order to obtain more information.\n\n        For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                    to percentage split and k-fold cross-validation sampling).\n            epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                    the complete data set.\n            iterations (int):                       Number of iterations (batches) in a single epoch.\n            callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n            class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n            transfer_learning (bool):               Option whether a transfer learning training should be performed.\n            metalearner_fitting (bool):             Option whether the Metalearner fitting process should be included in the\n                                                    Composite training process. The `train_metalearner()` function can also be\n                                                    run manually (or repeatedly).\n        Returns:\n            history (dict):                         A history dictionary from a Keras history object which contains several logs.\n        \"\"\"\n        temp_dg = training_generator    # Template DataGenerator variable for faster access\n        history_composite = {}           # Final history dictionary\n\n        # Create temporary model directory\n        self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                     suffix=\".composite\")\n\n        # Obtain training data\n        x = training_generator.samples\n        y = training_generator.labels\n        m = training_generator.metadata\n\n        # Apply percentage split sampling for metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                         stratified=True, iterative=True,\n                                         seed=self.sampling_seed)\n            # Pack data according to sampling\n            if len(ps_sampling[0]) == 3 : x, y, m = ps_sampling[0]\n            else : x, y = ps_sampling[0]\n\n        # Apply cross-validaton sampling\n        cv_sampling = sampling_kfold(x, y, m, n_splits=self.k_fold,\n                                     stratified=True, iterative=True)\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            # Pack data into a tuple\n            fold = cv_sampling[i]\n            if len(fold) == 4:\n                (train_x, train_y, test_x, test_y) = fold\n                data = (train_x, train_y, None, test_x, test_y, None)\n            else : data = fold\n\n            # Create model specific callback list\n            callbacks_model = callbacks.copy()\n            # Extend Callback list\n            path_model = os.path.join(self.cache_dir.name,\n                                      \"cv_\" + str(i) + \".model.keras\")\n            cb_mc = ModelCheckpoint(path_model,\n                                    monitor=\"val_loss\", verbose=1,\n                                    save_best_only=True, mode=\"min\")\n            cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                                 \"cv_\" + str(i) + \\\n                                                 \".logs.csv\"),\n                              separator=',', append=True)\n            callbacks_model.extend([cb_mc, cb_cl])\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Gather training parameters\n            parameters_training = {\"epochs\": epochs,\n                                \"iterations\": iterations,\n                                \"callbacks\": callbacks_model,\n                                \"class_weights\": class_weights,\n                                \"transfer_learning\": transfer_learning\n            }\n\n            # Start training process\n            process_queue = mp.Queue()\n            process_train = mp.Process(target=__training_process__,\n                                       args=(process_queue,\n                                             data,\n                                             model_paras,\n                                             datagen_paras,\n                                             parameters_training))\n            process_train.start()\n            process_train.join()\n            cv_history = process_queue.get()\n            # Combine logged history objects\n            hnn = {\"cv_\" + str(i) + \".\" + k: v for k, v in cv_history.items()}\n            history_composite = {**history_composite, **hnn}\n\n        # Perform metalearner model training\n        if isinstance(self.ml_model, Metalearner_Base):\n            self.train_metalearner(temp_dg)\n\n        # Return Composite history object\n        return history_composite\n\n    def train_metalearner(self, training_generator):\n        \"\"\" Training function for fitting the Metalearner model.\n\n        Function will be called automatically in the `train()` function if\n        the parameter `metalearner_fitting` is true.\n\n        However, this function can also be called multiple times for training\n        different Metalearner types without the need of time-extensive\n        re-training of the [NeuralNetwork][aucmedi.neural_network.model] models.\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                    to percentage split).\n        \"\"\"\n        # Skipping metalearner training if aggregate function\n        if isinstance(self.ml_model, Aggregate_Base) : return\n\n        temp_dg = training_generator    # Template DataGenerator variable for faster access\n        preds_ensemble = []\n\n        # Obtain training data\n        x = training_generator.samples\n        y = training_generator.labels\n        m = training_generator.metadata\n\n        # Apply percentage split sampling for metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                         stratified=True, iterative=True,\n                                         seed=self.sampling_seed)\n        # Pack data according to sampling\n        if len(ps_sampling[0]) == 3 : data_ensemble = ps_sampling[1]\n        else : data_ensemble = (*ps_sampling[1], None)\n\n        # Identify path to model directory\n        if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            path_model_dir = self.cache_dir.name\n        else : path_model_dir = self.cache_dir\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            # Load current model\n            path_model = os.path.join(path_model_dir,\n                                      \"cv_\" + str(i) + \".model.keras\")\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Start inference process for model i\n            process_queue = mp.Queue()\n            process_pred = mp.Process(target=__prediction_process__,\n                                      args=(process_queue,\n                                            model_paras,\n                                            path_model,\n                                            data_ensemble,\n                                            datagen_paras))\n            process_pred.start()\n            process_pred.join()\n            preds = process_queue.get()\n\n            # Append preds to ensemble\n            preds_ensemble.append(preds)\n\n        # Preprocess prediction ensemble\n        preds_ensemble = np.array(preds_ensemble)\n        preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n        s, m, c = preds_ensemble.shape\n        x_stack = np.reshape(preds_ensemble, (s, m*c))\n\n        # Start training of stacked metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            (_, y_stack, _) = data_ensemble\n            self.ml_model.train(x_stack, y_stack)\n            # Store metalearner model to disk\n            path_metalearner = os.path.join(path_model_dir,\n                                            \"metalearner.model.pickle\")\n            self.ml_model.dump(path_metalearner)\n\n    def predict(self, prediction_generator, return_ensemble=False):\n        \"\"\" Prediction function for Composite.\n\n        The fitted models and selected Metalearner/Aggregate function will predict classifications\n        for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        !!! info\n            More about Metalearners can be found here: [Metelearner][aucmedi.ensemble.metalearner]\n\n            More about Aggregate functions can be found here: [aggregate][aucmedi.ensemble.aggregate]\n\n        Args:\n            prediction_generator (DataGenerator):   A data generator which will be used for inference.\n            return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n        Returns:\n            preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n            ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                    Shape (n_models, n_samples, n_labels).\n        \"\"\"\n        # Verify if there is a linked cache dictionary\n        con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n                   os.path.exists(self.cache_dir.name))\n        con_var = (self.cache_dir is not None and \\\n                   not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n                   and os.path.exists(self.cache_dir))\n        if not con_tmp and not con_var:\n            raise FileNotFoundError(\"Composite instance does not have a valid\" \\\n                                    + \"model cache directory!\")\n\n        # Initialize some variables\n        temp_dg = prediction_generator\n        preds_ensemble = []\n        preds_final = []\n\n        # Extract data\n        data_test = (temp_dg.samples, temp_dg.labels, temp_dg.metadata)\n\n        # Identify path to model directory\n        if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            path_model_dir = self.cache_dir.name\n        else : path_model_dir = self.cache_dir\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            path_model = os.path.join(path_model_dir,\n                                      \"cv_\" + str(i) + \".model.keras\")\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Start inference process for model i\n            process_queue = mp.Queue()\n            process_pred = mp.Process(target=__prediction_process__,\n                                      args=(process_queue,\n                                            model_paras,\n                                            path_model,\n                                            data_test,\n                                            datagen_paras))\n            process_pred.start()\n            process_pred.join()\n            preds = process_queue.get()\n\n            # Append preds to ensemble\n            preds_ensemble.append(preds)\n\n        # Preprocess prediction ensemble\n        preds_ensemble = np.array(preds_ensemble)\n        preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n\n        # Apply heterogenous metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            s, m, c = preds_ensemble.shape\n            x_stack = np.reshape(preds_ensemble, (s, m*c))\n            preds_final = self.ml_model.predict(data=x_stack)\n        # Apply homogeneous aggregate function\n        elif isinstance(self.ml_model, Aggregate_Base):\n            for i in range(preds_ensemble.shape[0]):\n                pred_sample = self.ml_model.aggregate(preds_ensemble[i,:,:])\n                preds_final.append(pred_sample)\n\n        # Convert prediction list to NumPy\n        preds_final = np.asarray(preds_final)\n\n        # Return ensembled predictions\n        if return_ensemble : return preds_final, np.swapaxes(preds_ensemble,1,0)\n        else : return preds_final\n\n    # Dump model to file\n    def dump(self, directory_path):\n        \"\"\" Store temporary Composite models directory permanently to disk at desired location.\n\n        If the model directory is a provided path which is already persistent on the disk,\n        the directory is copied in order to keep original data persistent.\n\n        Args:\n            directory_path (str):       Path to store the model directory on disk.\n        \"\"\"\n        if self.cache_dir is None:\n            raise FileNotFoundError(\"Composite does not have a valid model cache directory!\")\n        elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            shutil.copytree(self.cache_dir.name, directory_path,\n                            dirs_exist_ok=True)\n            self.cache_dir.cleanup()\n            self.cache_dir = directory_path\n        else:\n            shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n            self.cache_dir = directory_path\n\n    # Load model from file\n    def load(self, directory_path):\n        \"\"\" Load a Composite model directory which can be used for Metalearner based inference.\n\n        Args:\n            directory_path (str):       Input path, from which the Composite models will be loaded.\n        \"\"\"\n        # Check directory existence\n        if not os.path.exists(directory_path):\n            raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                    directory_path)\n        # Check model existence\n        for i in range(len(self.model_list)):\n            path_model = os.path.join(directory_path,\n                                      \"cv_\" + str(i) + \".model.keras\")\n            if not os.path.exists(path_model):\n                raise FileNotFoundError(\"Composite model \" + str(i) + \\\n                                        \" does not exist!\", path_model)\n        # If heterogenous metalearner -&gt; load metalearner model file\n        if isinstance(self.ml_model, Metalearner_Base):\n            path_model = os.path.join(directory_path,\n                                      \"metalearner.model.pickle\")\n            if not os.path.exists(path_model):\n                raise FileNotFoundError(\"Metalearner model does not exist!\",\n                                        path_model)\n            self.ml_model.load(path_model)\n\n        # Update model directory\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.__init__","title":"<code>__init__(model_list, metalearner='logistic_regression', k_fold=3, sampling=[0.85, 0.15], fixed_datagenerator=False)</code>","text":"<p>Initialization function for creating a Composite object.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list of NeuralNetwork</code> <p>List of instances of AUCMEDI neural network class.                                         The number of models (<code>len(model_list)</code>) have to be equal to <code>k_fold</code>.</p> required <code>metalearner</code> <code>str, Metalearner or Aggregate</code> <p>Metalearner class instance / a string for an AUCMEDI Metalearner,                                         or Aggregate function / a string for an AUCMEDI Aggregate function.</p> <code>'logistic_regression'</code> <code>k_fold</code> <code>int</code> <p>Number of folds (k) for the Cross-Validation. Must be at least 2.</p> <code>3</code> <code>sampling</code> <code>list of float</code> <p>List of percentage values with split sizes. Should be 2x percentage values                                         for heterogenous metalearner (must sum up to 1.0).</p> <code>[0.85, 0.15]</code> <code>fixed_datagenerator</code> <code>bool</code> <p>Boolean, whether using fixed parameters of passed DataGenerator or                                         using default architecture paramters for Resizing and Standardize.</p> <code>False</code> Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def __init__(self, model_list, metalearner=\"logistic_regression\",\n             k_fold=3, sampling=[0.85, 0.15], fixed_datagenerator=False):\n    \"\"\" Initialization function for creating a Composite object.\n\n    Args:\n        model_list (list of NeuralNetwork):         List of instances of AUCMEDI neural network class.\n                                                    The number of models (`len(model_list)`) have to be equal to `k_fold`.\n        metalearner (str, Metalearner or Aggregate):Metalearner class instance / a string for an AUCMEDI Metalearner,\n                                                    or Aggregate function / a string for an AUCMEDI Aggregate function.\n        k_fold (int):                               Number of folds (k) for the Cross-Validation. Must be at least 2.\n        sampling (list of float):                   List of percentage values with split sizes. Should be 2x percentage values\n                                                    for heterogenous metalearner (must sum up to 1.0).\n        fixed_datagenerator (bool):                 Boolean, whether using fixed parameters of passed DataGenerator or\n                                                    using default architecture paramters for Resizing and Standardize.\n    \"\"\"\n    # Cache class variables\n    self.model_list = model_list\n    self.metalearner = metalearner\n    self.sampling = sampling\n    self.k_fold = k_fold\n    self.fixed_datagenerator = fixed_datagenerator\n    self.sampling_seed = 0\n    self.cache_dir = None\n\n    # Initialize Metalearner\n    if isinstance(metalearner, str) and metalearner in metalearner_dict:\n        self.ml_model = metalearner_dict[metalearner]()\n    elif isinstance(metalearner, str) and metalearner in aggregate_dict:\n        self.ml_model = aggregate_dict[metalearner]()\n    elif isinstance(metalearner, Metalearner_Base) or \\\n         isinstance(metalearner, Aggregate_Base):\n        self.ml_model = metalearner\n    else : raise TypeError(\"Unknown type of Metalearner (neither known \" + \\\n                           \"ensembler nor Aggregate or Metalearner class)!\")\n\n    # Verify model list length\n    if k_fold != len(model_list):\n        raise ValueError(\"Length of model_list and k_fold has to be equal!\")\n\n    # Set multiprocessing method to spawn\n    mp.set_start_method(\"spawn\", force=True)\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.dump","title":"<code>dump(directory_path)</code>","text":"<p>Store temporary Composite models directory permanently to disk at desired location.</p> <p>If the model directory is a provided path which is already persistent on the disk, the directory is copied in order to keep original data persistent.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Path to store the model directory on disk.</p> required Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def dump(self, directory_path):\n    \"\"\" Store temporary Composite models directory permanently to disk at desired location.\n\n    If the model directory is a provided path which is already persistent on the disk,\n    the directory is copied in order to keep original data persistent.\n\n    Args:\n        directory_path (str):       Path to store the model directory on disk.\n    \"\"\"\n    if self.cache_dir is None:\n        raise FileNotFoundError(\"Composite does not have a valid model cache directory!\")\n    elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        shutil.copytree(self.cache_dir.name, directory_path,\n                        dirs_exist_ok=True)\n        self.cache_dir.cleanup()\n        self.cache_dir = directory_path\n    else:\n        shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.load","title":"<code>load(directory_path)</code>","text":"<p>Load a Composite model directory which can be used for Metalearner based inference.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Input path, from which the Composite models will be loaded.</p> required Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def load(self, directory_path):\n    \"\"\" Load a Composite model directory which can be used for Metalearner based inference.\n\n    Args:\n        directory_path (str):       Input path, from which the Composite models will be loaded.\n    \"\"\"\n    # Check directory existence\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                directory_path)\n    # Check model existence\n    for i in range(len(self.model_list)):\n        path_model = os.path.join(directory_path,\n                                  \"cv_\" + str(i) + \".model.keras\")\n        if not os.path.exists(path_model):\n            raise FileNotFoundError(\"Composite model \" + str(i) + \\\n                                    \" does not exist!\", path_model)\n    # If heterogenous metalearner -&gt; load metalearner model file\n    if isinstance(self.ml_model, Metalearner_Base):\n        path_model = os.path.join(directory_path,\n                                  \"metalearner.model.pickle\")\n        if not os.path.exists(path_model):\n            raise FileNotFoundError(\"Metalearner model does not exist!\",\n                                    path_model)\n        self.ml_model.load(path_model)\n\n    # Update model directory\n    self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.predict","title":"<code>predict(prediction_generator, return_ensemble=False)</code>","text":"<p>Prediction function for Composite.</p> <p>The fitted models and selected Metalearner/Aggregate function will predict classifications for the provided DataGenerator.</p> <p>Info</p> <p>More about Metalearners can be found here: Metelearner</p> <p>More about Aggregate functions can be found here: aggregate</p> <p>Parameters:</p> Name Type Description Default <code>prediction_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for inference.</p> required <code>return_ensemble</code> <code>bool</code> <p>Option, whether gathered ensemble of predictions should be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels).</p> <code>ensemble</code> <code>numpy.ndarray</code> <p>Optional ensemble of predictions: Will be only passed if <code>return_ensemble=True</code>.                                     Shape (n_models, n_samples, n_labels).</p> Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def predict(self, prediction_generator, return_ensemble=False):\n    \"\"\" Prediction function for Composite.\n\n    The fitted models and selected Metalearner/Aggregate function will predict classifications\n    for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    !!! info\n        More about Metalearners can be found here: [Metelearner][aucmedi.ensemble.metalearner]\n\n        More about Aggregate functions can be found here: [aggregate][aucmedi.ensemble.aggregate]\n\n    Args:\n        prediction_generator (DataGenerator):   A data generator which will be used for inference.\n        return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n    Returns:\n        preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n        ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                Shape (n_models, n_samples, n_labels).\n    \"\"\"\n    # Verify if there is a linked cache dictionary\n    con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n               os.path.exists(self.cache_dir.name))\n    con_var = (self.cache_dir is not None and \\\n               not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n               and os.path.exists(self.cache_dir))\n    if not con_tmp and not con_var:\n        raise FileNotFoundError(\"Composite instance does not have a valid\" \\\n                                + \"model cache directory!\")\n\n    # Initialize some variables\n    temp_dg = prediction_generator\n    preds_ensemble = []\n    preds_final = []\n\n    # Extract data\n    data_test = (temp_dg.samples, temp_dg.labels, temp_dg.metadata)\n\n    # Identify path to model directory\n    if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        path_model_dir = self.cache_dir.name\n    else : path_model_dir = self.cache_dir\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        path_model = os.path.join(path_model_dir,\n                                  \"cv_\" + str(i) + \".model.keras\")\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Start inference process for model i\n        process_queue = mp.Queue()\n        process_pred = mp.Process(target=__prediction_process__,\n                                  args=(process_queue,\n                                        model_paras,\n                                        path_model,\n                                        data_test,\n                                        datagen_paras))\n        process_pred.start()\n        process_pred.join()\n        preds = process_queue.get()\n\n        # Append preds to ensemble\n        preds_ensemble.append(preds)\n\n    # Preprocess prediction ensemble\n    preds_ensemble = np.array(preds_ensemble)\n    preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n\n    # Apply heterogenous metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        s, m, c = preds_ensemble.shape\n        x_stack = np.reshape(preds_ensemble, (s, m*c))\n        preds_final = self.ml_model.predict(data=x_stack)\n    # Apply homogeneous aggregate function\n    elif isinstance(self.ml_model, Aggregate_Base):\n        for i in range(preds_ensemble.shape[0]):\n            pred_sample = self.ml_model.aggregate(preds_ensemble[i,:,:])\n            preds_final.append(pred_sample)\n\n    # Convert prediction list to NumPy\n    preds_final = np.asarray(preds_final)\n\n    # Return ensembled predictions\n    if return_ensemble : return preds_final, np.swapaxes(preds_ensemble,1,0)\n    else : return preds_final\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.train","title":"<code>train(training_generator, epochs=20, iterations=None, callbacks=[], class_weights=None, transfer_learning=False, metalearner_fitting=True)</code>","text":"<p>Training function for fitting the provided NeuralNetwork models.</p> <p>The training data will be sampled according to a percentage split in which DataGenerators for model training and metalearner training if a metalearner is provided. Else all data is used as model training subset. The model training subset is furthermore sampled via cross-validation.</p> <p>It is also possible to pass custom Callback classes in order to obtain more information.</p> <p>For more information on the fitting process, check out NeuralNetwork.train().</p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training (will be split according                                     to percentage split and k-fold cross-validation sampling).</p> required <code>epochs</code> <code>int</code> <p>Number of epochs. A single epoch is defined as one iteration through                                     the complete data set.</p> <code>20</code> <code>iterations</code> <code>int</code> <p>Number of iterations (batches) in a single epoch.</p> <code>None</code> <code>callbacks</code> <code>list of Callback classes</code> <p>A list of Callback classes for custom evaluation.</p> <code>[]</code> <code>class_weights</code> <code>dictionary or list</code> <p>A list or dictionary of float values to handle class unbalance.</p> <code>None</code> <code>transfer_learning</code> <code>bool</code> <p>Option whether a transfer learning training should be performed.</p> <code>False</code> <code>metalearner_fitting</code> <code>bool</code> <p>Option whether the Metalearner fitting process should be included in the                                     Composite training process. The <code>train_metalearner()</code> function can also be                                     run manually (or repeatedly).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def train(self, training_generator, epochs=20, iterations=None,\n          callbacks=[], class_weights=None, transfer_learning=False,\n          metalearner_fitting=True):\n    \"\"\" Training function for fitting the provided NeuralNetwork models.\n\n    The training data will be sampled according to a percentage split in which\n    [DataGenerators][aucmedi.data_processing.data_generator.DataGenerator] for model training\n    and metalearner training if a metalearner is provided. Else all data is used as model\n    training subset. The model training subset is furthermore sampled via cross-validation.\n\n    It is also possible to pass custom Callback classes in order to obtain more information.\n\n    For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                to percentage split and k-fold cross-validation sampling).\n        epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                the complete data set.\n        iterations (int):                       Number of iterations (batches) in a single epoch.\n        callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n        class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n        transfer_learning (bool):               Option whether a transfer learning training should be performed.\n        metalearner_fitting (bool):             Option whether the Metalearner fitting process should be included in the\n                                                Composite training process. The `train_metalearner()` function can also be\n                                                run manually (or repeatedly).\n    Returns:\n        history (dict):                         A history dictionary from a Keras history object which contains several logs.\n    \"\"\"\n    temp_dg = training_generator    # Template DataGenerator variable for faster access\n    history_composite = {}           # Final history dictionary\n\n    # Create temporary model directory\n    self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                 suffix=\".composite\")\n\n    # Obtain training data\n    x = training_generator.samples\n    y = training_generator.labels\n    m = training_generator.metadata\n\n    # Apply percentage split sampling for metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                     stratified=True, iterative=True,\n                                     seed=self.sampling_seed)\n        # Pack data according to sampling\n        if len(ps_sampling[0]) == 3 : x, y, m = ps_sampling[0]\n        else : x, y = ps_sampling[0]\n\n    # Apply cross-validaton sampling\n    cv_sampling = sampling_kfold(x, y, m, n_splits=self.k_fold,\n                                 stratified=True, iterative=True)\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        # Pack data into a tuple\n        fold = cv_sampling[i]\n        if len(fold) == 4:\n            (train_x, train_y, test_x, test_y) = fold\n            data = (train_x, train_y, None, test_x, test_y, None)\n        else : data = fold\n\n        # Create model specific callback list\n        callbacks_model = callbacks.copy()\n        # Extend Callback list\n        path_model = os.path.join(self.cache_dir.name,\n                                  \"cv_\" + str(i) + \".model.keras\")\n        cb_mc = ModelCheckpoint(path_model,\n                                monitor=\"val_loss\", verbose=1,\n                                save_best_only=True, mode=\"min\")\n        cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                             \"cv_\" + str(i) + \\\n                                             \".logs.csv\"),\n                          separator=',', append=True)\n        callbacks_model.extend([cb_mc, cb_cl])\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Gather training parameters\n        parameters_training = {\"epochs\": epochs,\n                            \"iterations\": iterations,\n                            \"callbacks\": callbacks_model,\n                            \"class_weights\": class_weights,\n                            \"transfer_learning\": transfer_learning\n        }\n\n        # Start training process\n        process_queue = mp.Queue()\n        process_train = mp.Process(target=__training_process__,\n                                   args=(process_queue,\n                                         data,\n                                         model_paras,\n                                         datagen_paras,\n                                         parameters_training))\n        process_train.start()\n        process_train.join()\n        cv_history = process_queue.get()\n        # Combine logged history objects\n        hnn = {\"cv_\" + str(i) + \".\" + k: v for k, v in cv_history.items()}\n        history_composite = {**history_composite, **hnn}\n\n    # Perform metalearner model training\n    if isinstance(self.ml_model, Metalearner_Base):\n        self.train_metalearner(temp_dg)\n\n    # Return Composite history object\n    return history_composite\n</code></pre>"},{"location":"reference/ensemble/composite/#aucmedi.ensemble.composite.Composite.train_metalearner","title":"<code>train_metalearner(training_generator)</code>","text":"<p>Training function for fitting the Metalearner model.</p> <p>Function will be called automatically in the <code>train()</code> function if the parameter <code>metalearner_fitting</code> is true.</p> <p>However, this function can also be called multiple times for training different Metalearner types without the need of time-extensive re-training of the NeuralNetwork models.</p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training (will be split according                                     to percentage split).</p> required Source code in <code>aucmedi/ensemble/composite.py</code> <pre><code>def train_metalearner(self, training_generator):\n    \"\"\" Training function for fitting the Metalearner model.\n\n    Function will be called automatically in the `train()` function if\n    the parameter `metalearner_fitting` is true.\n\n    However, this function can also be called multiple times for training\n    different Metalearner types without the need of time-extensive\n    re-training of the [NeuralNetwork][aucmedi.neural_network.model] models.\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                to percentage split).\n    \"\"\"\n    # Skipping metalearner training if aggregate function\n    if isinstance(self.ml_model, Aggregate_Base) : return\n\n    temp_dg = training_generator    # Template DataGenerator variable for faster access\n    preds_ensemble = []\n\n    # Obtain training data\n    x = training_generator.samples\n    y = training_generator.labels\n    m = training_generator.metadata\n\n    # Apply percentage split sampling for metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                     stratified=True, iterative=True,\n                                     seed=self.sampling_seed)\n    # Pack data according to sampling\n    if len(ps_sampling[0]) == 3 : data_ensemble = ps_sampling[1]\n    else : data_ensemble = (*ps_sampling[1], None)\n\n    # Identify path to model directory\n    if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        path_model_dir = self.cache_dir.name\n    else : path_model_dir = self.cache_dir\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        # Load current model\n        path_model = os.path.join(path_model_dir,\n                                  \"cv_\" + str(i) + \".model.keras\")\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Start inference process for model i\n        process_queue = mp.Queue()\n        process_pred = mp.Process(target=__prediction_process__,\n                                  args=(process_queue,\n                                        model_paras,\n                                        path_model,\n                                        data_ensemble,\n                                        datagen_paras))\n        process_pred.start()\n        process_pred.join()\n        preds = process_queue.get()\n\n        # Append preds to ensemble\n        preds_ensemble.append(preds)\n\n    # Preprocess prediction ensemble\n    preds_ensemble = np.array(preds_ensemble)\n    preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n    s, m, c = preds_ensemble.shape\n    x_stack = np.reshape(preds_ensemble, (s, m*c))\n\n    # Start training of stacked metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        (_, y_stack, _) = data_ensemble\n        self.ml_model.train(x_stack, y_stack)\n        # Store metalearner model to disk\n        path_metalearner = os.path.join(path_model_dir,\n                                        \"metalearner.model.pickle\")\n        self.ml_model.dump(path_metalearner)\n</code></pre>"},{"location":"reference/ensemble/stacking/","title":"Stacking","text":""},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking","title":"<code>Stacking</code>","text":"<p>A Stacking class providing functionality for metalearner based ensemble learning.</p> <p>In contrast to single algorithm approaches, the ensemble of different deep convolutional neural network architectures (also called heterogeneous ensemble learning) showed strong benefits for overall performance in several studies. The idea of the Stacking technique is to utilize diverse and independent models by stacking another machine learning algorithm on top of these predictions.</p> <p>In AUCMEDI, a percentage split is applied on the dataset into the subsets: train, validation and ensemble.</p> <p>On the train and validation subsets, one or multiple predefined NeuralNetwork models are trained, whereas on the ensemble subset a metalearner is fitted to merge NeuralNetwork model predictions into a single one.</p> Example <pre><code># Initialize some NeuralNetwork models\nmodel_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\nmodel_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\nmodel_c = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.EfficientNetB1\")\n\n# Initialize Stacking object\nel = Stacking(model_list=[model_a, model_b, model_c],\n              metalearner=\"logistic_regression\")\n\n# Initialize training DataGenerator for complete training data\ndatagen = DataGenerator(samples_train, \"images_dir/\",\n                        labels=train_labels_ohe, batch_size=3,\n                        resize=None, standardize_mode=None)\n# Train neural network and metalearner models\nel.train(datagen, epochs=100)\n\n# Initialize testing DataGenerator for testing data\ntest_gen = DataGenerator(samples_test, \"images_dir/\",\n                         resize=None, standardize_mode=None)\n# Run Inference\npreds = el.predict(test_gen)\n</code></pre> <p>Training Time Increase</p> <p>Stacking sequentially performs fitting processes for multiple models, which will drastically increase training time.</p> DataGenerator re-initialization <p>The passed DataGenerator for the train() and predict() function of the Stacking class will be re-initialized!</p> <p>This can result in redundant image preparation if <code>prepare_images=True</code>.</p> <p>Furthermore, the parameters <code>resize</code> and <code>standardize_mode</code> are automatically re-initialized with NeuralNetwork model specific values (<code>model.meta_standardize</code> for <code>standardize_mode</code> and <code>model.meta_input</code> for <code>input_shape</code>).</p> <p>If desired (but not recommended!), it is possible to modify the meta variables of the NeuralNetwork model as follows: <pre><code># For input_shape\nmodel_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\",\n                        input_shape=(64,64))\n# For standardize_mode\nmodel_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\nmodel_b.meta_standardize = \"torch\"\n</code></pre></p> NeuralNetwork re-initialization <p>The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!</p> <p>Attention: Metrics are not passed to the processes due to pickling issues.</p> Technical Details <p>For the training and inference process, each model will create an individual process via the Python multiprocessing package.</p> <p>This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs, which is why more and more redundant data pile up with an increasing number of models.</p> <p>Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next model.</p> Reference for Ensemble Learning Techniques <p>Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022). An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. arXiv e-print: https://arxiv.org/abs/2201.11440</p> Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>class Stacking:\n    \"\"\" A Stacking class providing functionality for metalearner based ensemble learning.\n\n    In contrast to single algorithm approaches, the ensemble of different deep convolutional neural network architectures\n    (also called heterogeneous ensemble learning) showed strong benefits for overall performance in several studies.\n    The idea of the Stacking technique is to utilize diverse and independent models by stacking another machine learning\n    algorithm on top of these predictions.\n\n    In AUCMEDI, a percentage split is applied on the dataset into the subsets: train, validation and ensemble.\n\n    On the train and validation subsets, one or multiple predefined [NeuralNetwork][aucmedi.neural_network.model]\n    models are trained, whereas on the ensemble subset a metalearner is fitted to merge\n    [NeuralNetwork][aucmedi.neural_network.model] model predictions into a single one.\n\n    ???+ example\n        ```python\n        # Initialize some NeuralNetwork models\n        model_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\")\n        model_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\n        model_c = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.EfficientNetB1\")\n\n        # Initialize Stacking object\n        el = Stacking(model_list=[model_a, model_b, model_c],\n                      metalearner=\"logistic_regression\")\n\n        # Initialize training DataGenerator for complete training data\n        datagen = DataGenerator(samples_train, \"images_dir/\",\n                                labels=train_labels_ohe, batch_size=3,\n                                resize=None, standardize_mode=None)\n        # Train neural network and metalearner models\n        el.train(datagen, epochs=100)\n\n        # Initialize testing DataGenerator for testing data\n        test_gen = DataGenerator(samples_test, \"images_dir/\",\n                                 resize=None, standardize_mode=None)\n        # Run Inference\n        preds = el.predict(test_gen)\n        ```\n\n    !!! warning \"Training Time Increase\"\n        Stacking sequentially performs fitting processes for multiple models, which will drastically increase training time.\n\n    ??? warning \"DataGenerator re-initialization\"\n        The passed DataGenerator for the train() and predict() function of the Stacking class will be re-initialized!\n\n        This can result in redundant image preparation if `prepare_images=True`.\n\n        Furthermore, the parameters `resize` and `standardize_mode` are automatically re-initialized with\n        NeuralNetwork model specific values (`model.meta_standardize` for `standardize_mode` and\n        `model.meta_input` for `input_shape`).\n\n        If desired (but not recommended!), it is possible to modify the meta variables of the NeuralNetwork model as follows:\n        ```python\n        # For input_shape\n        model_a = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.ResNet50\",\n                                input_shape=(64,64))\n        # For standardize_mode\n        model_b = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.MobileNetV2\")\n        model_b.meta_standardize = \"torch\"\n        ```\n\n    ??? warning \"NeuralNetwork re-initialization\"\n        The passed NeuralNetwork for the train() and predict() function of the Composite class will be re-initialized!\n\n        Attention: Metrics are not passed to the processes due to pickling issues.\n\n    ??? info \"Technical Details\"\n        For the training and inference process, each model will create an individual process via the Python multiprocessing package.\n\n        This is crucial as TensorFlow does not fully support the VRAM memory garbage collection in GPUs,\n        which is why more and more redundant data pile up with an increasing number of models.\n\n        Via separate processes, it is possible to clean up the TensorFlow environment and rebuild it again for the next model.\n\n    ??? reference \"Reference for Ensemble Learning Techniques\"\n        Dominik M\u00fcller, I\u00f1aki Soto-Rey and Frank Kramer. (2022).\n        An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks.\n        arXiv e-print: [https://arxiv.org/abs/2201.11440](https://arxiv.org/abs/2201.11440)\n    \"\"\"\n    def __init__(self, model_list, metalearner=\"logistic_regression\",\n                 sampling=[0.7, 0.1, 0.2]):\n        \"\"\" Initialization function for creating a Stacking object.\n\n        Args:\n            model_list (list of NeuralNetwork):        List of instances of AUCMEDI neural network class.\n            metalearner (str, Metalearner or Aggregate):Metalearner class instance / a string for an AUCMEDI Metalearner,\n                                                        or Aggregate function / a string for an AUCMEDI Aggregate function.\n            sampling (list of float):                   List of percentage values with split sizes. Should be 3x percentage values\n                                                        for heterogenous metalearner and 2x percentage values for homogeneous\n                                                        Aggregate functions (must sum up to 1.0).\n        \"\"\"\n        # Cache class variables\n        self.model_list = model_list\n        self.metalearner = metalearner\n        self.sampling = sampling\n        self.sampling_seed = 0\n        self.cache_dir = None\n\n        # Initialize Metalearner\n        if isinstance(metalearner, str) and metalearner in metalearner_dict:\n            self.ml_model = metalearner_dict[metalearner]()\n        elif isinstance(metalearner, str) and metalearner in aggregate_dict:\n            self.ml_model = aggregate_dict[metalearner]()\n        elif isinstance(metalearner, Metalearner_Base) or \\\n             isinstance(metalearner, Aggregate_Base):\n            self.ml_model = metalearner\n        else : raise TypeError(\"Unknown type of Metalearner (neither known \" + \\\n                               \"ensembler nor Aggregate or Metalearner class)!\")\n\n        # Set multiprocessing method to spawn\n        mp.set_start_method(\"spawn\", force=True)\n\n    def train(self, training_generator, epochs=20, iterations=None,\n              callbacks=[], class_weights=None, transfer_learning=False,\n              metalearner_fitting=True):\n        \"\"\" Training function for fitting the provided Stacking models.\n\n        The training data will be sampled according to a percentage split in which\n        [DataGenerators][aucmedi.data_processing.data_generator.DataGenerator] for model training\n        and validation as well as one for the metalearner training will be automatically created.\n\n        It is also possible to pass custom Callback classes in order to obtain more information.\n\n        For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                    to percentage split sampling).\n            epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                    the complete data set.\n            iterations (int):                       Number of iterations (batches) in a single epoch.\n            callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n            class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n            transfer_learning (bool):               Option whether a transfer learning training should be performed.\n            metalearner_fitting (bool):             Option whether the Metalearner fitting process should be included in the\n                                                    Stacking training process. The `train_metalearner()` function can also be\n                                                    run manually (or repeatedly).\n        Returns:\n            history (dict):                   A history dictionary from a Keras history object which contains several logs.\n        \"\"\"\n        temp_dg = training_generator    # Template DataGenerator variable for faster access\n        history_stacking = {}           # Final history dictionary\n\n        # Create temporary model directory\n        self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                     suffix=\".stacking\")\n\n        # Obtain training data\n        x = training_generator.samples\n        y = training_generator.labels\n        m = training_generator.metadata\n\n        # Apply percentage split sampling\n        ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                     stratified=True, iterative=True,\n                                     seed=self.sampling_seed)\n\n        # Pack data according to sampling\n        if len(ps_sampling[0]) == 3:\n            data_train = ps_sampling[0]\n            data_val = ps_sampling[1]\n        else:\n            data_train = (*ps_sampling[0], None)\n            data_val = (*ps_sampling[1], None)\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            # Create model specific callback list\n            callbacks_model = callbacks.copy()\n            # Extend Callback list\n            path_model = os.path.join(self.cache_dir.name,\n                                      \"nn_\" + str(i) + \".model.keras\")\n            cb_mc = ModelCheckpoint(path_model,\n                                    monitor=\"val_loss\", verbose=1,\n                                    save_best_only=True, mode=\"min\")\n            cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                                 \"nn_\" + str(i) + \\\n                                                 \".logs.csv\"),\n                              separator=',', append=True)\n            callbacks_model.extend([cb_mc, cb_cl])\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Gather training parameters\n            parameters_training = {\"epochs\": epochs,\n                                \"iterations\": iterations,\n                                \"callbacks\": callbacks_model,\n                                \"class_weights\": class_weights,\n                                \"transfer_learning\": transfer_learning\n            }\n\n            # Start training process\n            process_queue = mp.Queue()\n            process_train = mp.Process(target=__training_process__,\n                                       args=(process_queue,\n                                             model_paras,\n                                             data_train,\n                                             data_val,\n                                             datagen_paras,\n                                             parameters_training))\n            process_train.start()\n            process_train.join()\n            nn_history = process_queue.get()\n            # Combine logged history objects\n            hnn = {\"nn_\" + str(i) + \".\" + k: v for k, v in nn_history.items()}\n            history_stacking = {**history_stacking, **hnn}\n\n        # Perform metalearner model training\n        self.train_metalearner(temp_dg)\n\n        # Return Stacking history object\n        return history_stacking\n\n    def train_metalearner(self, training_generator):\n        \"\"\" Training function for fitting the Metalearner model.\n\n        Function will be called automatically in the `train()` function if\n        the parameter `metalearner_fitting` is true.\n\n        However, this function can also be called multiple times for training\n        different Metalearner types without the need of time-extensive\n        re-training of the [NeuralNetwork][aucmedi.neural_network.model] models.\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                    to percentage split sampling).\n        \"\"\"\n        # Skipping metalearner training if aggregate function\n        if isinstance(self.ml_model, Aggregate_Base) : return\n\n        temp_dg = training_generator    # Template DataGenerator variable for faster access\n        preds_ensemble = []\n\n        # Obtain training data\n        x = training_generator.samples\n        y = training_generator.labels\n        m = training_generator.metadata\n\n        # Apply percentage split sampling\n        ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                     stratified=True, iterative=True,\n                                     seed=self.sampling_seed)\n\n        # Pack data according to sampling\n        if len(ps_sampling[0]) == 3 : data_ensemble = ps_sampling[2]\n        else : data_ensemble = (*ps_sampling[2], None)\n\n        # Identify path to model directory\n        if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            path_model_dir = self.cache_dir.name\n        else : path_model_dir = self.cache_dir\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            #  Load current model\n            path_model = os.path.join(path_model_dir,\n                                      \"nn_\" + str(i) + \".model.keras\")\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Start inference process for model i\n            process_queue = mp.Queue()\n            process_pred = mp.Process(target=__prediction_process__,\n                                      args=(process_queue,\n                                            model_paras,\n                                            path_model,\n                                            data_ensemble,\n                                            datagen_paras))\n            process_pred.start()\n            process_pred.join()\n            preds = process_queue.get()\n\n            # Append preds to ensemble\n            preds_ensemble.append(preds)\n\n        # Preprocess prediction ensemble\n        preds_ensemble = np.array(preds_ensemble)\n        preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n        s, m, c = preds_ensemble.shape\n        x_stack = np.reshape(preds_ensemble, (s, m*c))\n\n        # Start training of stacked metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            (_, y_stack, _) = data_ensemble\n            self.ml_model.train(x_stack, y_stack)\n            # Store metalearner model to disk\n            path_metalearner = os.path.join(path_model_dir,\n                                            \"metalearner.model.pickle\")\n            self.ml_model.dump(path_metalearner)\n\n    def predict(self, prediction_generator, return_ensemble=False):\n        \"\"\" Prediction function for Stacking.\n\n        The fitted models and selected Metalearner will predict classifications for the provided\n        [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        !!! info\n            More about Metalearners can be found here: [Metelearner][aucmedi.ensemble.metalearner]\n\n            More about Aggregate functions can be found here: [aggregate][aucmedi.ensemble.aggregate]\n\n        Args:\n            prediction_generator (DataGenerator):   A data generator which will be used for inference.\n            return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n        Returns:\n            preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n            ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                    Shape (n_models, n_samples, n_labels).\n        \"\"\"\n        # Verify if there is a linked cache dictionary\n        con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n                   os.path.exists(self.cache_dir.name))\n        con_var = (self.cache_dir is not None and \\\n                   not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n                   and os.path.exists(self.cache_dir))\n        if not con_tmp and not con_var:\n            raise FileNotFoundError(\"Stacking does not have a valid model cache directory!\")\n\n        # Initialize some variables\n        temp_dg = prediction_generator\n        preds_ensemble = []\n        preds_final = []\n\n        # Extract data\n        data_test = (temp_dg.samples, temp_dg.labels, temp_dg.metadata)\n\n        # Identify path to model directory\n        if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            path_model_dir = self.cache_dir.name\n        else : path_model_dir = self.cache_dir\n\n        # Sequentially iterate over model list\n        for i in range(len(self.model_list)):\n            path_model = os.path.join(path_model_dir,\n                                      \"nn_\" + str(i) + \".model.keras\")\n\n            # Gather NeuralNetwork parameters\n            model_paras = {\n                \"n_labels\": self.model_list[i].n_labels,\n                \"channels\": self.model_list[i].channels,\n                \"input_shape\": self.model_list[i].input_shape,\n                \"architecture\": self.model_list[i].architecture,\n                \"pretrained_weights\": self.model_list[i].pretrained_weights,\n                \"loss\": self.model_list[i].loss,\n                \"metrics\": None,\n                \"activation_output\": self.model_list[i].activation_output,\n                \"fcl_dropout\": self.model_list[i].fcl_dropout,\n                \"meta_variables\": self.model_list[i].meta_variables,\n                \"learning_rate\": self.model_list[i].learning_rate,\n            }\n\n            # Gather DataGenerator parameters\n            datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                             \"batch_size\": temp_dg.batch_size,\n                             \"data_aug\": temp_dg.data_aug,\n                             \"seed\": temp_dg.seed,\n                             \"subfunctions\": temp_dg.subfunctions,\n                             \"shuffle\": temp_dg.shuffle,\n                             \"standardize_mode\": self.model_list[i].meta_standardize,\n                             \"resize\": self.model_list[i].meta_input,\n                             \"grayscale\": temp_dg.grayscale,\n                             \"prepare_images\": temp_dg.prepare_images,\n                             \"sample_weights\": temp_dg.sample_weights,\n                             \"image_format\": temp_dg.image_format,\n                             \"loader\": temp_dg.sample_loader,\n                             \"workers\": temp_dg.workers,\n                             \"kwargs\": temp_dg.kwargs\n            }\n\n            # Start inference process for model i\n            process_queue = mp.Queue()\n            process_pred = mp.Process(target=__prediction_process__,\n                                      args=(process_queue,\n                                            model_paras,\n                                            path_model,\n                                            data_test,\n                                            datagen_paras))\n            process_pred.start()\n            process_pred.join()\n            preds = process_queue.get()\n\n            # Append preds to ensemble\n            preds_ensemble.append(preds)\n\n        # Preprocess prediction ensemble\n        preds_ensemble = np.array(preds_ensemble)\n        preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n\n        # Apply heterogenous metalearner\n        if isinstance(self.ml_model, Metalearner_Base):\n            s, m, c = preds_ensemble.shape\n            x_stack = np.reshape(preds_ensemble, (s, m*c))\n            preds_final = self.ml_model.predict(data=x_stack)\n        # Apply homogeneous aggregate function\n        elif isinstance(self.ml_model, Aggregate_Base):\n            for i in range(preds_ensemble.shape[0]):\n                pred_sample = self.ml_model.aggregate(preds_ensemble[i,:,:])\n                preds_final.append(pred_sample)\n\n        # Convert prediction list to NumPy\n        preds_final = np.asarray(preds_final)\n\n        # Return ensembled predictions\n        if return_ensemble : return preds_final, np.swapaxes(preds_ensemble,1,0)\n        else : return preds_final\n\n    # Dump model to file\n    def dump(self, directory_path):\n        \"\"\" Store temporary Stacking models directory permanently to disk at desired location.\n\n        If the model directory is a provided path which is already persistent on the disk,\n        the directory is copied in order to keep original data persistent.\n\n        Args:\n            directory_path (str):       Path to store the model directory on disk.\n        \"\"\"\n        if self.cache_dir is None:\n            raise FileNotFoundError(\"Stacking does not have a valid model cache directory!\")\n        elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n            shutil.copytree(self.cache_dir.name, directory_path,\n                            dirs_exist_ok=True)\n            self.cache_dir.cleanup()\n            self.cache_dir = directory_path\n        else:\n            shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n            self.cache_dir = directory_path\n\n    # Load model from file\n    def load(self, directory_path):\n        \"\"\" Load a Stacking model directory which can be used for Metalearner based inference.\n\n        Args:\n            directory_path (str):       Input path, from which the Stacking models will be loaded.\n        \"\"\"\n        # Check directory existence\n        if not os.path.exists(directory_path):\n            raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                    directory_path)\n        # Check model existence\n        for i in range(len(self.model_list)):\n            path_model = os.path.join(directory_path,\n                                      \"nn_\" + str(i) + \".model.keras\")\n            if not os.path.exists(path_model):\n                raise FileNotFoundError(\"Stacking model \" + str(i) + \\\n                                        \" does not exist!\", path_model)\n        # If heterogenous metalearner -&gt; load metalearner model file\n        if isinstance(self.ml_model, Metalearner_Base):\n            path_model = os.path.join(directory_path,\n                                      \"metalearner.model.pickle\")\n            if not os.path.exists(path_model):\n                raise FileNotFoundError(\"Metalearner model does not exist!\",\n                                        path_model)\n            self.ml_model.load(path_model)\n\n        # Update model directory\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.__init__","title":"<code>__init__(model_list, metalearner='logistic_regression', sampling=[0.7, 0.1, 0.2])</code>","text":"<p>Initialization function for creating a Stacking object.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list of NeuralNetwork</code> <p>List of instances of AUCMEDI neural network class.</p> required <code>metalearner</code> <code>str, Metalearner or Aggregate</code> <p>Metalearner class instance / a string for an AUCMEDI Metalearner,                                         or Aggregate function / a string for an AUCMEDI Aggregate function.</p> <code>'logistic_regression'</code> <code>sampling</code> <code>list of float</code> <p>List of percentage values with split sizes. Should be 3x percentage values                                         for heterogenous metalearner and 2x percentage values for homogeneous                                         Aggregate functions (must sum up to 1.0).</p> <code>[0.7, 0.1, 0.2]</code> Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def __init__(self, model_list, metalearner=\"logistic_regression\",\n             sampling=[0.7, 0.1, 0.2]):\n    \"\"\" Initialization function for creating a Stacking object.\n\n    Args:\n        model_list (list of NeuralNetwork):        List of instances of AUCMEDI neural network class.\n        metalearner (str, Metalearner or Aggregate):Metalearner class instance / a string for an AUCMEDI Metalearner,\n                                                    or Aggregate function / a string for an AUCMEDI Aggregate function.\n        sampling (list of float):                   List of percentage values with split sizes. Should be 3x percentage values\n                                                    for heterogenous metalearner and 2x percentage values for homogeneous\n                                                    Aggregate functions (must sum up to 1.0).\n    \"\"\"\n    # Cache class variables\n    self.model_list = model_list\n    self.metalearner = metalearner\n    self.sampling = sampling\n    self.sampling_seed = 0\n    self.cache_dir = None\n\n    # Initialize Metalearner\n    if isinstance(metalearner, str) and metalearner in metalearner_dict:\n        self.ml_model = metalearner_dict[metalearner]()\n    elif isinstance(metalearner, str) and metalearner in aggregate_dict:\n        self.ml_model = aggregate_dict[metalearner]()\n    elif isinstance(metalearner, Metalearner_Base) or \\\n         isinstance(metalearner, Aggregate_Base):\n        self.ml_model = metalearner\n    else : raise TypeError(\"Unknown type of Metalearner (neither known \" + \\\n                           \"ensembler nor Aggregate or Metalearner class)!\")\n\n    # Set multiprocessing method to spawn\n    mp.set_start_method(\"spawn\", force=True)\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.dump","title":"<code>dump(directory_path)</code>","text":"<p>Store temporary Stacking models directory permanently to disk at desired location.</p> <p>If the model directory is a provided path which is already persistent on the disk, the directory is copied in order to keep original data persistent.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Path to store the model directory on disk.</p> required Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def dump(self, directory_path):\n    \"\"\" Store temporary Stacking models directory permanently to disk at desired location.\n\n    If the model directory is a provided path which is already persistent on the disk,\n    the directory is copied in order to keep original data persistent.\n\n    Args:\n        directory_path (str):       Path to store the model directory on disk.\n    \"\"\"\n    if self.cache_dir is None:\n        raise FileNotFoundError(\"Stacking does not have a valid model cache directory!\")\n    elif isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        shutil.copytree(self.cache_dir.name, directory_path,\n                        dirs_exist_ok=True)\n        self.cache_dir.cleanup()\n        self.cache_dir = directory_path\n    else:\n        shutil.copytree(self.cache_dir, directory_path, dirs_exist_ok=True)\n        self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.load","title":"<code>load(directory_path)</code>","text":"<p>Load a Stacking model directory which can be used for Metalearner based inference.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Input path, from which the Stacking models will be loaded.</p> required Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def load(self, directory_path):\n    \"\"\" Load a Stacking model directory which can be used for Metalearner based inference.\n\n    Args:\n        directory_path (str):       Input path, from which the Stacking models will be loaded.\n    \"\"\"\n    # Check directory existence\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(\"Provided model directory path does not exist!\",\n                                directory_path)\n    # Check model existence\n    for i in range(len(self.model_list)):\n        path_model = os.path.join(directory_path,\n                                  \"nn_\" + str(i) + \".model.keras\")\n        if not os.path.exists(path_model):\n            raise FileNotFoundError(\"Stacking model \" + str(i) + \\\n                                    \" does not exist!\", path_model)\n    # If heterogenous metalearner -&gt; load metalearner model file\n    if isinstance(self.ml_model, Metalearner_Base):\n        path_model = os.path.join(directory_path,\n                                  \"metalearner.model.pickle\")\n        if not os.path.exists(path_model):\n            raise FileNotFoundError(\"Metalearner model does not exist!\",\n                                    path_model)\n        self.ml_model.load(path_model)\n\n    # Update model directory\n    self.cache_dir = directory_path\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.predict","title":"<code>predict(prediction_generator, return_ensemble=False)</code>","text":"<p>Prediction function for Stacking.</p> <p>The fitted models and selected Metalearner will predict classifications for the provided DataGenerator.</p> <p>Info</p> <p>More about Metalearners can be found here: Metelearner</p> <p>More about Aggregate functions can be found here: aggregate</p> <p>Parameters:</p> Name Type Description Default <code>prediction_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for inference.</p> required <code>return_ensemble</code> <code>bool</code> <p>Option, whether gathered ensemble of predictions should be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels).</p> <code>ensemble</code> <code>numpy.ndarray</code> <p>Optional ensemble of predictions: Will be only passed if <code>return_ensemble=True</code>.                                     Shape (n_models, n_samples, n_labels).</p> Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def predict(self, prediction_generator, return_ensemble=False):\n    \"\"\" Prediction function for Stacking.\n\n    The fitted models and selected Metalearner will predict classifications for the provided\n    [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    !!! info\n        More about Metalearners can be found here: [Metelearner][aucmedi.ensemble.metalearner]\n\n        More about Aggregate functions can be found here: [aggregate][aucmedi.ensemble.aggregate]\n\n    Args:\n        prediction_generator (DataGenerator):   A data generator which will be used for inference.\n        return_ensemble (bool):                 Option, whether gathered ensemble of predictions should be returned.\n\n    Returns:\n        preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n        ensemble (numpy.ndarray):               Optional ensemble of predictions: Will be only passed if `return_ensemble=True`.\n                                                Shape (n_models, n_samples, n_labels).\n    \"\"\"\n    # Verify if there is a linked cache dictionary\n    con_tmp = (isinstance(self.cache_dir, tempfile.TemporaryDirectory) and \\\n               os.path.exists(self.cache_dir.name))\n    con_var = (self.cache_dir is not None and \\\n               not isinstance(self.cache_dir, tempfile.TemporaryDirectory) \\\n               and os.path.exists(self.cache_dir))\n    if not con_tmp and not con_var:\n        raise FileNotFoundError(\"Stacking does not have a valid model cache directory!\")\n\n    # Initialize some variables\n    temp_dg = prediction_generator\n    preds_ensemble = []\n    preds_final = []\n\n    # Extract data\n    data_test = (temp_dg.samples, temp_dg.labels, temp_dg.metadata)\n\n    # Identify path to model directory\n    if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        path_model_dir = self.cache_dir.name\n    else : path_model_dir = self.cache_dir\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        path_model = os.path.join(path_model_dir,\n                                  \"nn_\" + str(i) + \".model.keras\")\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Start inference process for model i\n        process_queue = mp.Queue()\n        process_pred = mp.Process(target=__prediction_process__,\n                                  args=(process_queue,\n                                        model_paras,\n                                        path_model,\n                                        data_test,\n                                        datagen_paras))\n        process_pred.start()\n        process_pred.join()\n        preds = process_queue.get()\n\n        # Append preds to ensemble\n        preds_ensemble.append(preds)\n\n    # Preprocess prediction ensemble\n    preds_ensemble = np.array(preds_ensemble)\n    preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n\n    # Apply heterogenous metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        s, m, c = preds_ensemble.shape\n        x_stack = np.reshape(preds_ensemble, (s, m*c))\n        preds_final = self.ml_model.predict(data=x_stack)\n    # Apply homogeneous aggregate function\n    elif isinstance(self.ml_model, Aggregate_Base):\n        for i in range(preds_ensemble.shape[0]):\n            pred_sample = self.ml_model.aggregate(preds_ensemble[i,:,:])\n            preds_final.append(pred_sample)\n\n    # Convert prediction list to NumPy\n    preds_final = np.asarray(preds_final)\n\n    # Return ensembled predictions\n    if return_ensemble : return preds_final, np.swapaxes(preds_ensemble,1,0)\n    else : return preds_final\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.train","title":"<code>train(training_generator, epochs=20, iterations=None, callbacks=[], class_weights=None, transfer_learning=False, metalearner_fitting=True)</code>","text":"<p>Training function for fitting the provided Stacking models.</p> <p>The training data will be sampled according to a percentage split in which DataGenerators for model training and validation as well as one for the metalearner training will be automatically created.</p> <p>It is also possible to pass custom Callback classes in order to obtain more information.</p> <p>For more information on the fitting process, check out NeuralNetwork.train().</p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training (will be split according                                     to percentage split sampling).</p> required <code>epochs</code> <code>int</code> <p>Number of epochs. A single epoch is defined as one iteration through                                     the complete data set.</p> <code>20</code> <code>iterations</code> <code>int</code> <p>Number of iterations (batches) in a single epoch.</p> <code>None</code> <code>callbacks</code> <code>list of Callback classes</code> <p>A list of Callback classes for custom evaluation.</p> <code>[]</code> <code>class_weights</code> <code>dictionary or list</code> <p>A list or dictionary of float values to handle class unbalance.</p> <code>None</code> <code>transfer_learning</code> <code>bool</code> <p>Option whether a transfer learning training should be performed.</p> <code>False</code> <code>metalearner_fitting</code> <code>bool</code> <p>Option whether the Metalearner fitting process should be included in the                                     Stacking training process. The <code>train_metalearner()</code> function can also be                                     run manually (or repeatedly).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def train(self, training_generator, epochs=20, iterations=None,\n          callbacks=[], class_weights=None, transfer_learning=False,\n          metalearner_fitting=True):\n    \"\"\" Training function for fitting the provided Stacking models.\n\n    The training data will be sampled according to a percentage split in which\n    [DataGenerators][aucmedi.data_processing.data_generator.DataGenerator] for model training\n    and validation as well as one for the metalearner training will be automatically created.\n\n    It is also possible to pass custom Callback classes in order to obtain more information.\n\n    For more information on the fitting process, check out [NeuralNetwork.train()][aucmedi.neural_network.model.NeuralNetwork.train].\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                to percentage split sampling).\n        epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                the complete data set.\n        iterations (int):                       Number of iterations (batches) in a single epoch.\n        callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n        class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n        transfer_learning (bool):               Option whether a transfer learning training should be performed.\n        metalearner_fitting (bool):             Option whether the Metalearner fitting process should be included in the\n                                                Stacking training process. The `train_metalearner()` function can also be\n                                                run manually (or repeatedly).\n    Returns:\n        history (dict):                   A history dictionary from a Keras history object which contains several logs.\n    \"\"\"\n    temp_dg = training_generator    # Template DataGenerator variable for faster access\n    history_stacking = {}           # Final history dictionary\n\n    # Create temporary model directory\n    self.cache_dir = tempfile.TemporaryDirectory(prefix=\"aucmedi.tmp.\",\n                                                 suffix=\".stacking\")\n\n    # Obtain training data\n    x = training_generator.samples\n    y = training_generator.labels\n    m = training_generator.metadata\n\n    # Apply percentage split sampling\n    ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                 stratified=True, iterative=True,\n                                 seed=self.sampling_seed)\n\n    # Pack data according to sampling\n    if len(ps_sampling[0]) == 3:\n        data_train = ps_sampling[0]\n        data_val = ps_sampling[1]\n    else:\n        data_train = (*ps_sampling[0], None)\n        data_val = (*ps_sampling[1], None)\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        # Create model specific callback list\n        callbacks_model = callbacks.copy()\n        # Extend Callback list\n        path_model = os.path.join(self.cache_dir.name,\n                                  \"nn_\" + str(i) + \".model.keras\")\n        cb_mc = ModelCheckpoint(path_model,\n                                monitor=\"val_loss\", verbose=1,\n                                save_best_only=True, mode=\"min\")\n        cb_cl = CSVLogger(os.path.join(self.cache_dir.name,\n                                             \"nn_\" + str(i) + \\\n                                             \".logs.csv\"),\n                          separator=',', append=True)\n        callbacks_model.extend([cb_mc, cb_cl])\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Gather training parameters\n        parameters_training = {\"epochs\": epochs,\n                            \"iterations\": iterations,\n                            \"callbacks\": callbacks_model,\n                            \"class_weights\": class_weights,\n                            \"transfer_learning\": transfer_learning\n        }\n\n        # Start training process\n        process_queue = mp.Queue()\n        process_train = mp.Process(target=__training_process__,\n                                   args=(process_queue,\n                                         model_paras,\n                                         data_train,\n                                         data_val,\n                                         datagen_paras,\n                                         parameters_training))\n        process_train.start()\n        process_train.join()\n        nn_history = process_queue.get()\n        # Combine logged history objects\n        hnn = {\"nn_\" + str(i) + \".\" + k: v for k, v in nn_history.items()}\n        history_stacking = {**history_stacking, **hnn}\n\n    # Perform metalearner model training\n    self.train_metalearner(temp_dg)\n\n    # Return Stacking history object\n    return history_stacking\n</code></pre>"},{"location":"reference/ensemble/stacking/#aucmedi.ensemble.stacking.Stacking.train_metalearner","title":"<code>train_metalearner(training_generator)</code>","text":"<p>Training function for fitting the Metalearner model.</p> <p>Function will be called automatically in the <code>train()</code> function if the parameter <code>metalearner_fitting</code> is true.</p> <p>However, this function can also be called multiple times for training different Metalearner types without the need of time-extensive re-training of the NeuralNetwork models.</p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training (will be split according                                     to percentage split sampling).</p> required Source code in <code>aucmedi/ensemble/stacking.py</code> <pre><code>def train_metalearner(self, training_generator):\n    \"\"\" Training function for fitting the Metalearner model.\n\n    Function will be called automatically in the `train()` function if\n    the parameter `metalearner_fitting` is true.\n\n    However, this function can also be called multiple times for training\n    different Metalearner types without the need of time-extensive\n    re-training of the [NeuralNetwork][aucmedi.neural_network.model] models.\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training (will be split according\n                                                to percentage split sampling).\n    \"\"\"\n    # Skipping metalearner training if aggregate function\n    if isinstance(self.ml_model, Aggregate_Base) : return\n\n    temp_dg = training_generator    # Template DataGenerator variable for faster access\n    preds_ensemble = []\n\n    # Obtain training data\n    x = training_generator.samples\n    y = training_generator.labels\n    m = training_generator.metadata\n\n    # Apply percentage split sampling\n    ps_sampling = sampling_split(x, y, m, sampling=self.sampling,\n                                 stratified=True, iterative=True,\n                                 seed=self.sampling_seed)\n\n    # Pack data according to sampling\n    if len(ps_sampling[0]) == 3 : data_ensemble = ps_sampling[2]\n    else : data_ensemble = (*ps_sampling[2], None)\n\n    # Identify path to model directory\n    if isinstance(self.cache_dir, tempfile.TemporaryDirectory):\n        path_model_dir = self.cache_dir.name\n    else : path_model_dir = self.cache_dir\n\n    # Sequentially iterate over model list\n    for i in range(len(self.model_list)):\n        #  Load current model\n        path_model = os.path.join(path_model_dir,\n                                  \"nn_\" + str(i) + \".model.keras\")\n\n        # Gather NeuralNetwork parameters\n        model_paras = {\n            \"n_labels\": self.model_list[i].n_labels,\n            \"channels\": self.model_list[i].channels,\n            \"input_shape\": self.model_list[i].input_shape,\n            \"architecture\": self.model_list[i].architecture,\n            \"pretrained_weights\": self.model_list[i].pretrained_weights,\n            \"loss\": self.model_list[i].loss,\n            \"metrics\": None,\n            \"activation_output\": self.model_list[i].activation_output,\n            \"fcl_dropout\": self.model_list[i].fcl_dropout,\n            \"meta_variables\": self.model_list[i].meta_variables,\n            \"learning_rate\": self.model_list[i].learning_rate,\n        }\n\n        # Gather DataGenerator parameters\n        datagen_paras = {\"path_imagedir\": temp_dg.path_imagedir,\n                         \"batch_size\": temp_dg.batch_size,\n                         \"data_aug\": temp_dg.data_aug,\n                         \"seed\": temp_dg.seed,\n                         \"subfunctions\": temp_dg.subfunctions,\n                         \"shuffle\": temp_dg.shuffle,\n                         \"standardize_mode\": self.model_list[i].meta_standardize,\n                         \"resize\": self.model_list[i].meta_input,\n                         \"grayscale\": temp_dg.grayscale,\n                         \"prepare_images\": temp_dg.prepare_images,\n                         \"sample_weights\": temp_dg.sample_weights,\n                         \"image_format\": temp_dg.image_format,\n                         \"loader\": temp_dg.sample_loader,\n                         \"workers\": temp_dg.workers,\n                         \"kwargs\": temp_dg.kwargs\n        }\n\n        # Start inference process for model i\n        process_queue = mp.Queue()\n        process_pred = mp.Process(target=__prediction_process__,\n                                  args=(process_queue,\n                                        model_paras,\n                                        path_model,\n                                        data_ensemble,\n                                        datagen_paras))\n        process_pred.start()\n        process_pred.join()\n        preds = process_queue.get()\n\n        # Append preds to ensemble\n        preds_ensemble.append(preds)\n\n    # Preprocess prediction ensemble\n    preds_ensemble = np.array(preds_ensemble)\n    preds_ensemble = np.swapaxes(preds_ensemble, 0, 1)\n    s, m, c = preds_ensemble.shape\n    x_stack = np.reshape(preds_ensemble, (s, m*c))\n\n    # Start training of stacked metalearner\n    if isinstance(self.ml_model, Metalearner_Base):\n        (_, y_stack, _) = data_ensemble\n        self.ml_model.train(x_stack, y_stack)\n        # Store metalearner model to disk\n        path_metalearner = os.path.join(path_model_dir,\n                                        \"metalearner.model.pickle\")\n        self.ml_model.dump(path_metalearner)\n</code></pre>"},{"location":"reference/ensemble/aggregate/","title":"Aggregate","text":"<p>Library of implemented Aggregate functions in AUCMEDI.</p> <p>An Aggregate function can be passed to an ensemble and merges multiple class predictions into a single prediction.</p> <pre><code>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\nExample: [[0.5, 0.4, 0.1],\n          [0.4, 0.3, 0.3],\n          [0.5, 0.2, 0.3]]\n-&gt; shape (3, 3)\n\nMerged prediction encoded in a NumPy matrix with shape (1, N_classes).\nExample: [[0.4, 0.3, 0.3]]\n-&gt; shape (1, 3)\n</code></pre> Example <pre><code># Recommended: Apply an Ensemble like Augmenting (test-time augmentation) with Majority Vote\npreds = predict_augmenting(model, test_datagen, n_cycles=5, aggregate=\"majority_vote\")\n\n# Manual: Apply an Ensemble like Augmenting (test-time augmentation) with Majority Vote\nfrom aucmedi.ensemble.aggregate import MajorityVote\nmy_agg = MajorityVote()\npreds = predict_augmenting(model, test_datagen, n_cycles=5, aggregate=my_agg)\n</code></pre> <p>Aggregate functions are based on the abstract base class Aggregate_Base, which allows simple integration of custom aggregate methods for Ensemble.</p>"},{"location":"reference/ensemble/aggregate/#aucmedi.ensemble.aggregate.aggregate_dict","title":"<code>aggregate_dict = {'mean': AveragingMean, 'median': AveragingMedian, 'majority_vote': MajorityVote, 'softmax': Softmax, 'global_argmax': GlobalArgmax}</code>  <code>module-attribute</code>","text":"<p>Dictionary of implemented Aggregate functions. </p>"},{"location":"reference/ensemble/aggregate/agg_base/","title":"Agg base","text":""},{"location":"reference/ensemble/aggregate/agg_base/#aucmedi.ensemble.aggregate.agg_base.Aggregate_Base","title":"<code>Aggregate_Base</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class for an Aggregation class.</p> <pre><code>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\nExample: [[0.5, 0.4, 0.1],\n          [0.4, 0.3, 0.3],\n          [0.5, 0.2, 0.3]]\n-&gt; shape (3, 3)\n\nMerged prediction encoded in a NumPy matrix with shape (1, N_classes).\nExample: [[0.4, 0.3, 0.3]]\n-&gt; shape (1, 3)\n</code></pre> Create a custom Aggregation class <pre><code>from aucmedi.ensemble.aggregate.agg_base import Aggregate_Base\n\nclass My_custom_Aggregate(Aggregate_Base):\n    def __init__(self):                 # you can pass class variables here\n        pass\n\n    def aggregate(self, preds):\n        preds_combined = np.mean(preds, axis=0)     # do some combination operation\n        return preds_combined                       # return combined predictions\n</code></pre> <p>Required Functions</p> Function Description <code>__init__()</code> Object creation function. <code>aggregate()</code> Merge multiple class predictions into a single prediction. Source code in <code>aucmedi/ensemble/aggregate/agg_base.py</code> <pre><code>class Aggregate_Base(ABC):\n    \"\"\" An abstract base class for an Aggregation class.\n\n    ```\n    Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n    Example: [[0.5, 0.4, 0.1],\n              [0.4, 0.3, 0.3],\n              [0.5, 0.2, 0.3]]\n    -&gt; shape (3, 3)\n\n    Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n    Example: [[0.4, 0.3, 0.3]]\n    -&gt; shape (1, 3)\n    ```\n\n    ???+ example \"Create a custom Aggregation class\"\n        ```python\n        from aucmedi.ensemble.aggregate.agg_base import Aggregate_Base\n\n        class My_custom_Aggregate(Aggregate_Base):\n            def __init__(self):                 # you can pass class variables here\n                pass\n\n            def aggregate(self, preds):\n                preds_combined = np.mean(preds, axis=0)     # do some combination operation\n                return preds_combined                       # return combined predictions\n        ```\n\n    !!! info \"Required Functions\"\n        | Function            | Description                                                |\n        | ------------------- | ---------------------------------------------------------- |\n        | `__init__()`        | Object creation function.                                  |\n        | `aggregate()`       | Merge multiple class predictions into a single prediction. |\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    @abstractmethod\n    def __init__(self):\n        \"\"\" Initialization function which will be called during the Aggregation object creation.\n\n        This function can be used to pass variables and options in the Aggregation instance.\n        There are no mandatory parameters for the initialization.\n        \"\"\"\n        pass\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    @abstractmethod\n    def aggregate(self, preds):\n        \"\"\" Aggregate the image by merging multiple predictions into a single one.\n\n        It is required to return the merged predictions (as NumPy matrix).\n        It is possible to pass configurations through the initialization function for this class.\n\n        Args:\n            preds (numpy.ndarray):      Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n        Returns:\n            pred (numpy.ndarray):       Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n        \"\"\"\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/agg_base/#aucmedi.ensemble.aggregate.agg_base.Aggregate_Base.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialization function which will be called during the Aggregation object creation.</p> <p>This function can be used to pass variables and options in the Aggregation instance. There are no mandatory parameters for the initialization.</p> Source code in <code>aucmedi/ensemble/aggregate/agg_base.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n    \"\"\" Initialization function which will be called during the Aggregation object creation.\n\n    This function can be used to pass variables and options in the Aggregation instance.\n    There are no mandatory parameters for the initialization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/aggregate/agg_base/#aucmedi.ensemble.aggregate.agg_base.Aggregate_Base.aggregate","title":"<code>aggregate(preds)</code>  <code>abstractmethod</code>","text":"<p>Aggregate the image by merging multiple predictions into a single one.</p> <p>It is required to return the merged predictions (as NumPy matrix). It is possible to pass configurations through the initialization function for this class.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>numpy.ndarray</code> <p>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).</p> required <p>Returns:</p> Name Type Description <code>pred</code> <code>numpy.ndarray</code> <p>Merged prediction encoded in a NumPy matrix with shape (1, N_classes).</p> Source code in <code>aucmedi/ensemble/aggregate/agg_base.py</code> <pre><code>@abstractmethod\ndef aggregate(self, preds):\n    \"\"\" Aggregate the image by merging multiple predictions into a single one.\n\n    It is required to return the merged predictions (as NumPy matrix).\n    It is possible to pass configurations through the initialization function for this class.\n\n    Args:\n        preds (numpy.ndarray):      Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n    Returns:\n        pred (numpy.ndarray):       Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n    \"\"\"\n    return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/averaging_mean/","title":"Averaging mean","text":""},{"location":"reference/ensemble/aggregate/averaging_mean/#aucmedi.ensemble.aggregate.averaging_mean.AveragingMean","title":"<code>AveragingMean</code>","text":"<p>         Bases: <code>Aggregate_Base</code></p> <p>Aggregate function based on averaging via mean.</p> <p>This class should be passed to an ensemble function/class for combining predictions.</p> Source code in <code>aucmedi/ensemble/aggregate/averaging_mean.py</code> <pre><code>class AveragingMean(Aggregate_Base):\n    \"\"\" Aggregate function based on averaging via mean.\n\n    This class should be passed to an ensemble function/class for combining predictions.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        # No hyperparameter adjustment required for this method, therefore skip\n        pass\n\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    def aggregate(self, preds):\n        # Merge predictions via mean\n        pred = np.mean(preds, axis=0)\n        # Return merged prediction\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/averaging_median/","title":"Averaging median","text":""},{"location":"reference/ensemble/aggregate/averaging_median/#aucmedi.ensemble.aggregate.averaging_median.AveragingMedian","title":"<code>AveragingMedian</code>","text":"<p>         Bases: <code>Aggregate_Base</code></p> <p>Aggregate function based on averaging via median.</p> <p>This class should be passed to an ensemble function/class for combining predictions.</p> Source code in <code>aucmedi/ensemble/aggregate/averaging_median.py</code> <pre><code>class AveragingMedian(Aggregate_Base):\n    \"\"\" Aggregate function based on averaging via median.\n\n    This class should be passed to an ensemble function/class for combining predictions.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        # No hyperparameter adjustment required for this method, therefore skip\n        pass\n\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    def aggregate(self, preds):\n        # Merge predictions via median\n        pred = np.median(preds, axis=0)\n        # Return merged prediction\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/global_argmax/","title":"Global argmax","text":""},{"location":"reference/ensemble/aggregate/global_argmax/#aucmedi.ensemble.aggregate.global_argmax.GlobalArgmax","title":"<code>GlobalArgmax</code>","text":"<p>         Bases: <code>Aggregate_Base</code></p> <p>Aggregate function based on Global Argmax.</p> <p>This class should be passed to an ensemble function/class for combining predictions.</p> Source code in <code>aucmedi/ensemble/aggregate/global_argmax.py</code> <pre><code>class GlobalArgmax(Aggregate_Base):\n    \"\"\" Aggregate function based on Global Argmax.\n\n    This class should be passed to an ensemble function/class for combining predictions.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        # No hyperparameter adjustment required for this method, therefore skip\n        pass\n\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    def aggregate(self, preds):\n        # Identify global argmax\n        max = np.amax(preds)\n        argmax_flatten = np.argmax(preds)\n        argmax = np.unravel_index(argmax_flatten, preds.shape)[-1]\n\n        # Compute prediction by global argmax and equally distributed remaining\n        # probability for other classes\n        prob_remaining = np.divide(1-max, preds.shape[1]-1)\n        pred = np.full((preds.shape[1],), fill_value=prob_remaining)\n        pred[argmax] = max\n\n        # Return prediction\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/majority_vote/","title":"Majority vote","text":""},{"location":"reference/ensemble/aggregate/majority_vote/#aucmedi.ensemble.aggregate.majority_vote.MajorityVote","title":"<code>MajorityVote</code>","text":"<p>         Bases: <code>Aggregate_Base</code></p> <p>Aggregate function based on majority vote.</p> <p>This class should be passed to an ensemble function/class for combining predictions.</p> Source code in <code>aucmedi/ensemble/aggregate/majority_vote.py</code> <pre><code>class MajorityVote(Aggregate_Base):\n    \"\"\" Aggregate function based on majority vote.\n\n    This class should be passed to an ensemble function/class for combining predictions.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        # No hyperparameter adjustment required for this method, therefore skip\n        pass\n\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    def aggregate(self, preds):\n        # Count votes\n        votes = np.argmax(preds, axis=1)\n        # Identify majority\n        majority_vote = np.argmax(np.bincount(votes))\n        # Create prediction based on majority vote\n        pred = np.zeros((preds.shape[1]))\n        pred[majority_vote] = 1\n        # Return prediction\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/softmax/","title":"Softmax","text":""},{"location":"reference/ensemble/aggregate/softmax/#aucmedi.ensemble.aggregate.softmax.Softmax","title":"<code>Softmax</code>","text":"<p>         Bases: <code>Aggregate_Base</code></p> <p>Aggregate function based on softmax.</p> <p>This class should be passed to an ensemble function/class for combining predictions.</p> Source code in <code>aucmedi/ensemble/aggregate/softmax.py</code> <pre><code>class Softmax(Aggregate_Base):\n    \"\"\" Aggregate function based on softmax.\n\n    This class should be passed to an ensemble function/class for combining predictions.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        # No hyperparameter adjustment required for this method, therefore skip\n        pass\n\n    #---------------------------------------------#\n    #                  Aggregate                  #\n    #---------------------------------------------#\n    def aggregate(self, preds):\n        # Sum up predictions\n        preds_sum = np.sum(preds, axis=0)\n        # Calculate softmax\n        pred = compute_softmax(preds_sum)\n        # Return prediction\n        return pred\n</code></pre>"},{"location":"reference/ensemble/aggregate/softmax/#aucmedi.ensemble.aggregate.softmax.compute_softmax","title":"<code>compute_softmax(x)</code>","text":"<p>Compute softmax values for each sets of scores in x.</p> Source code in <code>aucmedi/ensemble/aggregate/softmax.py</code> <pre><code>def compute_softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n</code></pre>"},{"location":"reference/ensemble/metalearner/","title":"Metalearner","text":"<p>Library of implemented Metalearners in AUCMEDI.</p> <p>A Metalearner can be passed to an ensemble like Stacking and merges multiple class predictions into a single prediction.</p> <p>Metalearner are similar to aggregate() functions, however Metalearners are models which require fitting before usage.</p> <pre><code>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\nExample: [[0.5, 0.4, 0.1],\n          [0.4, 0.3, 0.3],\n          [0.5, 0.2, 0.3]]\n-&gt; shape (3, 3)\n\nMerged prediction encoded in a NumPy matrix with shape (1, N_classes).\nExample: [[0.4, 0.3, 0.3]]\n-&gt; shape (1, 3)\n</code></pre> <p>Warning</p> <p>Not all Metalearners support multi-label classification!</p> <p>If a Metalearner has multi-label support can be found in its docs entry.</p> <p>Metalearners are based on the abstract base class Metalearner_Base, which allows simple integration of custom Metalearners for Ensemble.</p>"},{"location":"reference/ensemble/metalearner/#aucmedi.ensemble.metalearner.metalearner_dict","title":"<code>metalearner_dict = {'logistic_regression': LogisticRegression, 'naive_bayes': NaiveBayes, 'support_vector_machine': SupportVectorMachine, 'gaussian_process': GaussianProcess, 'decision_tree': DecisionTree, 'best_model': BestModel, 'weighted_mean': AveragingWeightedMean, 'random_forest': RandomForest, 'k_neighbors': KNearestNeighbors, 'mlp': MLP}</code>  <code>module-attribute</code>","text":"<p>Dictionary of implemented Metalearners. </p>"},{"location":"reference/ensemble/metalearner/averaging_mean_weighted/","title":"Averaging mean weighted","text":""},{"location":"reference/ensemble/metalearner/averaging_mean_weighted/#aucmedi.ensemble.metalearner.averaging_mean_weighted.AveragingWeightedMean","title":"<code>AveragingWeightedMean</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Weighted Mean based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>This Metalearner computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) for each model, and utilizes these scores for a weighted Mean to average predictions.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Source code in <code>aucmedi/ensemble/metalearner/averaging_mean_weighted.py</code> <pre><code>class AveragingWeightedMean(Metalearner_Base):\n    \"\"\" A Weighted Mean based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    This Metalearner computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n    for each model, and utilizes these scores for a weighted Mean to average predictions.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = {}\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Identify number of models and classes\n        n_classes = y.shape[1]\n        n_models = int(x.shape[1] / n_classes)\n        # Preprocess data input\n        data = np.reshape(x, (x.shape[0], n_models, n_classes))\n\n        # Compute AUC scores and store them to cache\n        weights = []\n        for m in range(n_models):\n            pred = data[:,m,:]\n            score = roc_auc_score(y, pred, average=\"macro\")\n            weights.append(score)\n\n        # Store results to cache\n        self.model[\"weights\"] = weights\n        self.model[\"n_classes\"] = n_classes\n        self.model[\"n_models\"] = n_models\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Preprocess data input\n        preds = np.reshape(data, (data.shape[0],\n                                  self.model[\"n_models\"],\n                                  self.model[\"n_classes\"]))\n        # Compute weighted mean\n        pred = np.average(preds, axis=1, weights=self.model[\"weights\"])\n        # Return results\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/best_model/","title":"Best model","text":""},{"location":"reference/ensemble/metalearner/best_model/#aucmedi.ensemble.metalearner.best_model.BestModel","title":"<code>BestModel</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Best Model based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>This Metalearner computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) for each model, and simply utilizes only the predictions of the best scoring model.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Source code in <code>aucmedi/ensemble/metalearner/best_model.py</code> <pre><code>class BestModel(Metalearner_Base):\n    \"\"\" A Best Model based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    This Metalearner computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n    for each model, and simply utilizes only the predictions of the best scoring model.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = {}\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Identify number of models and classes\n        n_classes = y.shape[1]\n        n_models = int(x.shape[1] / n_classes)\n        # Preprocess data input\n        data = np.reshape(x, (x.shape[0], n_models, n_classes))\n\n        # Compute AUC scores and store them to cache\n        for m in range(n_models):\n            pred = data[:,m,:]\n            score = roc_auc_score(y, pred, average=\"macro\")\n            self.model[\"nn_\" + str(m)] = score\n\n        # Identify best model and store results to cache\n        best_model = max(self.model, key=self.model.get)\n        self.model[\"best_model\"] = best_model\n        self.model[\"n_classes\"] = n_classes\n        self.model[\"n_models\"] = n_models\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Preprocess data input\n        preds = np.reshape(data, (data.shape[0],\n                                  self.model[\"n_models\"],\n                                  self.model[\"n_classes\"]))\n\n        # Obtain prediction probabilities of best model\n        m = int(self.model[\"best_model\"].split(\"_\")[-1])\n        pred_best = preds[:,m,:]\n        # Return results\n        return pred_best\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/decision_tree/","title":"Decision tree","text":""},{"location":"reference/ensemble/metalearner/decision_tree/#aucmedi.ensemble.metalearner.decision_tree.DecisionTree","title":"<code>DecisionTree</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Decision Tree based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/decision_tree.py</code> <pre><code>class DecisionTree(Metalearner_Base):\n    \"\"\" A Decision Tree based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = DecisionTreeClassifier(random_state=0)\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Postprocess decision tree predictions\n        pred = np.asarray(pred)\n        pred = np.swapaxes(pred[:,:,1], 0, 1)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/gaussian_process/","title":"Gaussian process","text":""},{"location":"reference/ensemble/metalearner/gaussian_process/#aucmedi.ensemble.metalearner.gaussian_process.GaussianProcess","title":"<code>GaussianProcess</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Gaussian Process based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Warning</p> <p>Can only be utilized for binary and multi-class tasks.</p> <p>Does not work on multi-label annotations!</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/gaussian_process.py</code> <pre><code>class GaussianProcess(Metalearner_Base):\n    \"\"\" A Gaussian Process based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! warning\n        Can only be utilized for binary and multi-class tasks.\n\n        Does not work on multi-label annotations!\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = GaussianProcessClassifier(random_state=0,\n                                               multi_class=\"one_vs_rest\")\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Preprocess to sparse encoding\n        y = np.argmax(y, axis=-1)\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/k_neighbors/","title":"K neighbors","text":""},{"location":"reference/ensemble/metalearner/k_neighbors/#aucmedi.ensemble.metalearner.k_neighbors.KNearestNeighbors","title":"<code>KNearestNeighbors</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A k-Nearest Neighbors based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/k_neighbors.py</code> <pre><code>class KNearestNeighbors(Metalearner_Base):\n    \"\"\" A k-Nearest Neighbors based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = KNeighborsClassifier()\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Postprocess decision tree predictions\n        pred = np.asarray(pred)\n        pred = np.swapaxes(pred[:,:,1], 0, 1)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/logistic_regression/","title":"Logistic regression","text":""},{"location":"reference/ensemble/metalearner/logistic_regression/#aucmedi.ensemble.metalearner.logistic_regression.LogisticRegression","title":"<code>LogisticRegression</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Logistic Regression based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Warning</p> <p>Can only be utilized for binary and multi-class tasks.</p> <p>Does not work on multi-label annotations!</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/logistic_regression.py</code> <pre><code>class LogisticRegression(Metalearner_Base):\n    \"\"\" A Logistic Regression based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! warning\n        Can only be utilized for binary and multi-class tasks.\n\n        Does not work on multi-label annotations!\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = LRscikit(random_state=0, solver=\"newton-cg\",\n                              multi_class=\"multinomial\")\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Preprocess to sparse encoding\n        y = np.argmax(y, axis=-1)\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/","title":"Ml base","text":""},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base","title":"<code>Metalearner_Base</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class for a Metalearner class.</p> <p>Metalearner are similar to Aggregate functions, however Metalearners are models which require fitting before usage.</p> <p>Metalearners are utilized in Stacking pipelines.</p> <p>A Metalearner act as a combiner algorithm which is trained to make a final prediction using predictions of other algorithms (<code>NeuralNetwork</code>) as inputs.</p> <pre><code>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\nExample: [[0.5, 0.4, 0.1],\n          [0.4, 0.3, 0.3],\n          [0.5, 0.2, 0.3]]\n-&gt; shape (3, 3)\n\nMerged prediction encoded in a NumPy matrix with shape (1, N_classes).\nExample: [[0.4, 0.3, 0.3]]\n-&gt; shape (1, 3)\n</code></pre> <p>Required Functions</p> Function Description <code>__init__()</code> Object creation function. <code>training()</code> Fit Metalearner model. <code>prediction()</code> Merge multiple class predictions into a single prediction. <code>dump()</code> Store Metalearner model to disk. <code>load()</code> Load Metalearner model from disk. Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>class Metalearner_Base(ABC):\n    \"\"\" An abstract base class for a Metalearner class.\n\n    Metalearner are similar to [Aggregate functions][aucmedi.ensemble.aggregate],\n    however Metalearners are models which require fitting before usage.\n\n    Metalearners are utilized in [Stacking][aucmedi.ensemble.stacking] pipelines.\n\n    A Metalearner act as a combiner algorithm which is trained to make a final prediction\n    using predictions of other algorithms (`NeuralNetwork`) as inputs.\n\n    ```\n    Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n    Example: [[0.5, 0.4, 0.1],\n              [0.4, 0.3, 0.3],\n              [0.5, 0.2, 0.3]]\n    -&gt; shape (3, 3)\n\n    Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n    Example: [[0.4, 0.3, 0.3]]\n    -&gt; shape (1, 3)\n    ```\n\n    !!! info \"Required Functions\"\n        | Function            | Description                                                |\n        | ------------------- | ---------------------------------------------------------- |\n        | `__init__()`        | Object creation function.                                  |\n        | `training()`        | Fit Metalearner model.                                     |\n        | `prediction()`      | Merge multiple class predictions into a single prediction. |\n        | `dump()`            | Store Metalearner model to disk.                           |\n        | `load()`            | Load Metalearner model from disk.                          |\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    @abstractmethod\n    def __init__(self):\n        \"\"\" Initialization function which will be called during the Metalearner object creation.\n\n        This function can be used to pass variables and options in the Metalearner instance.\n        There are no mandatory parameters for the initialization.\n        \"\"\"\n        pass\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    @abstractmethod\n    def train(self, x, y):\n        \"\"\" Training function to fit the Metalearner model.\n\n        Args:\n            x (numpy.ndarray):          Assembled prediction dataset encoded in a NumPy matrix with shape (N_samples, N_classes*N_models).\n            y (numpy.ndarray):          Classification list with One-Hot Encoding. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n        \"\"\"\n        pass\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    @abstractmethod\n    def predict(self, data):\n        \"\"\" Merge multiple predictions for a sample into a single prediction.\n\n        It is required to return the merged predictions (as NumPy matrix).\n        It is possible to pass configurations through the initialization function for this class.\n\n        Args:\n            data (numpy.ndarray):       Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n        Returns:\n            pred (numpy.ndarray):       Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n        \"\"\"\n        pass\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    @abstractmethod\n    def dump(self, path):\n        \"\"\" Store Metalearner model to disk.\n\n        Args:\n            path (str):                 Path to store the model on disk.\n        \"\"\"\n        pass\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    @abstractmethod\n    def load(self, path):\n        \"\"\" Load Metalearner model and its weights from a file.\n\n        Args:\n            path (str):                 Input path from which the model will be loaded.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialization function which will be called during the Metalearner object creation.</p> <p>This function can be used to pass variables and options in the Metalearner instance. There are no mandatory parameters for the initialization.</p> Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n    \"\"\" Initialization function which will be called during the Metalearner object creation.\n\n    This function can be used to pass variables and options in the Metalearner instance.\n    There are no mandatory parameters for the initialization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base.dump","title":"<code>dump(path)</code>  <code>abstractmethod</code>","text":"<p>Store Metalearner model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to store the model on disk.</p> required Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>@abstractmethod\ndef dump(self, path):\n    \"\"\" Store Metalearner model to disk.\n\n    Args:\n        path (str):                 Path to store the model on disk.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base.load","title":"<code>load(path)</code>  <code>abstractmethod</code>","text":"<p>Load Metalearner model and its weights from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Input path from which the model will be loaded.</p> required Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>@abstractmethod\ndef load(self, path):\n    \"\"\" Load Metalearner model and its weights from a file.\n\n    Args:\n        path (str):                 Input path from which the model will be loaded.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base.predict","title":"<code>predict(data)</code>  <code>abstractmethod</code>","text":"<p>Merge multiple predictions for a sample into a single prediction.</p> <p>It is required to return the merged predictions (as NumPy matrix). It is possible to pass configurations through the initialization function for this class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>numpy.ndarray</code> <p>Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).</p> required <p>Returns:</p> Name Type Description <code>pred</code> <code>numpy.ndarray</code> <p>Merged prediction encoded in a NumPy matrix with shape (1, N_classes).</p> Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>@abstractmethod\ndef predict(self, data):\n    \"\"\" Merge multiple predictions for a sample into a single prediction.\n\n    It is required to return the merged predictions (as NumPy matrix).\n    It is possible to pass configurations through the initialization function for this class.\n\n    Args:\n        data (numpy.ndarray):       Assembled predictions encoded in a NumPy matrix with shape (N_models, N_classes).\n    Returns:\n        pred (numpy.ndarray):       Merged prediction encoded in a NumPy matrix with shape (1, N_classes).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/ml_base/#aucmedi.ensemble.metalearner.ml_base.Metalearner_Base.train","title":"<code>train(x, y)</code>  <code>abstractmethod</code>","text":"<p>Training function to fit the Metalearner model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>numpy.ndarray</code> <p>Assembled prediction dataset encoded in a NumPy matrix with shape (N_samples, N_classes*N_models).</p> required <code>y</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                         input_interface.</p> required Source code in <code>aucmedi/ensemble/metalearner/ml_base.py</code> <pre><code>@abstractmethod\ndef train(self, x, y):\n    \"\"\" Training function to fit the Metalearner model.\n\n    Args:\n        x (numpy.ndarray):          Assembled prediction dataset encoded in a NumPy matrix with shape (N_samples, N_classes*N_models).\n        y (numpy.ndarray):          Classification list with One-Hot Encoding. Provided by\n                                    [input_interface][aucmedi.data_processing.io_data.input_interface].\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/ensemble/metalearner/mlp/","title":"Mlp","text":""},{"location":"reference/ensemble/metalearner/mlp/#aucmedi.ensemble.metalearner.mlp.MLP","title":"<code>MLP</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>An MLP Neural Network (scikit-learn) based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/mlp.py</code> <pre><code>class MLP(Metalearner_Base):\n    \"\"\" An MLP Neural Network (scikit-learn) based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = MLPClassifier(random_state=0)\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/naive_bayes/","title":"Naive bayes","text":""},{"location":"reference/ensemble/metalearner/naive_bayes/#aucmedi.ensemble.metalearner.naive_bayes.NaiveBayes","title":"<code>NaiveBayes</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Naive Bayes based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Warning</p> <p>Can only be utilized for binary and multi-class tasks.</p> <p>Does not work on multi-label annotations!</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/naive_bayes.py</code> <pre><code>class NaiveBayes(Metalearner_Base):\n    \"\"\" A Naive Bayes based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! warning\n        Can only be utilized for binary and multi-class tasks.\n\n        Does not work on multi-label annotations!\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = ComplementNB()\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Preprocess to sparse encoding\n        y = np.argmax(y, axis=-1)\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/random_forest/","title":"Random forest","text":""},{"location":"reference/ensemble/metalearner/random_forest/#aucmedi.ensemble.metalearner.random_forest.RandomForest","title":"<code>RandomForest</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Random Forest based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Info</p> <p>Can be utilized for binary, multi-class and multi-label tasks.</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/random_forest.py</code> <pre><code>class RandomForest(Metalearner_Base):\n    \"\"\" A Random Forest based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! info\n        Can be utilized for binary, multi-class and multi-label tasks.\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = RandomForestClassifier(random_state=0)\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Postprocess decision tree predictions\n        pred = np.asarray(pred)\n        pred = np.swapaxes(pred[:,:,1], 0, 1)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/ensemble/metalearner/support_vector_machine/","title":"Support vector machine","text":""},{"location":"reference/ensemble/metalearner/support_vector_machine/#aucmedi.ensemble.metalearner.support_vector_machine.SupportVectorMachine","title":"<code>SupportVectorMachine</code>","text":"<p>         Bases: <code>Metalearner_Base</code></p> <p>A Support Vector Machine based Metalearner.</p> <p>This class should be passed to an ensemble function/class like Stacking for combining predictions.</p> <p>Warning</p> <p>Can only be utilized for binary and multi-class tasks.</p> <p>Does not work on multi-label annotations!</p> Reference - Implementation <p>https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</p> <p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</p> Source code in <code>aucmedi/ensemble/metalearner/support_vector_machine.py</code> <pre><code>class SupportVectorMachine(Metalearner_Base):\n    \"\"\" A Support Vector Machine based Metalearner.\n\n    This class should be passed to an ensemble function/class like Stacking for combining predictions.\n\n    !!! warning\n        Can only be utilized for binary and multi-class tasks.\n\n        Does not work on multi-label annotations!\n\n    ???+ abstract \"Reference - Implementation\"\n        https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n\n        Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n        https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self):\n        self.model = SVC(random_state=0,\n                         probability=True,\n                         gamma=\"scale\")\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    def train(self, x, y):\n        # Preprocess to sparse encoding\n        y = np.argmax(y, axis=-1)\n        # Train model\n        self.model = self.model.fit(x, y)\n\n    #---------------------------------------------#\n    #                  Prediction                 #\n    #---------------------------------------------#\n    def predict(self, data):\n        # Compute prediction probabilities via fitted model\n        pred = self.model.predict_proba(data)\n        # Return results as NumPy array\n        return pred\n\n    #---------------------------------------------#\n    #              Dump Model to Disk             #\n    #---------------------------------------------#\n    def dump(self, path):\n        # Dump model to disk via pickle\n        with open(path, \"wb\") as pickle_writer:\n            pickle.dump(self.model, pickle_writer)\n\n    #---------------------------------------------#\n    #             Load Model from Disk            #\n    #---------------------------------------------#\n    def load(self, path):\n        # Load model from disk via pickle\n        with open(path, \"rb\") as pickle_reader:\n            self.model = pickle.load(pickle_reader)\n</code></pre>"},{"location":"reference/evaluation/","title":"Evaluation","text":"<p>Library of evaluation functions in the AUCMEDI environment.</p> <p>The idea of the evaluation function library is to provide quick and simple functionality for inspecting and analyzing data as well as results generated by AUCMEDI.</p> <p>AUCMEDI currently supports the following evaluation techniques:</p> Evaluation Technique Description Dataset Analysis Peak into the loaded medical imaging dataset. Fitting Evaluation Evaluate the fitting curve of a model training process. Performance Evaluation Evaluate the performance of a single model / prediction list through various metrics. Performance Comparison Compare the performance of predictions from multiple models."},{"location":"reference/evaluation/comparison/","title":"Comparison","text":""},{"location":"reference/evaluation/comparison/#aucmedi.evaluation.comparison.evaluate_comparison","title":"<code>evaluate_comparison(pred_list, labels, out_path, model_names=None, class_names=None, multi_label=False, metrics_threshold=0.5, macro_average_classes=False, suffix=None)</code>","text":"<p>Function for performance comparison evaluation based on predictions from multiple models.</p> Example <pre><code># Import libraries\nfrom aucmedi import *\nfrom aucmedi.evaluation import *\nfrom aucmedi.ensemble import *\n\n# Load data\nds = input_interface(interface=\"csv\",                       # Interface type\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize model\nmodel_a = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n\n# Initialize Bagging object for 3-fold cross-validation\nel = Bagging(model_a, k_fold=3)\n\n# Do some predictions via Bagging (return also all prediction ensembles)\ndatagen_test = DataGenerator(samples, \"dataset/images/\", labels=None,\n                             resize=model.meta_input, standardize_mode=model.meta_standardize)\npred_merged, pred_ensemble = el.predict(datagen_test, return_ensemble=True)\n\n# Pass prediction ensemble to evaluation function\nevaluate_comparison(pred_ensemble, class_ohe, out_path=\"./\", class_names=class_names)\n\n\n# Do some predictions with manually initialized models\nmodel_b = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\nmodel_c = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.MobileNetV2\")\n\npred_a = model_a.predict(datagen_test)\npred_b = model_b.predict(datagen_test)\npred_c = model_c.predict(datagen_test)\n\npred_ensemble = [pred_a, pred_b, pred_c]\n\n# Pass prediction ensemble to evaluation function\nevaluate_comparison(pred_ensemble, class_ohe, out_path=\"./\", class_names=class_names)\n</code></pre> <p>Created files in directory of <code>out_path</code>:</p> <ul> <li>\"plot.comparison.beside.png\"</li> <li>\"plot.comparison.gain.png\"</li> </ul> Preview for Bar Plot <p></p> <p>Predictions based on ISIC 2019 Challenge with macro-averaged class-wise metrics.</p> Preview for Confusion Matrix <p></p> <p>Predictions based on ISIC 2019 Challenge.</p> <p>Parameters:</p> Name Type Description Default <code>pred_list</code> <code>list of numpy.ndarray</code> <p>A list of NumPy arrays containing predictions from multiple models formatted with shape                                 (n_models, n_samples, n_labels). Provided by NeuralNetwork.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                                 input_interface.</p> required <code>out_path</code> <code>str</code> <p>Path to directory in which plotted figures are stored.</p> required <code>model_names</code> <code>list of str</code> <p>List of names for corresponding models which are for visualization. If not provided (<code>None</code>                                 provided), model index of <code>pred_list</code> will be used.</p> <code>None</code> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for evaluation. Provided by                                 input_interface.                                 If not provided (<code>None</code> provided), class indices will be used.</p> <code>None</code> <code>multi_label</code> <code>bool</code> <p>Option, whether task is multi-label based (has impact on evaluation).</p> <code>False</code> <code>metrics_threshold</code> <code>float</code> <p>Only required if 'multi_label==True`. Threshold value if prediction is positive.                                 Used in metric computation for CSV and bar plot.</p> <code>0.5</code> <code>macro_average_classes</code> <code>bool</code> <p>Option, whether classes should be macro-averaged in order to increase visualization overview.</p> <code>False</code> <code>suffix</code> <code>str</code> <p>Special suffix to add in the created figure filename.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df_merged</code> <code>pandas.DataFrame</code> <p>Dataframe containing the merged metrics of all models.</p> <code>df_gain</code> <code>pandas.DataFrame</code> <p>Dataframe containing performance gain compared to first model.</p> Source code in <code>aucmedi/evaluation/comparison.py</code> <pre><code>def evaluate_comparison(pred_list,\n                        labels,\n                        out_path,\n                        model_names=None,\n                        class_names=None,\n                        multi_label=False,\n                        metrics_threshold=0.5,\n                        macro_average_classes=False,\n                        suffix=None):\n    \"\"\" Function for performance comparison evaluation based on predictions from multiple models.\n\n    ???+ example\n        ```python\n        # Import libraries\n        from aucmedi import *\n        from aucmedi.evaluation import *\n        from aucmedi.ensemble import *\n\n        # Load data\n        ds = input_interface(interface=\"csv\",                       # Interface type\n                             path_imagedir=\"dataset/images/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Initialize model\n        model_a = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n\n        # Initialize Bagging object for 3-fold cross-validation\n        el = Bagging(model_a, k_fold=3)\n\n        # Do some predictions via Bagging (return also all prediction ensembles)\n        datagen_test = DataGenerator(samples, \"dataset/images/\", labels=None,\n                                     resize=model.meta_input, standardize_mode=model.meta_standardize)\n        pred_merged, pred_ensemble = el.predict(datagen_test, return_ensemble=True)\n\n        # Pass prediction ensemble to evaluation function\n        evaluate_comparison(pred_ensemble, class_ohe, out_path=\"./\", class_names=class_names)\n\n\n        # Do some predictions with manually initialized models\n        model_b = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n        model_c = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.MobileNetV2\")\n\n        pred_a = model_a.predict(datagen_test)\n        pred_b = model_b.predict(datagen_test)\n        pred_c = model_c.predict(datagen_test)\n\n        pred_ensemble = [pred_a, pred_b, pred_c]\n\n        # Pass prediction ensemble to evaluation function\n        evaluate_comparison(pred_ensemble, class_ohe, out_path=\"./\", class_names=class_names)\n        ```\n\n    Created files in directory of `out_path`:\n\n    - \"plot.comparison.beside.png\"\n    - \"plot.comparison.gain.png\"\n\n    ???+ info \"Preview for Bar Plot\"\n        ![Evaluation_Comparison_Beside](../../images/evaluation.plot.comparison.beside.png)\n\n        Predictions based on [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/) with\n        macro-averaged class-wise metrics.\n\n    ???+ info \"Preview for Confusion Matrix\"\n        ![Evaluation_Comparison_Gain](../../images/evaluation.plot.comparison.gain.png)\n\n        Predictions based on [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/).\n\n    Args:\n        pred_list (list of numpy.ndarray):  A list of NumPy arrays containing predictions from multiple models formatted with shape\n                                            (n_models, n_samples, n_labels). Provided by [NeuralNetwork][aucmedi.neural_network.model].\n        labels (numpy.ndarray):             Classification list with One-Hot Encoding. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n        out_path (str):                     Path to directory in which plotted figures are stored.\n        model_names (list of str):          List of names for corresponding models which are for visualization. If not provided (`None`\n                                            provided), model index of `pred_list` will be used.\n        class_names (list of str):          List of names for corresponding classes. Used for evaluation. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n                                            If not provided (`None` provided), class indices will be used.\n        multi_label (bool):                 Option, whether task is multi-label based (has impact on evaluation).\n        metrics_threshold (float):          Only required if 'multi_label==True`. Threshold value if prediction is positive.\n                                            Used in metric computation for CSV and bar plot.\n        macro_average_classes (bool):       Option, whether classes should be macro-averaged in order to increase visualization overview.\n        suffix (str):                       Special suffix to add in the created figure filename.\n\n    Returns:\n        df_merged (pandas.DataFrame):       Dataframe containing the merged metrics of all models.\n        df_gain (pandas.DataFrame):         Dataframe containing performance gain compared to first model.\n    \"\"\"\n    # Identify number of labels\n    n_labels = labels.shape[-1]\n    # Identify prediction threshold\n    if multi_label : threshold = metrics_threshold\n    else : threshold = None\n\n    # Compute metric dataframe for each mode\n    df_list = []\n    for m in range(0, len(pred_list)):\n        metrics = compute_metrics(pred_list[m], labels, n_labels, threshold)\n\n        # Rename class association in metrics dataframe\n        class_mapping = {}\n        if class_names is not None:\n            for c in range(len(class_names)):\n                class_mapping[c] = class_names[c]\n            metrics[\"class\"].replace(class_mapping, inplace=True)\n        if class_names is None:\n            metrics[\"class\"] = pd.Categorical(metrics[\"class\"])\n\n        # Assign model name to dataframe\n        if model_names is not None : metrics[\"model\"] = model_names[m]\n        else : metrics[\"model\"] = \"model_\" + str(m)\n\n        # Optional: Macro average classes\n        if macro_average_classes:\n            metrics_avg = metrics.groupby([\"metric\", \"model\"])[[\"score\"]].mean()\n            metrics = metrics_avg.reset_index()\n\n        # Append to dataframe list\n        df_list.append(metrics)\n    # Combine dataframes\n    df_merged = pd.concat(df_list, axis=0, ignore_index=True)\n\n    # Generate comparison beside plot\n    evalby_beside(df_merged, out_path, suffix)\n\n    # Generate comparison gain plot\n    df_gain = evalby_gain(df_merged, out_path, suffix)\n\n    # Return combined and gain dataframe\n    return df_merged, df_gain\n</code></pre>"},{"location":"reference/evaluation/dataset/","title":"Dataset","text":""},{"location":"reference/evaluation/dataset/#aucmedi.evaluation.dataset.evaluate_dataset","title":"<code>evaluate_dataset(samples, labels, out_path, class_names=None, show=False, plot_barplot=False, plot_heatmap=False, suffix=None)</code>","text":"<p>Function for dataset evaluation (descriptive statistics).</p> Example <pre><code># Import libraries\nfrom aucmedi import *\nfrom aucmedi.evaluation import *\n\n# Peak data information via the first pillar of AUCMEDI\nds = input_interface(interface=\"csv\",                       # Interface type\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Pass information to the evaluation function\nevaluate_dataset(samples, class_ohe, out_path=\"./\", class_names=class_names)\n</code></pre> <p>Created files in directory of <code>out_path</code>:</p> <ul> <li>\"plot.dataset.barplot.png\"</li> <li>\"plot.dataset.heatmap.png\"</li> </ul> Preview for Bar Plot <p></p> <p>Based on dataset: ISIC 2019 Challenge.</p> Preview for Heatmap <p></p> <p>Based on first 50 samples from dataset: ISIC 2019 Challenge.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of str</code> <p>List of sample/index encoded as Strings. Provided by                                 input_interface.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                                 input_interface.</p> required <code>out_path</code> <code>str</code> <p>Path to directory in which plotted figures are stored.</p> required <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for evaluation. Provided by                                 input_interface.                                 If not provided (<code>None</code> provided), class indices will be used.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Option, whether to also display the generated charts.</p> <code>False</code> <code>plot_barplot</code> <code>bool</code> <p>Option, whether to generate a bar plot of class distribution.</p> <code>False</code> <code>plot_heatmap</code> <code>bool</code> <p>Option, whether to generate a heatmap of class overview. Only recommended for subsets of ~50 samples.</p> <code>False</code> <code>suffix</code> <code>str</code> <p>Special suffix to add in the created figure filename.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df_cf</code> <code>pandas.DataFrame</code> <p>Dataframe containing the class distribution of the dataset.</p> Source code in <code>aucmedi/evaluation/dataset.py</code> <pre><code>def evaluate_dataset(samples,\n                     labels,\n                     out_path,\n                     class_names=None,\n                     show=False,\n                     plot_barplot=False,\n                     plot_heatmap=False,\n                     suffix=None):\n    \"\"\" Function for dataset evaluation (descriptive statistics).\n\n    ???+ example\n        ```python\n        # Import libraries\n        from aucmedi import *\n        from aucmedi.evaluation import *\n\n        # Peak data information via the first pillar of AUCMEDI\n        ds = input_interface(interface=\"csv\",                       # Interface type\n                             path_imagedir=\"dataset/images/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Pass information to the evaluation function\n        evaluate_dataset(samples, class_ohe, out_path=\"./\", class_names=class_names)\n        ```\n\n    Created files in directory of `out_path`:\n\n    - \"plot.dataset.barplot.png\"\n    - \"plot.dataset.heatmap.png\"\n\n    ???+ info \"Preview for Bar Plot\"\n        ![Evaluation_Dataset_Barplot](../../images/evaluation.plot.dataset.barplot.png)\n\n        Based on dataset: [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/).\n\n    ???+ info \"Preview for Heatmap\"\n        ![Evaluation_Dataset_Heatmap](../../images/evaluation.plot.dataset.heatmap.png)\n\n        Based on first 50 samples from dataset: [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/).\n\n    Args:\n        samples (list of str):              List of sample/index encoded as Strings. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n        labels (numpy.ndarray):             Classification list with One-Hot Encoding. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n        out_path (str):                     Path to directory in which plotted figures are stored.\n        class_names (list of str):          List of names for corresponding classes. Used for evaluation. Provided by\n                                            [input_interface][aucmedi.data_processing.io_data.input_interface].\n                                            If not provided (`None` provided), class indices will be used.\n        show (bool):                        Option, whether to also display the generated charts.\n        plot_barplot (bool):                Option, whether to generate a bar plot of class distribution.\n        plot_heatmap (bool):                Option, whether to generate a heatmap of class overview. Only recommended for subsets of ~50 samples.\n        suffix (str):                       Special suffix to add in the created figure filename.\n\n    Returns:\n        df_cf (pandas.DataFrame):           Dataframe containing the class distribution of the dataset.\n    \"\"\"\n\n    # Generate barplot\n    df_cf = evalby_barplot(labels, out_path, class_names, plot_barplot, show,\n                           suffix)\n\n    # Generate heatmap\n    if plot_heatmap:\n        evalby_heatmap(samples, labels, out_path, class_names, show, suffix)\n\n    # Return table with class distribution\n    return df_cf\n</code></pre>"},{"location":"reference/evaluation/fitting/","title":"Fitting","text":""},{"location":"reference/evaluation/fitting/#aucmedi.evaluation.fitting.evaluate_fitting","title":"<code>evaluate_fitting(train_history, out_path, monitor=['loss'], prefix_split='.', suffix=None, show=False)</code>","text":"<p>Function for automatic plot generation providing a training history dictionary.</p> <p>Preview</p> <p></p> <p>Created filename in directory of <code>out_path</code>:</p> <ul> <li>without suffix \"plot.fitting_course.png\"</li> <li>with suffix \"plot.fitting_course.SUFFIX.png\"</li> </ul> Example <pre><code># Initialize and train a model\nmodel = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\nhistory = model.train(datagen_train, datagen_validation, epochs=100)\n\n# Pass history dict to evaluation function\nevaluate_fitting(history, out_path=\"./\")\n\n# Figure will be created at: \"./plot.fitting_course.png\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>train_history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> required <code>out_path</code> <code>str</code> <p>Path to directory in which plotted figure is stored.</p> required <code>monitor</code> <code>list of str</code> <p>List of metrics which should be visualized in the fitting plot.</p> <code>['loss']</code> <code>prefix_split</code> <code>str</code> <p>Split prefix for keys in the history dictionary. Used for Bagging and Stacking.</p> <code>'.'</code> <code>suffix</code> <code>str</code> <p>Special suffix to add in the created figure filename.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Option, whether to also display the generated chart.</p> <code>False</code> Source code in <code>aucmedi/evaluation/fitting.py</code> <pre><code>def evaluate_fitting(train_history,\n                     out_path,\n                     monitor=[\"loss\"],\n                     prefix_split=\".\",\n                     suffix=None,\n                     show=False\n                     ):\n    \"\"\" Function for automatic plot generation providing a training history dictionary.\n\n    !!! info \"Preview\"\n        ![Evaluation_Fitting](../../images/evaluation.plot.fitting_course.png)\n\n    Created filename in directory of `out_path`:\n\n    - without suffix \"plot.fitting_course.png\"\n    - with suffix \"plot.fitting_course.SUFFIX.png\"\n\n    ???+ example\n        ```python\n        # Initialize and train a model\n        model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n        history = model.train(datagen_train, datagen_validation, epochs=100)\n\n        # Pass history dict to evaluation function\n        evaluate_fitting(history, out_path=\"./\")\n\n        # Figure will be created at: \"./plot.fitting_course.png\"\n        ```\n\n    Args:\n        train_history (dict):       A history dictionary from a Keras history object which contains several logs.\n        out_path (str):             Path to directory in which plotted figure is stored.\n        monitor (list of str):      List of metrics which should be visualized in the fitting plot.\n        prefix_split (str):         Split prefix for keys in the history dictionary. Used for Bagging and Stacking.\n        suffix (str):               Special suffix to add in the created figure filename.\n        show (bool):                Option, whether to also display the generated chart.\n    \"\"\"\n    # Convert to pandas dataframe\n    hist_prepared = dict([ (k,pd.Series(v)) for k,v in train_history.items() ])\n    dt = pd.DataFrame.from_dict(hist_prepared, orient=\"columns\")\n\n    # Identify all selected columns\n    selected_cols = []\n    for key in train_history:\n        for m in monitor:\n            if m in key:\n                selected_cols.append(key)\n                break\n\n    # Add epoch column\n    dt[\"epoch\"] = dt.index + 1\n    # Melt dataframe\n    dt_melted = dt.melt(id_vars=[\"epoch\"],\n                        value_vars=selected_cols,\n                        var_name=\"metric\",\n                        value_name=\"score\")\n\n    # Handle special prefix tags (if split-able by '.')\n    if prefix_split is not None:\n        for c in selected_cols:\n            valid_split = True\n            if prefix_split not in c:\n                valid_split = False\n                break\n        if valid_split:\n            dt_melted[[\"prefix\", \"metric\"]] = dt_melted[\"metric\"].str.split(\".\",\n                                                        expand=True)\n\n    # Remove NaN tags\n    dt_melted = dt_melted.dropna(axis=0)\n\n    # Preprocess transfer learning tag\n    if dt_melted[\"metric\"].str.startswith(\"tl_\").any():\n        # filter prefix groups\n        filter = dt_melted[dt_melted[\"metric\"].str.startswith(\"ft_\")]\n\n        # if prefix available -&gt; add epochs for each prefix group\n        if valid_split:\n            # identify number of epochs for each prefix\n            filter_tl = dt_melted[dt_melted[\"metric\"].str.startswith(\"tl_\")]\n            tl_epochs = filter_tl.groupby([\"prefix\"])[\"epoch\"].max()\n            # compute fine tune epoch update\n            group_repeats = filter.groupby([\"prefix\"]).size()\n            if group_repeats.empty : ft_update = 0\n            else : ft_update = tl_epochs.repeat(group_repeats).to_numpy()\n        # if no prefix available -&gt; add epochs to all ft phases\n        else:\n            # identify number of epochs global\n            filter_tl = dt_melted[dt_melted[\"metric\"].str.startswith(\"tl_\")]\n            tl_epochs = filter_tl[\"epoch\"].max()\n\n            # compute fine tune epoch update\n            ft_update = tl_epochs\n\n        # update number of epochs\n        dt_melted.loc[dt_melted[\"metric\"].str.startswith(\"ft_\"), \"epoch\"] =\\\n            filter[\"epoch\"].to_numpy() + ft_update\n    else : tl_epochs = None\n\n    # Remove preprocessed transfer learning tag from metric column\n    dt_melted[\"metric\"] = dt_melted[\"metric\"].apply(remove_tag, tag=\"tl_\")\n    dt_melted[\"metric\"] = dt_melted[\"metric\"].apply(remove_tag, tag=\"ft_\")\n\n    # Preprocess validation tag\n    dt_melted[\"subset\"] = np.where(dt_melted[\"metric\"].str.startswith(\"val_\"),\n                                   \"validation\", \"training\")\n    dt_melted[\"metric\"] = dt_melted[\"metric\"].apply(remove_tag, tag=\"val_\")\n\n    # Plot results via plotnine\n    fig = (ggplot(dt_melted, aes(\"epoch\", \"score\", color=\"subset\"))\n               + geom_line(size=1)\n               + ggtitle(\"Fitting Curve during Training Process\")\n               + xlab(\"Epoch\")\n               + ylab(\"Score\")\n               + scale_colour_discrete(name=\"Subset\")\n               + theme_bw()\n               + theme(subplots_adjust={'wspace':0.2}))\n\n    if prefix_split is not None and valid_split:\n        fig += facet_grid(\"prefix ~ metric\")\n    else : fig += facet_wrap(\"metric\", scales=\"free_y\")\n\n    if tl_epochs is not None and valid_split:\n        tle_df = tl_epochs.to_frame().reset_index()\n        fig += geom_vline(tle_df, aes(xintercept=\"epoch\"))\n    elif tl_epochs is not None and not valid_split:\n        fig += geom_vline(xintercept=tl_epochs)\n\n    # Store figure to disk\n    filename = \"plot.fitting_course\"\n    if suffix is not None : filename += \".\" + str(suffix)\n    filename += \".png\"\n    fig.save(filename=filename,\n             path=out_path, dpi=200, limitsize=False)\n\n    if show : print(fig)\n</code></pre>"},{"location":"reference/evaluation/metrics/","title":"Metrics","text":""},{"location":"reference/evaluation/metrics/#aucmedi.evaluation.metrics.compute_confusion_matrix","title":"<code>compute_confusion_matrix(preds, labels, n_labels)</code>","text":"<p>Function for computing a confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by                             NeuralNetwork.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                             input_interface.</p> required <code>n_labels</code> <code>int</code> <p>Number of classes. Provided by input_interface.</p> required <p>Returns:</p> Name Type Description <code>rawcm</code> <code>numpy.ndarray</code> <p>NumPy matrix with shape (n_labels, n_labels).</p> Source code in <code>aucmedi/evaluation/metrics.py</code> <pre><code>def compute_confusion_matrix(preds, labels, n_labels):\n    \"\"\" Function for computing a confusion matrix.\n\n    Args:\n        preds (numpy.ndarray):          A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by\n                                        [NeuralNetwork][aucmedi.neural_network.model].\n        labels (numpy.ndarray):         Classification list with One-Hot Encoding. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n        n_labels (int):                 Number of classes. Provided by [input_interface][aucmedi.data_processing.io_data.input_interface].\n\n    Returns:\n        rawcm (numpy.ndarray):          NumPy matrix with shape (n_labels, n_labels).\n    \"\"\"\n    preds_argmax = np.argmax(preds, axis=-1)\n    labels_argmax = np.argmax(labels, axis=-1)\n    rawcm = np.zeros((n_labels, n_labels))\n    for i in range(0, labels.shape[0]):\n        rawcm[labels_argmax[i]][preds_argmax[i]] += 1\n    return rawcm\n</code></pre>"},{"location":"reference/evaluation/metrics/#aucmedi.evaluation.metrics.compute_metrics","title":"<code>compute_metrics(preds, labels, n_labels, threshold=None)</code>","text":"<p>Function for computing various classification metrics.</p> <p>Computed Metrics</p> <p>F1, Accuracy, Sensitivity, Specificity, AUROC (AUC), Precision, FPR, FNR, FDR, TruePositives, TrueNegatives, FalsePositives, FalseNegatives</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by                             NeuralNetwork.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                             input_interface.</p> required <code>n_labels</code> <code>int</code> <p>Number of classes. Provided by input_interface.</p> required <code>threshold</code> <code>float</code> <p>Only required for multi_label data. Threshold value if prediction is positive.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>metrics</code> <code>pandas.DataFrame</code> <p>Dataframe containing all computed metrics (except ROC).</p> Source code in <code>aucmedi/evaluation/metrics.py</code> <pre><code>def compute_metrics(preds, labels, n_labels, threshold=None):\n    \"\"\" Function for computing various classification metrics.\n\n    !!! info \"Computed Metrics\"\n        F1, Accuracy, Sensitivity, Specificity, AUROC (AUC), Precision, FPR, FNR,\n        FDR, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n\n    Args:\n        preds (numpy.ndarray):          A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by\n                                        [NeuralNetwork][aucmedi.neural_network.model].\n        labels (numpy.ndarray):         Classification list with One-Hot Encoding. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n        n_labels (int):                 Number of classes. Provided by [input_interface][aucmedi.data_processing.io_data.input_interface].\n        threshold (float):              Only required for multi_label data. Threshold value if prediction is positive.\n\n    Returns:\n        metrics (pandas.DataFrame):     Dataframe containing all computed metrics (except ROC).\n    \"\"\"\n    df_list = []\n    for c in range(0, n_labels):\n        # Initialize variables\n        data_dict = {}\n\n        # Identify truth and prediction for class c\n        truth = labels[:, c]\n        if threshold is None:\n            pred_argmax = np.argmax(preds, axis=-1)\n            pred = (pred_argmax == c).astype(np.int8)\n        else:\n            pred = np.where(preds[:, c] &gt;= threshold, 1, 0)\n        # Obtain prediction confidence (probability)\n        pred_prob = preds[:, c]\n\n        # Compute the confusion matrix\n        tp, tn, fp, fn = compute_CM(truth, pred)\n        data_dict[\"TP\"] = tp\n        data_dict[\"TN\"] = tn\n        data_dict[\"FP\"] = fp\n        data_dict[\"FN\"] = fn\n\n        # Compute several metrics based on confusion matrix\n        data_dict[\"Sensitivity\"] = np.divide(tp, tp+fn)\n        data_dict[\"Specificity\"] = np.divide(tn, tn+fp)\n        data_dict[\"Precision\"] = np.divide(tp, tp+fp)\n        data_dict[\"FPR\"] = np.divide(fp, fp+tn)\n        data_dict[\"FNR\"] = np.divide(fn, fn+tp)\n        data_dict[\"FDR\"] = np.divide(fp, fp+tp)\n        data_dict[\"Accuracy\"] = np.divide(tp+tn, tp+tn+fp+fn)\n        data_dict[\"F1\"] = np.divide(2*tp, 2*tp+fp+fn)\n\n        # Compute area under the ROC curve\n        try:\n            data_dict[\"AUC\"] = roc_auc_score(truth, pred_prob)\n        except:\n            print(\"ROC AUC score is not defined.\")\n\n        # Parse metrics to dataframe\n        df = pd.DataFrame.from_dict(data_dict, orient=\"index\",\n                                    columns=[\"score\"])\n        df = df.reset_index()\n        df.rename(columns={\"index\": \"metric\"}, inplace=True)\n        df[\"class\"] = c\n\n        # Append dataframe to list\n        df_list.append(df)\n\n    # Combine dataframes\n    df_final = pd.concat(df_list, axis=0, ignore_index=True)\n    # Return final dataframe\n    return df_final\n</code></pre>"},{"location":"reference/evaluation/metrics/#aucmedi.evaluation.metrics.compute_roc","title":"<code>compute_roc(preds, labels, n_labels)</code>","text":"<p>Function for computing the data data of a ROC curve (FPR and TPR).</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by                             NeuralNetwork.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                             input_interface.</p> required <code>n_labels</code> <code>int</code> <p>Number of classes. Provided by input_interface.</p> required <p>Returns:</p> Name Type Description <code>fpr_list</code> <code>list of list</code> <p>List containing a list of false positive rate points for each class. Shape: (n_labels, tpr_coords).</p> <code>tpr_list</code> <code>list of list</code> <p>List containing a list of true positive rate points for each class. Shape: (n_labels, fpr_coords).</p> Source code in <code>aucmedi/evaluation/metrics.py</code> <pre><code>def compute_roc(preds, labels, n_labels):\n    \"\"\" Function for computing the data data of a ROC curve (FPR and TPR).\n\n    Args:\n        preds (numpy.ndarray):          A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by\n                                        [NeuralNetwork][aucmedi.neural_network.model].\n        labels (numpy.ndarray):         Classification list with One-Hot Encoding. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n        n_labels (int):                 Number of classes. Provided by [input_interface][aucmedi.data_processing.io_data.input_interface].\n    Returns:\n        fpr_list (list of list):        List containing a list of false positive rate points for each class. Shape: (n_labels, tpr_coords).\n        tpr_list (list of list):        List containing a list of true positive rate points for each class. Shape: (n_labels, fpr_coords).\n    \"\"\"\n    fpr_list = []\n    tpr_list = []\n    for i in range(0, n_labels):\n        truth_class = labels[:, i].astype(int)\n        pdprob_class = preds[:, i]\n        fpr, tpr, _ = roc_curve(truth_class, pdprob_class)\n        fpr_list.append(fpr)\n        tpr_list.append(tpr)\n    return fpr_list, tpr_list\n</code></pre>"},{"location":"reference/evaluation/performance/","title":"Performance","text":""},{"location":"reference/evaluation/performance/#aucmedi.evaluation.performance.evaluate_performance","title":"<code>evaluate_performance(preds, labels, out_path, show=False, class_names=None, multi_label=False, metrics_threshold=0.5, suffix=None, store_csv=True, plot_barplot=True, plot_confusion_matrix=True, plot_roc_curve=True)</code>","text":"<p>Function for automatic performance evaluation based on model predictions.</p> Example <pre><code># Import libraries\nfrom aucmedi import *\nfrom aucmedi.evaluation import *\n\n# Load data\nds = input_interface(interface=\"csv\",                       # Interface type\n                     path_imagedir=\"dataset/images/\",\n                     path_data=\"dataset/annotations.csv\",\n                     ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n(samples, class_ohe, nclasses, class_names, image_format) = ds\n\n# Initialize model\nmodel = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n\n# Do some predictions\ndatagen_test = DataGenerator(samples, \"dataset/images/\", labels=None,\n                             resize=model.meta_input, standardize_mode=model.meta_standardize)\npreds = model.predict(datagen_test)\n\n# Pass predictions to evaluation function\nevaluate_performance(preds, class_ohe, out_path=\"./\", class_names=class_names)\n</code></pre> <p>Created files in directory of <code>out_path</code>:</p> <ul> <li>with <code>store_csv</code>: \"metrics.performance.csv\"</li> <li>with <code>plot_barplot</code>: \"plot.performance.barplot.png\"</li> <li>with <code>plot_confusion_matrix</code>: \"plot.performance.confusion_matrix.png\"</li> <li>with <code>plot_roc_curve</code>: \"plot.performance.roc.png\"</li> </ul> Preview for Bar Plot <p></p> <p>Predictions based on ISIC 2019 Challenge utilizing a DenseNet121.</p> Preview for Confusion Matrix <p></p> <p>Predictions based on ISIC 2019 Challenge utilizing a DenseNet121.</p> Preview for ROC Curve <p></p> <p>Predictions based on ISIC 2019 Challenge utilizing a DenseNet121.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by                             NeuralNetwork.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>Classification list with One-Hot Encoding. Provided by                             input_interface.</p> required <code>out_path</code> <code>str</code> <p>Path to directory in which plotted figures are stored.</p> required <code>show</code> <code>bool</code> <p>Option, whether to also display the generated charts.</p> <code>False</code> <code>class_names</code> <code>list of str</code> <p>List of names for corresponding classes. Used for evaluation. Provided by                             input_interface.                             If not provided (<code>None</code> provided), class indices will be used.</p> <code>None</code> <code>multi_label</code> <code>bool</code> <p>Option, whether task is multi-label based (has impact on evaluation).</p> <code>False</code> <code>metrics_threshold</code> <code>float</code> <p>Only required if 'multi_label==True`. Threshold value if prediction is positive.                             Used in metric computation for CSV and bar plot.</p> <code>0.5</code> <code>suffix</code> <code>str</code> <p>Special suffix to add in the created figure filename.</p> <code>None</code> <code>store_csv</code> <code>bool</code> <p>Option, whether to generate a CSV file containing various metrics.</p> <code>True</code> <code>plot_barplot</code> <code>bool</code> <p>Option, whether to generate a bar plot of various metrics.</p> <code>True</code> <code>plot_confusion_matrix</code> <code>bool</code> <p>Option, whether to generate a confusion matrix plot.</p> <code>True</code> <code>plot_roc_curve</code> <code>bool</code> <p>Option, whether to generate a ROC curve plot.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics</code> <code>pandas.DataFrame</code> <p>Dataframe containing all computed metrics (except ROC).</p> Source code in <code>aucmedi/evaluation/performance.py</code> <pre><code>def evaluate_performance(preds,\n                         labels,\n                         out_path,\n                         show=False,\n                         class_names=None,\n                         multi_label=False,\n                         metrics_threshold=0.5,\n                         suffix=None,\n                         store_csv=True,\n                         plot_barplot=True,\n                         plot_confusion_matrix=True,\n                         plot_roc_curve=True):\n    \"\"\" Function for automatic performance evaluation based on model predictions.\n\n    ???+ example\n        ```python\n        # Import libraries\n        from aucmedi import *\n        from aucmedi.evaluation import *\n\n        # Load data\n        ds = input_interface(interface=\"csv\",                       # Interface type\n                             path_imagedir=\"dataset/images/\",\n                             path_data=\"dataset/annotations.csv\",\n                             ohe=False, col_sample=\"ID\", col_class=\"diagnosis\")\n        (samples, class_ohe, nclasses, class_names, image_format) = ds\n\n        # Initialize model\n        model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n\n        # Do some predictions\n        datagen_test = DataGenerator(samples, \"dataset/images/\", labels=None,\n                                     resize=model.meta_input, standardize_mode=model.meta_standardize)\n        preds = model.predict(datagen_test)\n\n        # Pass predictions to evaluation function\n        evaluate_performance(preds, class_ohe, out_path=\"./\", class_names=class_names)\n        ```\n\n    Created files in directory of `out_path`:\n\n    - with `store_csv`: \"metrics.performance.csv\"\n    - with `plot_barplot`: \"plot.performance.barplot.png\"\n    - with `plot_confusion_matrix`: \"plot.performance.confusion_matrix.png\"\n    - with `plot_roc_curve`: \"plot.performance.roc.png\"\n\n    ???+ info \"Preview for Bar Plot\"\n        ![Evaluation_Performance_Barplot](../../images/evaluation.plot.performance.barplot.png)\n\n        Predictions based on [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/)\n        utilizing a DenseNet121.\n\n    ???+ info \"Preview for Confusion Matrix\"\n        ![Evaluation_Performance_ConfusionMatrix](../../images/evaluation.plot.performance.confusion_matrix.png)\n\n        Predictions based on [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/)\n        utilizing a DenseNet121.\n\n    ???+ info \"Preview for ROC Curve\"\n        ![Evaluation_Performance_ROCcurve](../../images/evaluation.plot.performance.roc.png)\n\n        Predictions based on [ISIC 2019 Challenge](https://challenge.isic-archive.com/landing/2019/)\n        utilizing a DenseNet121.\n\n    Args:\n        preds (numpy.ndarray):          A NumPy array of predictions formatted with shape (n_samples, n_labels). Provided by\n                                        [NeuralNetwork][aucmedi.neural_network.model].\n        labels (numpy.ndarray):         Classification list with One-Hot Encoding. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n        out_path (str):                 Path to directory in which plotted figures are stored.\n        show (bool):                    Option, whether to also display the generated charts.\n        class_names (list of str):      List of names for corresponding classes. Used for evaluation. Provided by\n                                        [input_interface][aucmedi.data_processing.io_data.input_interface].\n                                        If not provided (`None` provided), class indices will be used.\n        multi_label (bool):             Option, whether task is multi-label based (has impact on evaluation).\n        metrics_threshold (float):      Only required if 'multi_label==True`. Threshold value if prediction is positive.\n                                        Used in metric computation for CSV and bar plot.\n        suffix (str):                   Special suffix to add in the created figure filename.\n        store_csv (bool):               Option, whether to generate a CSV file containing various metrics.\n        plot_barplot (bool):            Option, whether to generate a bar plot of various metrics.\n        plot_confusion_matrix (bool):   Option, whether to generate a confusion matrix plot.\n        plot_roc_curve (bool):          Option, whether to generate a ROC curve plot.\n\n    Returns:\n        metrics (pandas.DataFrame):     Dataframe containing all computed metrics (except ROC).\n    \"\"\"\n    # Identify number of labels\n    n_labels = labels.shape[-1]\n    # Identify prediction threshold\n    if multi_label : threshold = metrics_threshold\n    else : threshold = None\n\n    # Compute metrics\n    metrics = compute_metrics(preds, labels, n_labels, threshold)\n    cm = compute_confusion_matrix(preds, labels, n_labels)\n    fpr_list, tpr_list = compute_roc(preds, labels, n_labels)\n\n    # Rename columns in metrics dataframe\n    class_mapping = {}\n    if class_names is not None:\n        for c in range(len(class_names)):\n            class_mapping[c] = class_names[c]\n        metrics[\"class\"].replace(class_mapping, inplace=True)\n    if class_names is None : metrics[\"class\"] = pd.Categorical(metrics[\"class\"])\n\n    # Store metrics to CSV\n    if store_csv:\n        evalby_csv(metrics, out_path, class_names, suffix=suffix)\n\n    # Generate bar plot\n    if plot_barplot:\n        evalby_barplot(metrics, out_path, class_names, show=show, suffix=suffix)\n\n    # Generate confusion matrix plot\n    if plot_confusion_matrix and not multi_label:\n        evalby_confusion_matrix(cm, out_path, class_names, show=show, suffix=suffix)\n\n    # Generate ROC curve\n    if plot_roc_curve:\n        evalby_rocplot(fpr_list, tpr_list, out_path, class_names, show=show, suffix=suffix)\n\n    # Return metrics\n    return metrics\n</code></pre>"},{"location":"reference/neural_network/","title":"Neural network","text":"<p>A pillar of any AUCMEDI pipeline: aucmedi.neural_network.model.NeuralNetwork</p> <p>The Neural Network class is a powerful interface to the deep neural network world in AUCMEDI.</p> Pillars of AUCMEDI <ul> <li>aucmedi.data_processing.io_data.input_interface</li> <li>aucmedi.data_processing.data_generator.DataGenerator</li> <li>aucmedi.neural_network.model.NeuralNetwork</li> </ul> <p>With an initialized Neural Network instance, it is possible to run training and predictions.</p> Example <pre><code># Import\nfrom aucmedi import *\n\n# Initialize model\nmodel = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n\n# Do some training\ndatagen_train = DataGenerator(samples[:100], \"images_dir/\", labels=class_ohe[:100],\n                              resize=model.meta_input, standardize_mode=model.meta_standardize)\nmodel.train(datagen_train, epochs=50)\n\n# Do some predictions\ndatagen_test = DataGenerator(samples[100:150], \"images_dir/\", labels=None,\n                             resize=model.meta_input, standardize_mode=model.meta_standardize)\npreds = model.predict(datagen_test)\n</code></pre>"},{"location":"reference/neural_network/loss_functions/","title":"Loss functions","text":""},{"location":"reference/neural_network/loss_functions/#aucmedi.neural_network.loss_functions.binary_focal_loss","title":"<code>binary_focal_loss(alpha=0.25, gamma=2.0)</code>","text":"<p>Binary form of focal loss computation.</p> <p>FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t) where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.</p> Example <pre><code>from aucmedi.neural_network.loss_functions import *\nmy_loss = binary_focal_loss(alpha=0.75)\n\nmodel = NeuralNetwork(n_labels=1, channels=3, loss=my_loss)\n</code></pre> Reference - Implementation <p>Author: Umberto Griffo  GitHub: https://github.com/umbertogriffo  Source: https://github.com/umbertogriffo/focal-loss-keras </p> Reference - Publication <p>Focal Loss for Dense Object Detection (Aug 2017)  Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r  https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Class weight for positive class.</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>Tunable focusing parameter (\u03b3 \u2265 0).</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>Loss Function</code> <p>A TensorFlow compatible loss function. This object can be                                 passed to the NeuralNetwork <code>loss</code> parameter.</p> Source code in <code>aucmedi/neural_network/loss_functions.py</code> <pre><code>def binary_focal_loss(alpha=0.25, gamma=2.0):\n    \"\"\" Binary form of focal loss computation.\n\n    FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n    where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n\n    ??? example\n        ```python\n        from aucmedi.neural_network.loss_functions import *\n        my_loss = binary_focal_loss(alpha=0.75)\n\n        model = NeuralNetwork(n_labels=1, channels=3, loss=my_loss)\n        ```\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Umberto Griffo &lt;br&gt;\n        GitHub: [https://github.com/umbertogriffo](https://github.com/umbertogriffo) &lt;br&gt;\n        Source: [https://github.com/umbertogriffo/focal-loss-keras](https://github.com/umbertogriffo/focal-loss-keras) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Focal Loss for Dense Object Detection (Aug 2017) &lt;br&gt;\n        Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r &lt;br&gt;\n        [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)\n\n    Args:\n        alpha (float):      Class weight for positive class.\n        gamma (float):      Tunable focusing parameter (\u03b3 \u2265 0).\n\n    Returns:\n        loss (Loss Function):               A TensorFlow compatible loss function. This object can be\n                                            passed to the [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork] `loss` parameter.\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        # y_pred = y_pred + epsilon\n        # Clip the prediciton value\n        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n        # Calculate p_t\n        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n        # Calculate alpha_t\n        alpha_factor = K.ones_like(y_true) * alpha\n        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n        # Calculate cross entropy\n        cross_entropy = -K.log(p_t)\n        weight = alpha_t * K.pow((1 - p_t), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.mean(K.sum(loss, axis=1))\n        return loss\n\n    return binary_focal_loss_fixed\n</code></pre>"},{"location":"reference/neural_network/loss_functions/#aucmedi.neural_network.loss_functions.categorical_focal_loss","title":"<code>categorical_focal_loss(alpha, gamma=2.0)</code>","text":"<p>Softmax version of focal loss.</p> <p>When there is a skew between different categories/labels in your data set, you can try to apply this function as a loss.</p> <pre><code>       m\n  FL = \u2211  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n      c=1\n\n  where m = number of classes, c = class and o = observation\n</code></pre> <p>The <code>class_weights_list</code> obtained from compute_class_weights can be provided as parameter <code>alpha</code>.</p> Example <pre><code># Compute class weights\nfrom aucmedi.utils.class_weights import compute_class_weights\ncw_loss, cw_fit = compute_class_weights(class_ohe)\n\nfrom aucmedi.neural_network.loss_functions import *\nmy_loss = categorical_focal_loss(alpha=cw_loss)\n\nmodel = NeuralNetwork(n_labels=6, channels=3, loss=my_loss)\n</code></pre> Reference - Implementation <p>Author: Umberto Griffo  GitHub: https://github.com/umbertogriffo  Source: https://github.com/umbertogriffo/focal-loss-keras </p> Reference - Publication <p>Focal Loss for Dense Object Detection (Aug 2017)  Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r  https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>list of float</code> <p>The same as weighing factor in balanced cross entropy.                         Alpha is used to specify the weight of different categories/labels,                         the size of the array needs to be consistent with the number of classes.</p> required <code>gamma</code> <code>float</code> <p>Focusing parameter for modulating factor (1-p).</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>Loss Function</code> <p>A TensorFlow compatible loss function. This object can be                                 passed to the NeuralNetwork <code>loss</code> parameter.</p> Source code in <code>aucmedi/neural_network/loss_functions.py</code> <pre><code>def categorical_focal_loss(alpha, gamma=2.0):\n    \"\"\" Softmax version of focal loss.\n\n    When there is a skew between different categories/labels in your data set,\n    you can try to apply this function as a loss.\n\n    ```\n           m\n      FL = \u2211  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n          c=1\n\n      where m = number of classes, c = class and o = observation\n    ```\n\n    The `class_weights_list` obtained from [compute_class_weights][aucmedi.utils.class_weights.compute_class_weights]\n    can be provided as parameter `alpha`.\n\n    ??? example\n        ```python\n        # Compute class weights\n        from aucmedi.utils.class_weights import compute_class_weights\n        cw_loss, cw_fit = compute_class_weights(class_ohe)\n\n        from aucmedi.neural_network.loss_functions import *\n        my_loss = categorical_focal_loss(alpha=cw_loss)\n\n        model = NeuralNetwork(n_labels=6, channels=3, loss=my_loss)\n        ```\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Umberto Griffo &lt;br&gt;\n        GitHub: [https://github.com/umbertogriffo](https://github.com/umbertogriffo) &lt;br&gt;\n        Source: [https://github.com/umbertogriffo/focal-loss-keras](https://github.com/umbertogriffo/focal-loss-keras) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Focal Loss for Dense Object Detection (Aug 2017) &lt;br&gt;\n        Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r &lt;br&gt;\n        [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)\n\n    Args:\n        alpha (list of float):      The same as weighing factor in balanced cross entropy.\n                                    Alpha is used to specify the weight of different categories/labels,\n                                    the size of the array needs to be consistent with the number of classes.\n        gamma (float):              Focusing parameter for modulating factor (1-p).\n\n    Returns:\n        loss (Loss Function):               A TensorFlow compatible loss function. This object can be\n                                            passed to the [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork] `loss` parameter.\n    \"\"\"\n    alpha = np.array(alpha, dtype=np.float32)\n\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        # Clip the prediction value to prevent NaN's and Inf's\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        # Calculate Cross Entropy\n        cross_entropy = -y_true * K.log(y_pred)\n\n        # Calculate Focal Loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n        # Compute mean loss in mini_batch\n        return K.mean(K.sum(loss, axis=-1))\n\n    return categorical_focal_loss_fixed\n</code></pre>"},{"location":"reference/neural_network/loss_functions/#aucmedi.neural_network.loss_functions.multilabel_focal_loss","title":"<code>multilabel_focal_loss(class_weights, gamma=2.0, class_sparsity_coefficient=1.0)</code>","text":"<p>Focal loss for multi-label classification.</p> Example <pre><code># Compute class weights\nfrom aucmedi.utils.class_weights import compute_class_weights\nclass_weights = compute_multilabel_weights(class_ohe)\n\nfrom aucmedi.neural_network.loss_functions import *\nmy_loss = multilabel_focal_loss(class_weights=class_weights)\n\nmodel = NeuralNetwork(n_labels=6, channels=3, loss=my_loss,\n                       activation_output=\"sigmoid\")\n</code></pre> Reference - Implementation <p>Author: Sushant Tripathy  LinkedIn: https://www.linkedin.com/in/sushanttripathy/  Source: https://github.com/sushanttripathy/Keras_loss_functions/blob/master/focal_loss.py </p> Reference - Publication <p>Focal Loss for Dense Object Detection (Aug 2017)  Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r   https://arxiv.org/abs/1708.02002 </p> <p>Parameters:</p> Name Type Description Default <code>class_weights</code> <code>list of float</code> <p>Non-zero, positive class-weights. This is used instead                                 of alpha parameter.</p> required <code>gamma</code> <code>float</code> <p>The Gamma parameter in Focal Loss. Default value (2.0).</p> <code>2.0</code> <code>class_sparsity_coefficient</code> <code>float</code> <p>The weight of True labels over False labels. Useful                                 if True labels are sparse. Default value (1.0).</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>Loss Function</code> <p>A TensorFlow compatible loss function. This object can be                                 passed to the NeuralNetwork <code>loss</code> parameter.</p> Source code in <code>aucmedi/neural_network/loss_functions.py</code> <pre><code>def multilabel_focal_loss(class_weights, gamma=2.0,\n                          class_sparsity_coefficient=1.0):\n    \"\"\" Focal loss for multi-label classification.\n\n    ??? example\n        ```python\n        # Compute class weights\n        from aucmedi.utils.class_weights import compute_class_weights\n        class_weights = compute_multilabel_weights(class_ohe)\n\n        from aucmedi.neural_network.loss_functions import *\n        my_loss = multilabel_focal_loss(class_weights=class_weights)\n\n        model = NeuralNetwork(n_labels=6, channels=3, loss=my_loss,\n                               activation_output=\"sigmoid\")\n        ```\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Sushant Tripathy &lt;br&gt;\n        LinkedIn: [https://www.linkedin.com/in/sushanttripathy/](https://www.linkedin.com/in/sushanttripathy/) &lt;br&gt;\n        Source: [https://github.com/sushanttripathy/Keras_loss_functions/blob/master/focal_loss.py](https://github.com/sushanttripathy/Keras_loss_functions/blob/master/focal_loss.py) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Focal Loss for Dense Object Detection (Aug 2017) &lt;br&gt;\n        Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r  &lt;br&gt;\n        [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002) &lt;br&gt;\n\n    Args:\n        class_weights (list of float):      Non-zero, positive class-weights. This is used instead\n                                            of alpha parameter.\n        gamma (float):                      The Gamma parameter in Focal Loss. Default value (2.0).\n        class_sparsity_coefficient (float): The weight of True labels over False labels. Useful\n                                            if True labels are sparse. Default value (1.0).\n    Returns:\n        loss (Loss Function):               A TensorFlow compatible loss function. This object can be\n                                            passed to the [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork] `loss` parameter.\n    \"\"\"\n    class_weights = K.constant(class_weights, tf.float32)\n    gamma = K.constant(gamma, tf.float32)\n    class_sparsity_coefficient = K.constant(class_sparsity_coefficient,\n                                            tf.float32)\n\n    def focal_loss_function(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n\n        predictions_0 = (1.0 - y_true) * y_pred\n        predictions_1 = y_true * y_pred\n\n        cross_entropy_0 = (1.0 - y_true) * (-K.log(K.clip(1.0 - predictions_0,\n                                K.epsilon(), 1.0 - K.epsilon())))\n        cross_entropy_1 = y_true *(class_sparsity_coefficient * -K.log(K.clip(\n                                predictions_1, K.epsilon(), 1.0 - K.epsilon())))\n\n        cross_entropy = cross_entropy_1 + cross_entropy_0\n        class_weighted_cross_entropy = cross_entropy * class_weights\n\n        weight_1 = K.pow(K.clip(1.0 - predictions_1,\n                                K.epsilon(), 1.0 - K.epsilon()), gamma)\n        weight_0 = K.pow(K.clip(predictions_0, K.epsilon(),\n                                1.0 - K.epsilon()), gamma)\n\n        weight = weight_0 + weight_1\n        focal_loss_tensor = weight * class_weighted_cross_entropy\n\n        return K.mean(focal_loss_tensor, axis=1)\n\n    return focal_loss_function\n</code></pre>"},{"location":"reference/neural_network/model/","title":"Model","text":""},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork","title":"<code>NeuralNetwork</code>","text":"<p>Neural Network class providing functionality for handling all model methods.</p> <p>This class is the third of the three pillars of AUCMEDI.</p> Pillars of AUCMEDI <ul> <li>aucmedi.data_processing.io_data.input_interface</li> <li>aucmedi.data_processing.data_generator.DataGenerator</li> <li>aucmedi.neural_network.model.NeuralNetwork</li> </ul> <p>With an initialized Neural Network model instance, it is possible to run training and predictions.</p> Example: How to use <pre><code># Initialize model\nmodel = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n# Do some training\ndatagen_train = DataGenerator(samples[:100], \"images_dir/\", labels=class_ohe[:100],\n                              resize=model.meta_input, standardize_mode=model.meta_standardize)\nmodel.train(datagen_train, epochs=50)\n# Do some predictions\ndatagen_test = DataGenerator(samples[100:150], \"images_dir/\", labels=None,\n                             resize=model.meta_input, standardize_mode=model.meta_standardize)\npreds = model.predict(datagen_test)\n</code></pre> Example: How to select an Architecture <pre><code># 2D architecture\nmy_model_a = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n# 3D architecture for multi-label classification (sigmoid activation)\nmy_model_b = NeuralNetwork(n_labels=8, channels=3, architecture=\"3D.ResNet50\",\n                            activation_output=\"sigmoid\")\n# 2D architecture with custom input_shape\nmy_model_c = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.Xception\",\n                            input_shape=(512,512))\n</code></pre> List of implemented Architectures <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.</p> <ul> <li>2D Architectures: aucmedi.neural_network.architectures.image</li> <li>3D Architectures: aucmedi.neural_network.architectures.volume</li> </ul> Classification Types Type Activation Function Binary classification <code>activation_output=\"softmax\"</code>: Only a single class is correct. Multi-class classification <code>activation_output=\"softmax\"</code>: Only a single class is correct. Multi-label classification <code>activation_output=\"sigmoid\"</code>: Multiple classes can be correct. <p>Defined by the Classifier of an Architecture.</p> Example: How to obtain required parameters for the DataGenerator? <p>Be aware that the input_size and standardize_mode are just recommendations and can be changed by desire.  However, the recommended parameter are required for transfer learning.</p> Recommended way<pre><code>my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=my_model.meta_input,                  # (224,224)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Manual way<pre><code>from aucmedi.neural_network.architectures import Classifier,                                                          architecture_dict,                                                          supported_standardize_mode\n\nclassification_head = Classifier(n_labels=4, activation_output=\"softmax\")\nmy_arch = architecture_dict[\"3D.DenseNet121\"](classification_head,\n                                              channels=1,\n                                              input_shape=(128,128,128))\n\nmy_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n\nfrom aucmedi.neural_network.architectures import supported_standardize_mode\nsf_norm = supported_standardize_mode[\"3D.DenseNet121\"]\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=(128,128,128),                        # (128,128,128)\n                      standardize_mode=sf_norm)                    # \"torch\"\n</code></pre> Example: How to integrate metadata in AUCMEDI? <pre><code>from aucmedi import *\nimport numpy as np\n\nmy_metadata = np.random.rand(len(samples), 10)\n\nmy_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                          meta_variables=10)\n\nmy_dg = DataGenerator(samples, \"images_dir/\",\n                      labels=None, metadata=my_metadata,\n                      resize=my_model.meta_input,                  # (224,224)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>class NeuralNetwork:\n    \"\"\" Neural Network class providing functionality for handling all model methods.\n\n    This class is the third of the three pillars of AUCMEDI.\n\n    ??? info \"Pillars of AUCMEDI\"\n        - [aucmedi.data_processing.io_data.input_interface][]\n        - [aucmedi.data_processing.data_generator.DataGenerator][]\n        - [aucmedi.neural_network.model.NeuralNetwork][]\n\n    With an initialized Neural Network model instance, it is possible to run training and predictions.\n\n    ??? example \"Example: How to use\"\n        ```python\n        # Initialize model\n        model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.ResNet50\")\n        # Do some training\n        datagen_train = DataGenerator(samples[:100], \"images_dir/\", labels=class_ohe[:100],\n                                      resize=model.meta_input, standardize_mode=model.meta_standardize)\n        model.train(datagen_train, epochs=50)\n        # Do some predictions\n        datagen_test = DataGenerator(samples[100:150], \"images_dir/\", labels=None,\n                                     resize=model.meta_input, standardize_mode=model.meta_standardize)\n        preds = model.predict(datagen_test)\n        ```\n\n    ??? example \"Example: How to select an Architecture\"\n        ```python\n        # 2D architecture\n        my_model_a = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n        # 3D architecture for multi-label classification (sigmoid activation)\n        my_model_b = NeuralNetwork(n_labels=8, channels=3, architecture=\"3D.ResNet50\",\n                                    activation_output=\"sigmoid\")\n        # 2D architecture with custom input_shape\n        my_model_c = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.Xception\",\n                                    input_shape=(512,512))\n        ```\n\n    ??? note \"List of implemented Architectures\"\n        AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.\n\n        - 2D Architectures: [aucmedi.neural_network.architectures.image][]\n        - 3D Architectures: [aucmedi.neural_network.architectures.volume][]\n\n    ??? note \"Classification Types\"\n        | Type                       | Activation Function                                             |\n        | -------------------------- | --------------------------------------------------------------- |\n        | Binary classification      | `activation_output=\"softmax\"`: Only a single class is correct.  |\n        | Multi-class classification | `activation_output=\"softmax\"`: Only a single class is correct.  |\n        | Multi-label classification | `activation_output=\"sigmoid\"`: Multiple classes can be correct. |\n\n        Defined by the [Classifier][aucmedi.neural_network.architectures.classifier] of an\n        [Architecture][aucmedi.neural_network.architectures].\n\n    ??? example \"Example: How to obtain required parameters for the DataGenerator?\"\n        Be aware that the input_size and standardize_mode are just recommendations and\n        can be changed by desire. &lt;br&gt;\n        However, the recommended parameter are required for transfer learning.\n\n        ```python title=\"Recommended way\"\n        my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n\n        my_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                              resize=my_model.meta_input,                  # (224,224)\n                              standardize_mode=my_model.meta_standardize)  # \"torch\"\n        ```\n\n        ```python title=\"Manual way\"\n        from aucmedi.neural_network.architectures import Classifier, \\\n                                                         architecture_dict, \\\n                                                         supported_standardize_mode\n\n        classification_head = Classifier(n_labels=4, activation_output=\"softmax\")\n        my_arch = architecture_dict[\"3D.DenseNet121\"](classification_head,\n                                                      channels=1,\n                                                      input_shape=(128,128,128))\n\n        my_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n\n        from aucmedi.neural_network.architectures import supported_standardize_mode\n        sf_norm = supported_standardize_mode[\"3D.DenseNet121\"]\n        my_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                              resize=(128,128,128),                        # (128,128,128)\n                              standardize_mode=sf_norm)                    # \"torch\"\n        ```\n\n    ??? example \"Example: How to integrate metadata in AUCMEDI?\"\n        ```python\n        from aucmedi import *\n        import numpy as np\n\n        my_metadata = np.random.rand(len(samples), 10)\n\n        my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                                  meta_variables=10)\n\n        my_dg = DataGenerator(samples, \"images_dir/\",\n                              labels=None, metadata=my_metadata,\n                              resize=my_model.meta_input,                  # (224,224)\n                              standardize_mode=my_model.meta_standardize)  # \"torch\"\n        ```\n    \"\"\"\n    def __init__(self, n_labels, channels, input_shape=None, architecture=None,\n                 pretrained_weights=False, loss=\"categorical_crossentropy\",\n                 metrics=[\"categorical_accuracy\"], activation_output=\"softmax\",\n                 fcl_dropout=True, meta_variables=None, learning_rate=0.0001,\n                 verbose=1):\n        \"\"\" Initialization function for creating a Neural Network (model) object.\n\n        Args:\n            n_labels (int):                         Number of classes/labels (important for the last layer).\n            channels (int):                         Number of channels. Grayscale:1 or RGB:3.\n            input_shape (tuple):                    Input shape of the batch imaging data (including channel axis).\n                                                    If None is provided, the default input_shape for the architecture is selected\n                                                    from the architecture dictionary.\n            architecture (str or Architecture):     Key (str) or instance of a neural network model Architecture class instance.\n                                                    If a string is provided, the corresponding architecture is selected from the architecture dictionary.\n                                                    A string has to begin with either '3D.' or '2D' depending on the classification task.\n                                                    By default, a 2D Vanilla Model is used as architecture.\n            pretrained_weights (bool):              Option whether to utilize pretrained weights e.g. from ImageNet.\n            loss (Metric Function):                 The metric function which is used as loss for training.\n                                                    Any Metric Function defined in Keras, in aucmedi.neural_network.loss_functions or any custom\n                                                    metric function, which follows the Keras metric guidelines, can be used.\n            metrics (list of Metric Functions):     List of one or multiple Metric Functions, which will be shown during training.\n                                                    Any Metric Function defined in Keras or any custom metric function, which follows the Keras\n                                                    metric guidelines, can be used.\n            activation_output (str):                Activation function which should be used in the classification head\n                                                    ([Classifier][aucmedi.neural_network.architectures.classifier]).\n                                                    Based on [https://www.tensorflow.org/api_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n            fcl_dropout (bool):                     Option whether to utilize an additional Dense &amp; Dropout layer in the classification head\n                                                    ([Classifier][aucmedi.neural_network.architectures.classifier]).\n            meta_variables (int):                   Number of metadata variables, which should be included in the classification head.\n                                                    If `None`is provided, no metadata integration block will be added to the classification head\n                                                    ([Classifier][aucmedi.neural_network.architectures.classifier]).\n            learning_rate (float):                  Learning rate in which weights of the neural network will be updated.\n            verbose (int):                          Option (0/1) how much information should be written to stdout.\n\n        ???+ danger\n            Class attributes can be modified also after initialization, at will.\n            However, be aware of unexpected adverse effects (experimental)!\n\n        Attributes:\n            tf_epochs (int, default=5):             Transfer Learning configuration: Number of epochs with frozen layers except classification head.\n            tf_lr_start (float, default=1e-4):      Transfer Learning configuration: Starting learning rate for frozen layer fitting.\n            tf_lr_end (float, default=1e-5):        Transfer Learning configuration: Starting learning rate after layer unfreezing.\n            meta_input (tuple of int):              Meta variable: Input shape of architecture which can be passed to a DataGenerator. For example: (224, 224).\n            meta_standardize (str):                 Meta variable: Recommended standardize_mode of architecture which can be passed to a DataGenerator.\n                                                    For example: \"torch\".\n        \"\"\"\n        # Cache parameters\n        self.n_labels = n_labels\n        self.channels = channels\n        self.loss = loss\n        self.metrics = metrics\n        self.learning_rate = learning_rate\n        self.pretrained_weights = pretrained_weights\n        self.activation_output = activation_output\n        self.fcl_dropout = fcl_dropout\n        self.meta_variables = meta_variables\n        self.verbose = verbose\n\n        # Assemble architecture parameters\n        arch_paras = {\"channels\":channels,\n                      \"pretrained_weights\": pretrained_weights}\n        if input_shape is not None : arch_paras[\"input_shape\"] = input_shape\n        # Assemble classifier parameters\n        classifier_paras = {\"n_labels\": n_labels, \"fcl_dropout\": fcl_dropout,\n                            \"activation_output\": activation_output}\n        if meta_variables is not None:\n            classifier_paras[\"meta_variables\"] = meta_variables\n        # Initialize classifier for the classification head\n        arch_paras[\"classification_head\"] = Classifier(**classifier_paras)\n        # Initialize architecture if None provided\n        if architecture is None:\n            self.architecture = architecture_dict[\"2D.Vanilla\"](**arch_paras)\n            self.meta_standardize = \"z-score\"\n        # Initialize passed architecture from aucmedi library\n        elif isinstance(architecture, str) and architecture in architecture_dict:\n            self.architecture = architecture_dict[architecture](**arch_paras)\n            self.meta_standardize = supported_standardize_mode[architecture]\n        # Initialize passed architecture as parameter\n        else:\n            self.architecture = architecture\n            self.meta_standardize = None\n\n        # Build model utilizing the selected architecture\n        self.model = self.architecture.create_model()\n\n        # Compile model\n        self.model.compile(optimizer=Adam(learning_rate=learning_rate),\n                           loss=self.loss, metrics=self.metrics)\n\n        # Obtain final input shape\n        self.input_shape = self.architecture.input          # e.g. (224, 224, 3)\n        self.meta_input = self.architecture.input[:-1]      # e.g. (224, 224) -&gt; for DataGenerator\n        # Cache starting weights\n        self.initialization_weights = self.model.get_weights()\n\n    #---------------------------------------------#\n    #               Class Variables               #\n    #---------------------------------------------#\n    # Transfer Learning configurations\n    tf_epochs = 10\n    tf_lr_start = 1e-4\n    tf_lr_end = 1e-5\n\n    #---------------------------------------------#\n    #                  Training                   #\n    #---------------------------------------------#\n    # Training the Neural Network model\n    def train(self, training_generator, validation_generator=None, epochs=20,\n              iterations=None, callbacks=[], class_weights=None,\n              transfer_learning=False):\n        \"\"\" Fitting function for the Neural Network model performing a training process.\n\n        It is also possible to pass custom Callback classes in order to obtain more information.\n\n        If an optional validation [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator]\n        is provided, a validation set is analyzed regularly during the training process (after each epoch).\n\n        The transfer learning training runs two fitting processes.\n        The first one with frozen base model layers and a high learning rate,\n        whereas the second one with unfrozen layers and a small learning rate.\n\n        ??? info \"Keras History Objects for Transfer Learning\"\n            For the transfer learning training, two Keras history objects will be created.\n\n            However, in order to provide consistency with the single training without transfer learning,\n            only a single history dictionary will be returned.\n\n            For differentiation prefixes are added in front of the corresponding logging keys:\n            ```\n            - History Start -&gt;  prefix : tl     for \"transfer learning\"\n            - History End   -&gt;  prefix : ft     for \"fine tuning\"\n            ```\n\n        Args:\n            training_generator (DataGenerator):     A data generator which will be used for training.\n            validation_generator (DataGenerator):   A data generator which will be used for validation.\n            epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                    the complete data set.\n            iterations (int):                       Number of iterations (batches) in a single epoch.\n            callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n            class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n            transfer_learning (bool):               Option whether a transfer learning training should be performed. If true, a minimum of 5 epochs will be trained.\n\n        Returns:\n            history (dict):                   A history dictionary from a Keras history object which contains several logs.\n        \"\"\"\n        # Adjust number of iterations in training DataGenerator to allow repitition\n        if iterations is not None : training_generator.set_length(iterations)\n        # Running a standard training process\n        if not transfer_learning:\n            # Run training process with the Keras fit function\n            history = self.model.fit(training_generator,\n                                     validation_data=validation_generator,\n                                     callbacks=callbacks, epochs=epochs,\n                                     steps_per_epoch=iterations,\n                                     class_weight=class_weights,\n                                     verbose=self.verbose)\n            # Return logged history object\n            history_out = history.history\n        # Running a transfer learning training process\n        else:\n            # Freeze all base model layers (all layers after \"avg_pool\")\n            lever = False\n            for layer in reversed(self.model.layers):\n                if not lever and layer.name == \"avg_pool\" : lever = True\n                elif lever : layer.trainable = False\n            # Compile model with high learning rate\n            self.model.compile(optimizer=Adam(learning_rate=self.tf_lr_start),\n                               loss=self.loss, metrics=self.metrics)\n            # Run first training with frozen layers\n            history_start = self.model.fit(training_generator,\n                                           validation_data=validation_generator,\n                                           callbacks=callbacks,\n                                           epochs=self.tf_epochs,\n                                           steps_per_epoch=iterations,\n                                           class_weight=class_weights,\n                                           verbose=self.verbose)\n            # Unfreeze base model layers again\n            for layer in self.model.layers:\n                layer.trainable = True\n            # Compile model with lower learning rate\n            self.model.compile(optimizer=Adam(learning_rate=self.tf_lr_end),\n                               loss=self.loss, metrics=self.metrics)\n            # Run second training with unfrozed layers\n            history_end = self.model.fit(training_generator,\n                                         validation_data=validation_generator,\n                                         callbacks=callbacks, epochs=epochs,\n                                         initial_epoch=self.tf_epochs,\n                                         steps_per_epoch=iterations,\n                                         class_weight=class_weights,\n                                         verbose=self.verbose)\n            # Combine logged history objects\n            hs = {\"tl_\" + k: v for k, v in history_start.history.items()}       # prefix : tl for transfer learning\n            he = {\"ft_\" + k: v for k, v in history_end.history.items()}         # prefix : ft for fine tuning\n            history = {**hs, **he}\n            # Return combined history objects\n            history_out = history\n        # Reset number of iterations of the training DataGenerator\n        if iterations is not None : training_generator.reset_length()\n        # Return fitting history\n        return history_out\n\n    #---------------------------------------------#\n    #                 Prediction                  #\n    #---------------------------------------------#\n    def predict(self, prediction_generator):\n        \"\"\" Prediction function for the Neural Network model.\n\n        The fitted model will predict classifications for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n        Args:\n            prediction_generator (DataGenerator):   A data generator which will be used for inference.\n\n        Returns:\n            preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n        \"\"\"\n        # Run inference process with the Keras predict function\n        preds = self.model.predict(prediction_generator,\n                                   verbose=self.verbose)\n        # Output predictions results\n        return preds\n\n    #---------------------------------------------#\n    #               Model Management              #\n    #---------------------------------------------#\n    # Re-initialize model weights\n    def reset_weights(self):\n        \"\"\" Re-initialize weights of the neural network model.\n\n        Useful for training multiple models with the same NeuralNetwork object.\n        \"\"\"\n        self.model.set_weights(self.initialization_weights)\n\n    # Dump model to file\n    def dump(self, file_path):\n        \"\"\" Store model to disk.\n\n        Recommended to utilize the file format \".keras\".\n\n        Args:\n            file_path (str):    Path to store the model on disk.\n        \"\"\"\n        self.model.save(file_path)\n\n    # Load model from file\n    def load(self, file_path, custom_objects={}):\n        \"\"\" Load neural network model and its weights from a file.\n\n        After loading, the model will be compiled.\n\n        If loading a model in \".keras\" format, it is not necessary to define any custom_objects.\n\n        Args:\n            file_path (str):            Input path, from which the model will be loaded.\n            custom_objects (dict):      Dictionary of custom objects for compiling\n                                        (e.g. non-TensorFlow based loss functions or architectures).\n        \"\"\"\n        # Create model input path\n        self.model = load_model(file_path, custom_objects, compile=False)\n        # Compile model\n        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n                           loss=self.loss, metrics=self.metrics)\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.__init__","title":"<code>__init__(n_labels, channels, input_shape=None, architecture=None, pretrained_weights=False, loss='categorical_crossentropy', metrics=['categorical_accuracy'], activation_output='softmax', fcl_dropout=True, meta_variables=None, learning_rate=0.0001, verbose=1)</code>","text":"<p>Initialization function for creating a Neural Network (model) object.</p> <p>Parameters:</p> Name Type Description Default <code>n_labels</code> <code>int</code> <p>Number of classes/labels (important for the last layer).</p> required <code>channels</code> <code>int</code> <p>Number of channels. Grayscale:1 or RGB:3.</p> required <code>input_shape</code> <code>tuple</code> <p>Input shape of the batch imaging data (including channel axis).                                     If None is provided, the default input_shape for the architecture is selected                                     from the architecture dictionary.</p> <code>None</code> <code>architecture</code> <code>str or Architecture</code> <p>Key (str) or instance of a neural network model Architecture class instance.                                     If a string is provided, the corresponding architecture is selected from the architecture dictionary.                                     A string has to begin with either '3D.' or '2D' depending on the classification task.                                     By default, a 2D Vanilla Model is used as architecture.</p> <code>None</code> <code>pretrained_weights</code> <code>bool</code> <p>Option whether to utilize pretrained weights e.g. from ImageNet.</p> <code>False</code> <code>loss</code> <code>Metric Function</code> <p>The metric function which is used as loss for training.                                     Any Metric Function defined in Keras, in aucmedi.neural_network.loss_functions or any custom                                     metric function, which follows the Keras metric guidelines, can be used.</p> <code>'categorical_crossentropy'</code> <code>metrics</code> <code>list of Metric Functions</code> <p>List of one or multiple Metric Functions, which will be shown during training.                                     Any Metric Function defined in Keras or any custom metric function, which follows the Keras                                     metric guidelines, can be used.</p> <code>['categorical_accuracy']</code> <code>activation_output</code> <code>str</code> <p>Activation function which should be used in the classification head                                     (Classifier).                                     Based on https://www.tensorflow.org/api_docs/python/tf/keras/activations.</p> <code>'softmax'</code> <code>fcl_dropout</code> <code>bool</code> <p>Option whether to utilize an additional Dense &amp; Dropout layer in the classification head                                     (Classifier).</p> <code>True</code> <code>meta_variables</code> <code>int</code> <p>Number of metadata variables, which should be included in the classification head.                                     If <code>None</code>is provided, no metadata integration block will be added to the classification head                                     (Classifier).</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate in which weights of the neural network will be updated.</p> <code>0.0001</code> <code>verbose</code> <code>int</code> <p>Option (0/1) how much information should be written to stdout.</p> <code>1</code> Danger <p>Class attributes can be modified also after initialization, at will. However, be aware of unexpected adverse effects (experimental)!</p> <p>Attributes:</p> Name Type Description <code>tf_epochs</code> <code>int, default=5</code> <p>Transfer Learning configuration: Number of epochs with frozen layers except classification head.</p> <code>tf_lr_start</code> <code>float, default=1e-4</code> <p>Transfer Learning configuration: Starting learning rate for frozen layer fitting.</p> <code>tf_lr_end</code> <code>float, default=1e-5</code> <p>Transfer Learning configuration: Starting learning rate after layer unfreezing.</p> <code>meta_input</code> <code>tuple of int</code> <p>Meta variable: Input shape of architecture which can be passed to a DataGenerator. For example: (224, 224).</p> <code>meta_standardize</code> <code>str</code> <p>Meta variable: Recommended standardize_mode of architecture which can be passed to a DataGenerator.                                     For example: \"torch\".</p> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def __init__(self, n_labels, channels, input_shape=None, architecture=None,\n             pretrained_weights=False, loss=\"categorical_crossentropy\",\n             metrics=[\"categorical_accuracy\"], activation_output=\"softmax\",\n             fcl_dropout=True, meta_variables=None, learning_rate=0.0001,\n             verbose=1):\n    \"\"\" Initialization function for creating a Neural Network (model) object.\n\n    Args:\n        n_labels (int):                         Number of classes/labels (important for the last layer).\n        channels (int):                         Number of channels. Grayscale:1 or RGB:3.\n        input_shape (tuple):                    Input shape of the batch imaging data (including channel axis).\n                                                If None is provided, the default input_shape for the architecture is selected\n                                                from the architecture dictionary.\n        architecture (str or Architecture):     Key (str) or instance of a neural network model Architecture class instance.\n                                                If a string is provided, the corresponding architecture is selected from the architecture dictionary.\n                                                A string has to begin with either '3D.' or '2D' depending on the classification task.\n                                                By default, a 2D Vanilla Model is used as architecture.\n        pretrained_weights (bool):              Option whether to utilize pretrained weights e.g. from ImageNet.\n        loss (Metric Function):                 The metric function which is used as loss for training.\n                                                Any Metric Function defined in Keras, in aucmedi.neural_network.loss_functions or any custom\n                                                metric function, which follows the Keras metric guidelines, can be used.\n        metrics (list of Metric Functions):     List of one or multiple Metric Functions, which will be shown during training.\n                                                Any Metric Function defined in Keras or any custom metric function, which follows the Keras\n                                                metric guidelines, can be used.\n        activation_output (str):                Activation function which should be used in the classification head\n                                                ([Classifier][aucmedi.neural_network.architectures.classifier]).\n                                                Based on [https://www.tensorflow.org/api_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n        fcl_dropout (bool):                     Option whether to utilize an additional Dense &amp; Dropout layer in the classification head\n                                                ([Classifier][aucmedi.neural_network.architectures.classifier]).\n        meta_variables (int):                   Number of metadata variables, which should be included in the classification head.\n                                                If `None`is provided, no metadata integration block will be added to the classification head\n                                                ([Classifier][aucmedi.neural_network.architectures.classifier]).\n        learning_rate (float):                  Learning rate in which weights of the neural network will be updated.\n        verbose (int):                          Option (0/1) how much information should be written to stdout.\n\n    ???+ danger\n        Class attributes can be modified also after initialization, at will.\n        However, be aware of unexpected adverse effects (experimental)!\n\n    Attributes:\n        tf_epochs (int, default=5):             Transfer Learning configuration: Number of epochs with frozen layers except classification head.\n        tf_lr_start (float, default=1e-4):      Transfer Learning configuration: Starting learning rate for frozen layer fitting.\n        tf_lr_end (float, default=1e-5):        Transfer Learning configuration: Starting learning rate after layer unfreezing.\n        meta_input (tuple of int):              Meta variable: Input shape of architecture which can be passed to a DataGenerator. For example: (224, 224).\n        meta_standardize (str):                 Meta variable: Recommended standardize_mode of architecture which can be passed to a DataGenerator.\n                                                For example: \"torch\".\n    \"\"\"\n    # Cache parameters\n    self.n_labels = n_labels\n    self.channels = channels\n    self.loss = loss\n    self.metrics = metrics\n    self.learning_rate = learning_rate\n    self.pretrained_weights = pretrained_weights\n    self.activation_output = activation_output\n    self.fcl_dropout = fcl_dropout\n    self.meta_variables = meta_variables\n    self.verbose = verbose\n\n    # Assemble architecture parameters\n    arch_paras = {\"channels\":channels,\n                  \"pretrained_weights\": pretrained_weights}\n    if input_shape is not None : arch_paras[\"input_shape\"] = input_shape\n    # Assemble classifier parameters\n    classifier_paras = {\"n_labels\": n_labels, \"fcl_dropout\": fcl_dropout,\n                        \"activation_output\": activation_output}\n    if meta_variables is not None:\n        classifier_paras[\"meta_variables\"] = meta_variables\n    # Initialize classifier for the classification head\n    arch_paras[\"classification_head\"] = Classifier(**classifier_paras)\n    # Initialize architecture if None provided\n    if architecture is None:\n        self.architecture = architecture_dict[\"2D.Vanilla\"](**arch_paras)\n        self.meta_standardize = \"z-score\"\n    # Initialize passed architecture from aucmedi library\n    elif isinstance(architecture, str) and architecture in architecture_dict:\n        self.architecture = architecture_dict[architecture](**arch_paras)\n        self.meta_standardize = supported_standardize_mode[architecture]\n    # Initialize passed architecture as parameter\n    else:\n        self.architecture = architecture\n        self.meta_standardize = None\n\n    # Build model utilizing the selected architecture\n    self.model = self.architecture.create_model()\n\n    # Compile model\n    self.model.compile(optimizer=Adam(learning_rate=learning_rate),\n                       loss=self.loss, metrics=self.metrics)\n\n    # Obtain final input shape\n    self.input_shape = self.architecture.input          # e.g. (224, 224, 3)\n    self.meta_input = self.architecture.input[:-1]      # e.g. (224, 224) -&gt; for DataGenerator\n    # Cache starting weights\n    self.initialization_weights = self.model.get_weights()\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.dump","title":"<code>dump(file_path)</code>","text":"<p>Store model to disk.</p> <p>Recommended to utilize the file format \".keras\".</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to store the model on disk.</p> required Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def dump(self, file_path):\n    \"\"\" Store model to disk.\n\n    Recommended to utilize the file format \".keras\".\n\n    Args:\n        file_path (str):    Path to store the model on disk.\n    \"\"\"\n    self.model.save(file_path)\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.load","title":"<code>load(file_path, custom_objects={})</code>","text":"<p>Load neural network model and its weights from a file.</p> <p>After loading, the model will be compiled.</p> <p>If loading a model in \".keras\" format, it is not necessary to define any custom_objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Input path, from which the model will be loaded.</p> required <code>custom_objects</code> <code>dict</code> <p>Dictionary of custom objects for compiling                         (e.g. non-TensorFlow based loss functions or architectures).</p> <code>{}</code> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def load(self, file_path, custom_objects={}):\n    \"\"\" Load neural network model and its weights from a file.\n\n    After loading, the model will be compiled.\n\n    If loading a model in \".keras\" format, it is not necessary to define any custom_objects.\n\n    Args:\n        file_path (str):            Input path, from which the model will be loaded.\n        custom_objects (dict):      Dictionary of custom objects for compiling\n                                    (e.g. non-TensorFlow based loss functions or architectures).\n    \"\"\"\n    # Create model input path\n    self.model = load_model(file_path, custom_objects, compile=False)\n    # Compile model\n    self.model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n                       loss=self.loss, metrics=self.metrics)\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.predict","title":"<code>predict(prediction_generator)</code>","text":"<p>Prediction function for the Neural Network model.</p> <p>The fitted model will predict classifications for the provided DataGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for inference.</p> required <p>Returns:</p> Name Type Description <code>preds</code> <code>numpy.ndarray</code> <p>A NumPy array of predictions formatted with shape (n_samples, n_labels).</p> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def predict(self, prediction_generator):\n    \"\"\" Prediction function for the Neural Network model.\n\n    The fitted model will predict classifications for the provided [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    Args:\n        prediction_generator (DataGenerator):   A data generator which will be used for inference.\n\n    Returns:\n        preds (numpy.ndarray):                  A NumPy array of predictions formatted with shape (n_samples, n_labels).\n    \"\"\"\n    # Run inference process with the Keras predict function\n    preds = self.model.predict(prediction_generator,\n                               verbose=self.verbose)\n    # Output predictions results\n    return preds\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.reset_weights","title":"<code>reset_weights()</code>","text":"<p>Re-initialize weights of the neural network model.</p> <p>Useful for training multiple models with the same NeuralNetwork object.</p> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def reset_weights(self):\n    \"\"\" Re-initialize weights of the neural network model.\n\n    Useful for training multiple models with the same NeuralNetwork object.\n    \"\"\"\n    self.model.set_weights(self.initialization_weights)\n</code></pre>"},{"location":"reference/neural_network/model/#aucmedi.neural_network.model.NeuralNetwork.train","title":"<code>train(training_generator, validation_generator=None, epochs=20, iterations=None, callbacks=[], class_weights=None, transfer_learning=False)</code>","text":"<p>Fitting function for the Neural Network model performing a training process.</p> <p>It is also possible to pass custom Callback classes in order to obtain more information.</p> <p>If an optional validation DataGenerator is provided, a validation set is analyzed regularly during the training process (after each epoch).</p> <p>The transfer learning training runs two fitting processes. The first one with frozen base model layers and a high learning rate, whereas the second one with unfrozen layers and a small learning rate.</p> Keras History Objects for Transfer Learning <p>For the transfer learning training, two Keras history objects will be created.</p> <p>However, in order to provide consistency with the single training without transfer learning, only a single history dictionary will be returned.</p> <p>For differentiation prefixes are added in front of the corresponding logging keys: <pre><code>- History Start -&gt;  prefix : tl     for \"transfer learning\"\n- History End   -&gt;  prefix : ft     for \"fine tuning\"\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>training_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for training.</p> required <code>validation_generator</code> <code>DataGenerator</code> <p>A data generator which will be used for validation.</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of epochs. A single epoch is defined as one iteration through                                     the complete data set.</p> <code>20</code> <code>iterations</code> <code>int</code> <p>Number of iterations (batches) in a single epoch.</p> <code>None</code> <code>callbacks</code> <code>list of Callback classes</code> <p>A list of Callback classes for custom evaluation.</p> <code>[]</code> <code>class_weights</code> <code>dictionary or list</code> <p>A list or dictionary of float values to handle class unbalance.</p> <code>None</code> <code>transfer_learning</code> <code>bool</code> <p>Option whether a transfer learning training should be performed. If true, a minimum of 5 epochs will be trained.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> Source code in <code>aucmedi/neural_network/model.py</code> <pre><code>def train(self, training_generator, validation_generator=None, epochs=20,\n          iterations=None, callbacks=[], class_weights=None,\n          transfer_learning=False):\n    \"\"\" Fitting function for the Neural Network model performing a training process.\n\n    It is also possible to pass custom Callback classes in order to obtain more information.\n\n    If an optional validation [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator]\n    is provided, a validation set is analyzed regularly during the training process (after each epoch).\n\n    The transfer learning training runs two fitting processes.\n    The first one with frozen base model layers and a high learning rate,\n    whereas the second one with unfrozen layers and a small learning rate.\n\n    ??? info \"Keras History Objects for Transfer Learning\"\n        For the transfer learning training, two Keras history objects will be created.\n\n        However, in order to provide consistency with the single training without transfer learning,\n        only a single history dictionary will be returned.\n\n        For differentiation prefixes are added in front of the corresponding logging keys:\n        ```\n        - History Start -&gt;  prefix : tl     for \"transfer learning\"\n        - History End   -&gt;  prefix : ft     for \"fine tuning\"\n        ```\n\n    Args:\n        training_generator (DataGenerator):     A data generator which will be used for training.\n        validation_generator (DataGenerator):   A data generator which will be used for validation.\n        epochs (int):                           Number of epochs. A single epoch is defined as one iteration through\n                                                the complete data set.\n        iterations (int):                       Number of iterations (batches) in a single epoch.\n        callbacks (list of Callback classes):   A list of Callback classes for custom evaluation.\n        class_weights (dictionary or list):     A list or dictionary of float values to handle class unbalance.\n        transfer_learning (bool):               Option whether a transfer learning training should be performed. If true, a minimum of 5 epochs will be trained.\n\n    Returns:\n        history (dict):                   A history dictionary from a Keras history object which contains several logs.\n    \"\"\"\n    # Adjust number of iterations in training DataGenerator to allow repitition\n    if iterations is not None : training_generator.set_length(iterations)\n    # Running a standard training process\n    if not transfer_learning:\n        # Run training process with the Keras fit function\n        history = self.model.fit(training_generator,\n                                 validation_data=validation_generator,\n                                 callbacks=callbacks, epochs=epochs,\n                                 steps_per_epoch=iterations,\n                                 class_weight=class_weights,\n                                 verbose=self.verbose)\n        # Return logged history object\n        history_out = history.history\n    # Running a transfer learning training process\n    else:\n        # Freeze all base model layers (all layers after \"avg_pool\")\n        lever = False\n        for layer in reversed(self.model.layers):\n            if not lever and layer.name == \"avg_pool\" : lever = True\n            elif lever : layer.trainable = False\n        # Compile model with high learning rate\n        self.model.compile(optimizer=Adam(learning_rate=self.tf_lr_start),\n                           loss=self.loss, metrics=self.metrics)\n        # Run first training with frozen layers\n        history_start = self.model.fit(training_generator,\n                                       validation_data=validation_generator,\n                                       callbacks=callbacks,\n                                       epochs=self.tf_epochs,\n                                       steps_per_epoch=iterations,\n                                       class_weight=class_weights,\n                                       verbose=self.verbose)\n        # Unfreeze base model layers again\n        for layer in self.model.layers:\n            layer.trainable = True\n        # Compile model with lower learning rate\n        self.model.compile(optimizer=Adam(learning_rate=self.tf_lr_end),\n                           loss=self.loss, metrics=self.metrics)\n        # Run second training with unfrozed layers\n        history_end = self.model.fit(training_generator,\n                                     validation_data=validation_generator,\n                                     callbacks=callbacks, epochs=epochs,\n                                     initial_epoch=self.tf_epochs,\n                                     steps_per_epoch=iterations,\n                                     class_weight=class_weights,\n                                     verbose=self.verbose)\n        # Combine logged history objects\n        hs = {\"tl_\" + k: v for k, v in history_start.history.items()}       # prefix : tl for transfer learning\n        he = {\"ft_\" + k: v for k, v in history_end.history.items()}         # prefix : ft for fine tuning\n        history = {**hs, **he}\n        # Return combined history objects\n        history_out = history\n    # Reset number of iterations of the training DataGenerator\n    if iterations is not None : training_generator.reset_length()\n    # Return fitting history\n    return history_out\n</code></pre>"},{"location":"reference/neural_network/architectures/","title":"Architectures","text":"<p>Models are represented with the open-source neural network library TensorFlow.Keras     which provides an user-friendly API for commonly used neural-network building blocks.</p> <p>The already implemented architectures are configurable by custom input sizes, optional dropouts, transfer learning via pretrained weights, meta data inclusion or activation output depending on classification type.</p> <p>Additionally, AUCMEDI offers architectures for 2D image and 3D volume classification.</p> Example: How to select an Architecture <p>For architecture selection, just create a key (str) by adding \"2D.\" or \"3D.\" to the architecture name, and pass the key to the <code>architecture</code> parameter of the NeuralNetwork class.</p> <pre><code># 2D architecture\nmy_model_a = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n# 3D architecture for multi-label classification (sigmoid activation)\nmy_model_b = NeuralNetwork(n_labels=8, channels=3, architecture=\"3D.ResNet50\",\n                            activation_output=\"sigmoid\")\n# 2D architecture with custom input_shape\nmy_model_c = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.Xception\",\n                            input_shape=(512,512))\n</code></pre> List of implemented Architectures <p>AUCMEDI provides a large library of state-of-the-art and ready-to-use architectures.</p> <ul> <li>2D Architectures: aucmedi.neural_network.architectures.image</li> <li>3D Architectures: aucmedi.neural_network.architectures.volume</li> </ul> <p>Besides the flexibility in switching between already implemented architecture, the abstract base class interface for architectures offers the possibility for custom architecture integration into the AUCMEDI pipeline.</p> <p>Furthermore, AUCMEDI offers the powerful classification head interface Classifier, which can be used for all types of image classifications and will be automatically created in the NeuralNetwork class.</p>"},{"location":"reference/neural_network/architectures/arch_base/","title":"Arch base","text":""},{"location":"reference/neural_network/architectures/arch_base/#aucmedi.neural_network.architectures.arch_base.Architecture_Base","title":"<code>Architecture_Base</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class for an Architecture class.</p> <p>This class provides functionality for running the create_model function, which returns a tensorflow.keras model.</p> Create a custom Architecture <pre><code>from aucmedi.neural_network.architectures import Architecture_Base\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n\nclass My_custom_Architecture(Architecture_Base):\n    def __init__(self, classification_head, channels, input_shape=(224, 224),\n                 pretrained_weights=False):\n        self.classifier = classification_head\n        self.input = input_shape + (channels,)\n        self.pretrained_weights = pretrained_weights\n\n    def create_model(self):\n        # Initialize input layer\n        model_input = Input(shape=self.input)\n\n        # Add whatever architecture you want\n        model_base = Conv2D(filters=32)(model_input)\n        model_base = Conv2D(filters=64)(model_base)\n        model_base = MaxPooling2D(pool_size=2)(model_base)\n\n        # Add classification head via Classifier\n        my_keras_model = self.classifier.build(model_input=model_input,\n                                              model_output=model_base)\n        # Return created model\n        return my_keras_model\n</code></pre> Required Functions Function Description <code>__init__()</code> Object creation function. <code>create_model()</code> Creating and returning the architecture model. Source code in <code>aucmedi/neural_network/architectures/arch_base.py</code> <pre><code>class Architecture_Base(ABC):\n    \"\"\" An abstract base class for an Architecture class.\n\n    This class provides functionality for running the create_model function,\n    which returns a [tensorflow.keras model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n\n    ???+ example \"Create a custom Architecture\"\n        ```python\n        from aucmedi.neural_network.architectures import Architecture_Base\n        from tensorflow.keras import Input\n        from tensorflow.keras.layers import Conv2D, MaxPooling2D\n\n        class My_custom_Architecture(Architecture_Base):\n            def __init__(self, classification_head, channels, input_shape=(224, 224),\n                         pretrained_weights=False):\n                self.classifier = classification_head\n                self.input = input_shape + (channels,)\n                self.pretrained_weights = pretrained_weights\n\n            def create_model(self):\n                # Initialize input layer\n                model_input = Input(shape=self.input)\n\n                # Add whatever architecture you want\n                model_base = Conv2D(filters=32)(model_input)\n                model_base = Conv2D(filters=64)(model_base)\n                model_base = MaxPooling2D(pool_size=2)(model_base)\n\n                # Add classification head via Classifier\n                my_keras_model = self.classifier.build(model_input=model_input,\n                                                      model_output=model_base)\n                # Return created model\n                return my_keras_model\n        ```\n\n    ???+ info \"Required Functions\"\n        | Function            | Description                                    |\n        | ------------------- | ---------------------------------------------- |\n        | `__init__()`        | Object creation function.                      |\n        | `create_model()`    | Creating and returning the architecture model. |\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    @abstractmethod\n    def __init__(self, classification_head, channels, input_shape=(224, 224),\n                 pretrained_weights=False):\n        \"\"\" Functions which will be called during the Architecture object creation.\n\n        This function can be used to pass variables and options in the Architecture instance.\n\n        There are some mandatory required parameters for the initialization: The classification head as\n        [Classifier][aucmedi.neural_network.architectures.classifier], the number of channels, and the\n        input shape (x, y) for an image architecture or (x, y, z) for a volume architecture.\n\n        Args:\n            classification_head (Classifier):   Classifier object for building the classification head of the model.\n            channels (int):                     Number of channels. For example: Grayscale-&gt;1 or RGB-&gt;3.\n            input_shape (tuple):                Input shape of the image data for the first model layer (excluding channel axis).\n            pretrained_weights (bool):          Option whether to utilize pretrained weights e.g. for ImageNet.\n        \"\"\"\n        self.classifier = classification_head\n        self.input = input_shape + (channels,)\n        self.pretrained_weights = pretrained_weights\n\n    #---------------------------------------------#\n    #                Create Model                 #\n    #---------------------------------------------#\n    @abstractmethod\n    def create_model(self):\n        \"\"\" Create the deep learning or convolutional neural network model.\n\n        This function will be called inside the AUCMEDI model class and have to return a functional\n        Keras model. The model itself should be created here or in a subfunction called\n        by this function.\n\n        At the end of the model building process, the classification head must be appended\n        via calling the `build()` function of a [Classifier][aucmedi.neural_network.architectures.classifier].\n\n        Returns:\n            model (tf.keras model):            A Keras model.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/neural_network/architectures/arch_base/#aucmedi.neural_network.architectures.arch_base.Architecture_Base.__init__","title":"<code>__init__(classification_head, channels, input_shape=(224, 224), pretrained_weights=False)</code>  <code>abstractmethod</code>","text":"<p>Functions which will be called during the Architecture object creation.</p> <p>This function can be used to pass variables and options in the Architecture instance.</p> <p>There are some mandatory required parameters for the initialization: The classification head as Classifier, the number of channels, and the input shape (x, y) for an image architecture or (x, y, z) for a volume architecture.</p> <p>Parameters:</p> Name Type Description Default <code>classification_head</code> <code>Classifier</code> <p>Classifier object for building the classification head of the model.</p> required <code>channels</code> <code>int</code> <p>Number of channels. For example: Grayscale-&gt;1 or RGB-&gt;3.</p> required <code>input_shape</code> <code>tuple</code> <p>Input shape of the image data for the first model layer (excluding channel axis).</p> <code>(224, 224)</code> <code>pretrained_weights</code> <code>bool</code> <p>Option whether to utilize pretrained weights e.g. for ImageNet.</p> <code>False</code> Source code in <code>aucmedi/neural_network/architectures/arch_base.py</code> <pre><code>@abstractmethod\ndef __init__(self, classification_head, channels, input_shape=(224, 224),\n             pretrained_weights=False):\n    \"\"\" Functions which will be called during the Architecture object creation.\n\n    This function can be used to pass variables and options in the Architecture instance.\n\n    There are some mandatory required parameters for the initialization: The classification head as\n    [Classifier][aucmedi.neural_network.architectures.classifier], the number of channels, and the\n    input shape (x, y) for an image architecture or (x, y, z) for a volume architecture.\n\n    Args:\n        classification_head (Classifier):   Classifier object for building the classification head of the model.\n        channels (int):                     Number of channels. For example: Grayscale-&gt;1 or RGB-&gt;3.\n        input_shape (tuple):                Input shape of the image data for the first model layer (excluding channel axis).\n        pretrained_weights (bool):          Option whether to utilize pretrained weights e.g. for ImageNet.\n    \"\"\"\n    self.classifier = classification_head\n    self.input = input_shape + (channels,)\n    self.pretrained_weights = pretrained_weights\n</code></pre>"},{"location":"reference/neural_network/architectures/arch_base/#aucmedi.neural_network.architectures.arch_base.Architecture_Base.create_model","title":"<code>create_model()</code>  <code>abstractmethod</code>","text":"<p>Create the deep learning or convolutional neural network model.</p> <p>This function will be called inside the AUCMEDI model class and have to return a functional Keras model. The model itself should be created here or in a subfunction called by this function.</p> <p>At the end of the model building process, the classification head must be appended via calling the <code>build()</code> function of a Classifier.</p> <p>Returns:</p> Name Type Description <code>model</code> <code>tf.keras model</code> <p>A Keras model.</p> Source code in <code>aucmedi/neural_network/architectures/arch_base.py</code> <pre><code>@abstractmethod\ndef create_model(self):\n    \"\"\" Create the deep learning or convolutional neural network model.\n\n    This function will be called inside the AUCMEDI model class and have to return a functional\n    Keras model. The model itself should be created here or in a subfunction called\n    by this function.\n\n    At the end of the model building process, the classification head must be appended\n    via calling the `build()` function of a [Classifier][aucmedi.neural_network.architectures.classifier].\n\n    Returns:\n        model (tf.keras model):            A Keras model.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/neural_network/architectures/classifier/","title":"Classifier","text":""},{"location":"reference/neural_network/architectures/classifier/#aucmedi.neural_network.architectures.classifier.Classifier","title":"<code>Classifier</code>","text":"<p>A powerful interface for all types of image classifications.</p> <p>This class will be created automatically inside the NeuralNetwork class.</p> <p>Supported Features</p> <ul> <li>Binary classification</li> <li>Multi-class classification</li> <li>Multi-label classification</li> <li>2D/3D data</li> <li>Metadata encoded as NumPy arrays (int or float)</li> </ul> <p>This class provides functionality for building a classification head for an Architecture (tensorflow.keras model). A initialized classifier interface is passed to an architecture class. The <code>build()</code> function of the classification head is called in the <code>create_model()</code> function of the architecture.</p> <p>Structure of the AUCMEDI Classification Head</p> Layer Description GlobalAveragePooling Pooling from Architecture Output to a single spatial dimensions. Dense(units=512) Optional dense &amp; dropout layer if <code>fcl_dropout=True</code>. Dropout(0.3) Optional dense &amp; dropout layer if <code>fcl_dropout=True</code>. Concatenate() Optional appending of metadata to classification head. Dense(units=512) Optional dense &amp; dropout layer if metadata is present. Dropout(0.3) Optional dense &amp; dropout layer if metadata is present. Dense(units=256) Optional dense &amp; dropout layer if metadata is present. Dropout(0.3) Optional dense &amp; dropout layer if metadata is present. Dense(units=n_labels) Dense layer to the number of labels (classes). Activation(activation_output) Activation function corresponding to classification type. Classification Types Type Activation Function Binary classification <code>activation_output=\"softmax\"</code>: Only a single class is correct. Multi-class classification <code>activation_output=\"softmax\"</code>: Only a single class is correct. Multi-label classification <code>activation_output=\"sigmoid\"</code>: Multiple classes can be correct. <p>For more information on multi-class vs multi-label, check out this blog post from Rachel Draelos:  https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/</p> <p>The recommended way is to pass all required variables to the NeuralNetwork which automatically creates the Classifier and passes it to the Architecture.</p> Example <pre><code># Recommended way (automatic creation in NeuralNetwork)\nmodel = NeuralNetwork(n_labels=20, channels=3,\n                      input_shape=(32, 32), activation_output=\"sigmoid\",\n                      fcl_dropout=False)\n\n# Manual way\nfrom aucmedi.neural_network.architectures import Classifier\nfrom aucmedi.neural_network.architectures.image import Vanilla\n\nclassification_head = Classifier(n_labels=20, fcl_dropout=False,\n                                 activation_output=\"sigmoid\")\narch = Vanilla(classification_head, channels=3,\n               input_shape=(32, 32))\n</code></pre> Example: How to integrate metadata in AUCMEDI? <pre><code>from aucmedi import *\nimport numpy as np\n\nmy_metadata = np.random.rand(len(samples), 10)\n\nmy_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                          meta_variables=10)\n\nmy_dg = DataGenerator(samples, \"images_dir/\",\n                      labels=None, metadata=my_metadata,\n                      resize=my_model.meta_input,                  # (224,224)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Source code in <code>aucmedi/neural_network/architectures/classifier.py</code> <pre><code>class Classifier:\n    \"\"\" A powerful interface for all types of image classifications.\n\n    This class will be created automatically inside the [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork] class.\n\n    !!! info \"Supported Features\"\n        - Binary classification\n        - Multi-class classification\n        - Multi-label classification\n        - 2D/3D data\n        - Metadata encoded as NumPy arrays (int or float)\n\n    This class provides functionality for building a classification head for an\n    [Architecture][aucmedi.neural_network.architectures]\n    ([tensorflow.keras model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)).\n    A initialized classifier interface is passed to an architecture class.\n    The `build()` function of the classification head is called in the `create_model()`\n    function of the architecture.\n\n    !!! info \"Structure of the AUCMEDI Classification Head\"\n        | Layer                         | Description                                                      |\n        | ----------------------------- | ---------------------------------------------------------------- |\n        | GlobalAveragePooling          | Pooling from Architecture Output to a single spatial dimensions. |\n        | Dense(units=512)              | Optional dense &amp; dropout layer if `fcl_dropout=True`.            |\n        | Dropout(0.3)                  | Optional dense &amp; dropout layer if `fcl_dropout=True`.            |\n        | Concatenate()                 | Optional appending of metadata to classification head.           |\n        | Dense(units=512)              | Optional dense &amp; dropout layer if metadata is present.           |\n        | Dropout(0.3)                  | Optional dense &amp; dropout layer if metadata is present.           |\n        | Dense(units=256)              | Optional dense &amp; dropout layer if metadata is present.           |\n        | Dropout(0.3)                  | Optional dense &amp; dropout layer if metadata is present.           |\n        | Dense(units=n_labels)         | Dense layer to the number of labels (classes).                   |\n        | Activation(activation_output) | Activation function corresponding to classification type.        |\n\n    ???+ note \"Classification Types\"\n        | Type                       | Activation Function                                             |\n        | -------------------------- | --------------------------------------------------------------- |\n        | Binary classification      | `activation_output=\"softmax\"`: Only a single class is correct.  |\n        | Multi-class classification | `activation_output=\"softmax\"`: Only a single class is correct.  |\n        | Multi-label classification | `activation_output=\"sigmoid\"`: Multiple classes can be correct. |\n\n        For more information on multi-class vs multi-label, check out this blog post from Rachel Draelos: &lt;br&gt;\n        [https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)\n\n    The recommended way is to pass all required variables to the [NeuralNetwork][aucmedi.neural_network.model.NeuralNetwork]\n    which automatically creates the Classifier and passes it to the Architecture.\n\n    ???+ example\n        ```python\n        # Recommended way (automatic creation in NeuralNetwork)\n        model = NeuralNetwork(n_labels=20, channels=3,\n                              input_shape=(32, 32), activation_output=\"sigmoid\",\n                              fcl_dropout=False)\n\n        # Manual way\n        from aucmedi.neural_network.architectures import Classifier\n        from aucmedi.neural_network.architectures.image import Vanilla\n\n        classification_head = Classifier(n_labels=20, fcl_dropout=False,\n                                         activation_output=\"sigmoid\")\n        arch = Vanilla(classification_head, channels=3,\n                       input_shape=(32, 32))\n        ```\n\n    ??? example \"Example: How to integrate metadata in AUCMEDI?\"\n        ```python\n        from aucmedi import *\n        import numpy as np\n\n        my_metadata = np.random.rand(len(samples), 10)\n\n        my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\",\n                                  meta_variables=10)\n\n        my_dg = DataGenerator(samples, \"images_dir/\",\n                              labels=None, metadata=my_metadata,\n                              resize=my_model.meta_input,                  # (224,224)\n                              standardize_mode=my_model.meta_standardize)  # \"torch\"\n        ```\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    def __init__(self, n_labels, activation_output=\"softmax\",\n                 meta_variables=None, fcl_dropout=True):\n        \"\"\" Initialization function for creating a Classifier object.\n\n        The fully connected layer and dropout option (`fcl_dropout`) utilizes a 512 unit Dense layer with 30% Dropout.\n\n        Modi for activation_output: Check out [TensorFlow.Keras doc on activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n\n        Args:\n            n_labels (int):                 Number of classes/labels (important for the last layer of classification head).\n            activation_output (str):        Activation function which is used in the last classification layer.\n            meta_variables (int):           Number of metadata variables, which should be included in the classification head.\n                                            If `None`is provided, no metadata integration block will be added to the classification head.\n            fcl_dropout (bool):             Option whether to utilize a Dense &amp; Dropout layer before the last classification layer.\n        \"\"\"\n        self.n_labels = n_labels\n        self.activation_output = activation_output\n        self.meta_variables = meta_variables\n        self.fcl_dropout = fcl_dropout\n\n    #---------------------------------------------#\n    #                Create Model                 #\n    #---------------------------------------------#\n    def build(self, model_input, model_output):\n        \"\"\" Internal function which appends the classification head.\n\n        This function will be called from inside an [Architecture][aucmedi.neural_network.architectures] `create_model()` function\n        and must return a functional Keras model.\n        The `build()` function will append a classification head to the provided Keras model.\n\n        Args:\n            model_input (tf.keras layer):       Input layer of the model.\n            model_output (tf.keras layer):      Output layer of the model.\n\n        Returns:\n            model (tf.keras model):             A functional Keras model.\n        \"\"\"\n        # Apply GlobalAveragePooling to obtain a single spatial dimensions\n        if len(model_output.shape) == 4:            # for 2D architectures\n            model_head = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model_output)\n        elif len(model_output.shape) == 5:          # for 3D architectures\n            model_head = layers.GlobalAveragePooling3D(name=\"avg_pool\")(model_output)\n        # if not model output shape 4 or 5 -&gt; it is already GlobalAveragePooled to 2 dim\n        else : model_head = model_output\n\n        # Apply optional dense &amp; dropout layer\n        if self.fcl_dropout:\n            model_head = layers.Dense(units=512)(model_head)\n            model_head = layers.Dropout(0.3)(model_head)\n\n        # Apply metadata integration block\n        if self.meta_variables is not None:\n            # Define metadata input\n            model_meta = Input(shape=(self.meta_variables,))\n\n            # Integrate metadata into classification had\n            model_head = layers.concatenate([model_head, model_meta])\n\n            # Apply additional densely-connected NN layers\n            model_head = layers.Dense(units=512, activation=\"relu\")(model_head)\n            model_head = layers.Dropout(0.3)(model_head)\n            model_head = layers.Dense(units=256, activation=\"relu\")(model_head)\n            model_head = layers.Dropout(0.3)(model_head)\n\n        # Apply classifier\n        model_head = layers.Dense(self.n_labels, name=\"preds\")(model_head)\n        # Apply activation output according to classification type\n        model_head = layers.Activation(self.activation_output, name=\"probs\")(model_head)\n\n        # Obtain input layer\n        if self.meta_variables is not None:\n            input_layer = [model_input, model_meta]\n        else : input_layer = model_input\n\n        # Create tf.keras model\n        model = Model(inputs=input_layer, outputs=model_head)\n\n        # Return ready-to-use classifier model\n        return model\n</code></pre>"},{"location":"reference/neural_network/architectures/classifier/#aucmedi.neural_network.architectures.classifier.Classifier.__init__","title":"<code>__init__(n_labels, activation_output='softmax', meta_variables=None, fcl_dropout=True)</code>","text":"<p>Initialization function for creating a Classifier object.</p> <p>The fully connected layer and dropout option (<code>fcl_dropout</code>) utilizes a 512 unit Dense layer with 30% Dropout.</p> <p>Modi for activation_output: Check out TensorFlow.Keras doc on activation functions.</p> <p>Parameters:</p> Name Type Description Default <code>n_labels</code> <code>int</code> <p>Number of classes/labels (important for the last layer of classification head).</p> required <code>activation_output</code> <code>str</code> <p>Activation function which is used in the last classification layer.</p> <code>'softmax'</code> <code>meta_variables</code> <code>int</code> <p>Number of metadata variables, which should be included in the classification head.                             If <code>None</code>is provided, no metadata integration block will be added to the classification head.</p> <code>None</code> <code>fcl_dropout</code> <code>bool</code> <p>Option whether to utilize a Dense &amp; Dropout layer before the last classification layer.</p> <code>True</code> Source code in <code>aucmedi/neural_network/architectures/classifier.py</code> <pre><code>def __init__(self, n_labels, activation_output=\"softmax\",\n             meta_variables=None, fcl_dropout=True):\n    \"\"\" Initialization function for creating a Classifier object.\n\n    The fully connected layer and dropout option (`fcl_dropout`) utilizes a 512 unit Dense layer with 30% Dropout.\n\n    Modi for activation_output: Check out [TensorFlow.Keras doc on activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n\n    Args:\n        n_labels (int):                 Number of classes/labels (important for the last layer of classification head).\n        activation_output (str):        Activation function which is used in the last classification layer.\n        meta_variables (int):           Number of metadata variables, which should be included in the classification head.\n                                        If `None`is provided, no metadata integration block will be added to the classification head.\n        fcl_dropout (bool):             Option whether to utilize a Dense &amp; Dropout layer before the last classification layer.\n    \"\"\"\n    self.n_labels = n_labels\n    self.activation_output = activation_output\n    self.meta_variables = meta_variables\n    self.fcl_dropout = fcl_dropout\n</code></pre>"},{"location":"reference/neural_network/architectures/classifier/#aucmedi.neural_network.architectures.classifier.Classifier.build","title":"<code>build(model_input, model_output)</code>","text":"<p>Internal function which appends the classification head.</p> <p>This function will be called from inside an Architecture <code>create_model()</code> function and must return a functional Keras model. The <code>build()</code> function will append a classification head to the provided Keras model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>tf.keras layer</code> <p>Input layer of the model.</p> required <code>model_output</code> <code>tf.keras layer</code> <p>Output layer of the model.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>tf.keras model</code> <p>A functional Keras model.</p> Source code in <code>aucmedi/neural_network/architectures/classifier.py</code> <pre><code>def build(self, model_input, model_output):\n    \"\"\" Internal function which appends the classification head.\n\n    This function will be called from inside an [Architecture][aucmedi.neural_network.architectures] `create_model()` function\n    and must return a functional Keras model.\n    The `build()` function will append a classification head to the provided Keras model.\n\n    Args:\n        model_input (tf.keras layer):       Input layer of the model.\n        model_output (tf.keras layer):      Output layer of the model.\n\n    Returns:\n        model (tf.keras model):             A functional Keras model.\n    \"\"\"\n    # Apply GlobalAveragePooling to obtain a single spatial dimensions\n    if len(model_output.shape) == 4:            # for 2D architectures\n        model_head = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model_output)\n    elif len(model_output.shape) == 5:          # for 3D architectures\n        model_head = layers.GlobalAveragePooling3D(name=\"avg_pool\")(model_output)\n    # if not model output shape 4 or 5 -&gt; it is already GlobalAveragePooled to 2 dim\n    else : model_head = model_output\n\n    # Apply optional dense &amp; dropout layer\n    if self.fcl_dropout:\n        model_head = layers.Dense(units=512)(model_head)\n        model_head = layers.Dropout(0.3)(model_head)\n\n    # Apply metadata integration block\n    if self.meta_variables is not None:\n        # Define metadata input\n        model_meta = Input(shape=(self.meta_variables,))\n\n        # Integrate metadata into classification had\n        model_head = layers.concatenate([model_head, model_meta])\n\n        # Apply additional densely-connected NN layers\n        model_head = layers.Dense(units=512, activation=\"relu\")(model_head)\n        model_head = layers.Dropout(0.3)(model_head)\n        model_head = layers.Dense(units=256, activation=\"relu\")(model_head)\n        model_head = layers.Dropout(0.3)(model_head)\n\n    # Apply classifier\n    model_head = layers.Dense(self.n_labels, name=\"preds\")(model_head)\n    # Apply activation output according to classification type\n    model_head = layers.Activation(self.activation_output, name=\"probs\")(model_head)\n\n    # Obtain input layer\n    if self.meta_variables is not None:\n        input_layer = [model_input, model_meta]\n    else : input_layer = model_input\n\n    # Create tf.keras model\n    model = Model(inputs=input_layer, outputs=model_head)\n\n    # Return ready-to-use classifier model\n    return model\n</code></pre>"},{"location":"reference/neural_network/architectures/image/","title":"Image","text":""},{"location":"reference/neural_network/architectures/image/#aucmedi.neural_network.architectures.image.architecture_dict","title":"<code>architecture_dict = {'Vanilla': Vanilla, 'ResNet50': ResNet50, 'ResNet101': ResNet101, 'ResNet152': ResNet152, 'ResNet50V2': ResNet50V2, 'ResNet101V2': ResNet101V2, 'ResNet152V2': ResNet152V2, 'DenseNet121': DenseNet121, 'DenseNet169': DenseNet169, 'DenseNet201': DenseNet201, 'EfficientNetB0': EfficientNetB0, 'EfficientNetB1': EfficientNetB1, 'EfficientNetB2': EfficientNetB2, 'EfficientNetB3': EfficientNetB3, 'EfficientNetB4': EfficientNetB4, 'EfficientNetB5': EfficientNetB5, 'EfficientNetB6': EfficientNetB6, 'EfficientNetB7': EfficientNetB7, 'InceptionResNetV2': InceptionResNetV2, 'InceptionV3': InceptionV3, 'MobileNet': MobileNet, 'MobileNetV2': MobileNetV2, 'NASNetMobile': NASNetMobile, 'NASNetLarge': NASNetLarge, 'VGG16': VGG16, 'VGG19': VGG19, 'Xception': Xception, 'ConvNeXtBase': ConvNeXtBase, 'ConvNeXtTiny': ConvNeXtTiny, 'ConvNeXtSmall': ConvNeXtSmall, 'ConvNeXtLarge': ConvNeXtLarge}</code>  <code>module-attribute</code>","text":"<p>Dictionary of implemented 2D Architectures Methods in AUCMEDI.</p> <p>The base key (str) or an initialized Architecture can be passed to the NeuralNetwork class as <code>architecture</code> parameter.</p> Example Recommended via NeuralNetwork class<pre><code>my_model = NeuralNetwork(n_labels=4, channels=3, architecture=\"2D.Xception\",\n                          input_shape(512, 512), activation_output=\"softmax\")\n</code></pre> Manual via architecture_dict import<pre><code>from aucmedi.neural_network.architectures import Classifier, architecture_dict\n\nclassification_head = Classifier(n_labels=4, activation_output=\"softmax\")\nmy_arch = architecture_dict[\"2D.Xception\"](classification_head,\n                                           channels=3, input_shape=(512,512))\n\nmy_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n</code></pre> Manual via module import<pre><code>from aucmedi.neural_network.architectures import Classifier\nfrom aucmedi.neural_network.architectures.image import Xception\n\nclassification_head = Classifier(n_labels=4, activation_output=\"softmax\")\nmy_arch = Xception(classification_head,\n                                channels=3, input_shape=(512,512))\n\nmy_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n</code></pre> Warning <p>If passing an architecture key to the NeuralNetwork class, be aware that you have to add \"2D.\" in front of it.</p> <p>For example: <pre><code># for the image architecture \"ResNet101\"\narchitecture=\"2D.ResNet101\"\n</code></pre></p> <p>Architectures are based on the abstract base class aucmedi.neural_network.architectures.arch_base.Architecture_Base.</p>"},{"location":"reference/neural_network/architectures/image/#aucmedi.neural_network.architectures.image.supported_standardize_mode","title":"<code>supported_standardize_mode = {'Vanilla': 'z-score', 'ResNet50': 'caffe', 'ResNet101': 'caffe', 'ResNet152': 'caffe', 'ResNet50V2': 'tf', 'ResNet101V2': 'tf', 'ResNet152V2': 'tf', 'DenseNet121': 'torch', 'DenseNet169': 'torch', 'DenseNet201': 'torch', 'EfficientNetB0': 'caffe', 'EfficientNetB1': 'caffe', 'EfficientNetB2': 'caffe', 'EfficientNetB3': 'caffe', 'EfficientNetB4': 'caffe', 'EfficientNetB5': 'caffe', 'EfficientNetB6': 'caffe', 'EfficientNetB7': 'caffe', 'InceptionResNetV2': 'tf', 'InceptionV3': 'tf', 'MobileNet': 'tf', 'MobileNetV2': 'tf', 'NASNetMobile': 'tf', 'NASNetLarge': 'tf', 'VGG16': 'caffe', 'VGG19': 'caffe', 'Xception': 'tf', 'ConvNeXtBase': None, 'ConvNeXtTiny': None, 'ConvNeXtSmall': None, 'ConvNeXtLarge': None}</code>  <code>module-attribute</code>","text":"<p>Dictionary of recommended Standardize techniques for 2D Architectures Methods in AUCMEDI.</p> <p>The base key (str) can be passed to the DataGenerator as <code>standardize_mode</code> parameter.</p> Info <p>If training a new model from scratch, any Standardize technique can be used at will.  However, if training via transfer learning, it is required to use the recommended Standardize technique!</p> Example Recommended via the NeuralNetwork class<pre><code>my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"2D.DenseNet121\")\n\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=my_model.meta_input,                  # (224, 224)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Manual via supported_standardize_mode import<pre><code>from aucmedi.neural_network.architectures import supported_standardize_mode\nsf_norm = supported_standardize_mode[\"2D.DenseNet121\"]\n\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=(224, 224),                           # (224, 224)\n                      standardize_mode=sf_norm)                    # \"torch\"\n</code></pre> Warning <p>If using an architecture key for the supported_standardize_mode dictionary, be aware that you have to add \"2D.\" in front of it.</p> <p>For example: <pre><code># for the image architecture \"ResNet101\"\nfrom aucmedi.neural_network.architectures import supported_standardize_mode\nsf_norm = supported_standardize_mode[\"2D.ResNet101\"]\n</code></pre></p>"},{"location":"reference/neural_network/architectures/image/convnext_base/","title":"Convnext base","text":"<p>The classification variant of the ConvNeXt Base architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ConvNeXtBase\" Input_shape (224, 224) Standardization None <p>Recommended alternative <code>Input_shape</code> is 384x384 pixels.</p> <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>https://www.tensorflow.org/api_docs/python/tf/keras/applications/convnext </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/image/convnext_large/","title":"Convnext large","text":"<p>The classification variant of the ConvNeXt Large architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ConvNeXtLarge\" Input_shape (384x384) Standardization None <p>Recommended alternative <code>Input_shape</code> is 224x224 pixels.</p> <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>https://www.tensorflow.org/api_docs/python/tf/keras/applications/convnext </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/image/convnext_small/","title":"Convnext small","text":"<p>The classification variant of the ConvNeXt Small architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ConvNeXtSmall\" Input_shape (224, 224) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>https://www.tensorflow.org/api_docs/python/tf/keras/applications/convnext </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/image/convnext_tiny/","title":"Convnext tiny","text":"<p>The classification variant of the ConvNeXt Tiny architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ConvNeXtTiny\" Input_shape (224, 224) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>https://www.tensorflow.org/api_docs/python/tf/keras/applications/convnext </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/image/densenet121/","title":"Densenet121","text":"<p>The classification variant of the DenseNet121 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.DenseNet121\" Input_shape (224, 224) Standardization \"torch\" Reference - Implementation <p>https://keras.io/applications/#densenet </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/image/densenet169/","title":"Densenet169","text":"<p>The classification variant of the DenseNet169 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.DenseNet169\" Input_shape (224, 224) Standardization \"torch\" Reference - Implementation <p>https://keras.io/applications/#densenet </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/image/densenet201/","title":"Densenet201","text":"<p>The classification variant of the DenseNet201 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.DenseNet201\" Input_shape (224, 224) Standardization \"torch\" Reference - Implementation <p>https://keras.io/applications/#densenet </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb0/","title":"Efficientnetb0","text":"<p>The classification variant of the EfficientNetB0 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB0\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb1/","title":"Efficientnetb1","text":"<p>The classification variant of the EfficientNetB1 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB1\" Input_shape (240, 240) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb2/","title":"Efficientnetb2","text":"<p>The classification variant of the EfficientNetB2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB2\" Input_shape (260, 260) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb3/","title":"Efficientnetb3","text":"<p>The classification variant of the EfficientNetB3 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB3\" Input_shape (300, 300) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb4/","title":"Efficientnetb4","text":"<p>The classification variant of the EfficientNetB4 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB4\" Input_shape (380, 380) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb5/","title":"Efficientnetb5","text":"<p>The classification variant of the EfficientNetB5 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB5\" Input_shape (456, 456) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb6/","title":"Efficientnetb6","text":"<p>The classification variant of the EfficientNetB6 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB6\" Input_shape (528, 528) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/efficientnetb7/","title":"Efficientnetb7","text":"<p>The classification variant of the EfficientNetB7 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.EfficientNetB7\" Input_shape (600, 600) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/efficientnet/ </p> Reference - Publication <p>Mingxing Tan, Quoc V. Le. 28 May 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.  https://arxiv.org/abs/1905.11946</p>"},{"location":"reference/neural_network/architectures/image/inceptionresnetv2/","title":"Inceptionresnetv2","text":"<p>The classification variant of the InceptionResNetV2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.InceptionResNetV2\" Input_shape (299, 299) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/inceptionresnetv2/ </p> Reference - Publication <p>Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. 23 Feb 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.  https://arxiv.org/abs/1602.07261</p>"},{"location":"reference/neural_network/architectures/image/inceptionv3/","title":"Inceptionv3","text":"<p>The classification variant of the InceptionV3 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.InceptionV3\" Input_shape (299, 299) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/inceptionv3/ </p> Reference - Publication <p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna. 2 Dec 2015. Rethinking the Inception Architecture for Computer Vision.  https://arxiv.org/abs/1512.00567</p>"},{"location":"reference/neural_network/architectures/image/mobilenet/","title":"Mobilenet","text":"<p>The classification variant of the MobileNet architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.MobileNet\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/mobilenet/ </p> Reference - Publication <p>Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. 17 Apr 2017. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.  https://arxiv.org/abs/1704.04861</p>"},{"location":"reference/neural_network/architectures/image/mobilenetv2/","title":"Mobilenetv2","text":"<p>The classification variant of the MobileNetV2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.MobileNetV2\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/mobilenet/ </p> Reference - Publication <p>Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. 13 Jan 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks.  https://arxiv.org/abs/1801.04381</p>"},{"location":"reference/neural_network/architectures/image/nasnetlarge/","title":"Nasnetlarge","text":"<p>The classification variant of the NASNetLarge architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.NASNetLarge\" Input_shape (331, 331) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/nasnet/ </p> Reference - Publication <p>Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le. 10 Dec 2015. Learning Transferable Architectures for Scalable Image Recognition.  https://arxiv.org/abs/1707.07012</p>"},{"location":"reference/neural_network/architectures/image/nasnetmobile/","title":"Nasnetmobile","text":"<p>The classification variant of the NASNetMobile architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.NASNetMobile\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/nasnet/ </p> Reference - Publication <p>Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le. 10 Dec 2015. Learning Transferable Architectures for Scalable Image Recognition.  https://arxiv.org/abs/1707.07012</p>"},{"location":"reference/neural_network/architectures/image/resnet101/","title":"Resnet101","text":"<p>The classification variant of the ResNet101 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet101\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/image/resnet101v2/","title":"Resnet101v2","text":"<p>The classification variant of the ResNet101V2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet101V2\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 16 Mar 2016. Identity Mappings in Deep Residual Networks.  https://arxiv.org/abs/1603.05027</p>"},{"location":"reference/neural_network/architectures/image/resnet152/","title":"Resnet152","text":"<p>The classification variant of the ResNet152 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet152\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/image/resnet152v2/","title":"Resnet152v2","text":"<p>The classification variant of the ResNet152V2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet152V2\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 16 Mar 2016. Identity Mappings in Deep Residual Networks.  https://arxiv.org/abs/1603.05027</p>"},{"location":"reference/neural_network/architectures/image/resnet50/","title":"Resnet50","text":"<p>The classification variant of the ResNet50 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet50\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/image/resnet50v2/","title":"Resnet50v2","text":"<p>The classification variant of the ResNet50V2 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNet50V2\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/resnet/ </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 16 Mar 2016. Identity Mappings in Deep Residual Networks.  https://arxiv.org/abs/1603.05027</p>"},{"location":"reference/neural_network/architectures/image/resnext101/","title":"Resnext101","text":"<p>The classification variant of the ResNeXt101 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNeXt101\" Input_shape (224, 224) Standardization \"torch\" Reference - Implementation <p>https://github.com/keras-team/keras-applications </p> Reference - Publication <p>Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He. 16 Nov 2016. Aggregated Residual Transformations for Deep Neural Networks.  https://arxiv.org/abs/1611.05431</p>"},{"location":"reference/neural_network/architectures/image/resnext50/","title":"Resnext50","text":"<p>The classification variant of the ResNeXt50 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.ResNeXt50\" Input_shape (224, 224) Standardization \"torch\" Reference - Implementation <p>https://github.com/keras-team/keras-applications </p> Reference - Publication <p>Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He. 16 Nov 2016. Aggregated Residual Transformations for Deep Neural Networks.  https://arxiv.org/abs/1611.05431</p>"},{"location":"reference/neural_network/architectures/image/vanilla/","title":"Vanilla","text":"<p>The classification variant of the Vanilla architecture.</p> <p>No intensive hardware requirements, which makes it ideal for debugging.</p> Architecture Variable Value Key in architecture_dict \"2D.Vanilla\" Input_shape (224, 224) Standardization \"z-score\" Reference - Implementation <p>https://github.com/wanghsinwei/isic-2019/ </p>"},{"location":"reference/neural_network/architectures/image/vgg16/","title":"Vgg16","text":"<p>The classification variant of the VGG16 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.VGG16\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/vgg/ </p> Reference - Publication <p>Karen Simonyan, Andrew Zisserman. 04 Sep 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.  https://arxiv.org/abs/1409.1556</p>"},{"location":"reference/neural_network/architectures/image/vgg19/","title":"Vgg19","text":"<p>The classification variant of the VGG19 architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.VGG19\" Input_shape (224, 224) Standardization \"caffe\" Reference - Implementation <p>https://keras.io/api/applications/vgg/ </p> Reference - Publication <p>Karen Simonyan, Andrew Zisserman. 04 Sep 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.  https://arxiv.org/abs/1409.1556</p>"},{"location":"reference/neural_network/architectures/image/vit_b16/","title":"Vit b16","text":"<p>The classification variant of the Vision Transformer (ViT) version B16 architecture.</p> <p>Warning</p> <p>The ViT architectures only work for RGB encoding (channel size = 3).</p> Architecture Variable Value Key in architecture_dict \"2D.ViT_B16\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>Fausto Morales; https://github.com/faustomorales https://github.com/faustomorales/vit-keras </p> <p>Vo Van Tu; https://github.com/tuvovan https://github.com/tuvovan/Vision_Transformer_Keras </p> <p>Original: Google Research  https://github.com/google-research/vision_transformer </p> Reference - Publication <pre><code>@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n\n@article{tolstikhin2021mixer,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\n@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n\n@article{chen2021outperform,\n  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},\n  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},\n  journal={arXiv preprint arXiv:2106.01548},\n  year={2021},\n}\n\n@article{zhai2022lit,\n  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},\n  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={CVPR},\n  year={2022}\n}\n</code></pre>"},{"location":"reference/neural_network/architectures/image/vit_b32/","title":"Vit b32","text":"<p>The classification variant of the Vision Transformer (ViT) version B32 architecture.</p> <p>Warning</p> <p>The ViT architectures only work for RGB encoding (channel size = 3).</p> Architecture Variable Value Key in architecture_dict \"2D.ViT_B32\" Input_shape (224, 224) Standardization \"tf\" Reference - Implementation <p>Fausto Morales; https://github.com/faustomorales https://github.com/faustomorales/vit-keras </p> <p>Vo Van Tu; https://github.com/tuvovan https://github.com/tuvovan/Vision_Transformer_Keras </p> <p>Original: Google Research  https://github.com/google-research/vision_transformer </p> Reference - Publication <pre><code>@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n\n@article{tolstikhin2021mixer,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\n@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n\n@article{chen2021outperform,\n  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},\n  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},\n  journal={arXiv preprint arXiv:2106.01548},\n  year={2021},\n}\n\n@article{zhai2022lit,\n  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},\n  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={CVPR},\n  year={2022}\n}\n</code></pre>"},{"location":"reference/neural_network/architectures/image/vit_l16/","title":"Vit l16","text":"<p>The classification variant of the Vision Transformer (ViT) version L16 architecture.</p> <p>Warning</p> <p>The ViT architectures only work for RGB encoding (channel size = 3).</p> Architecture Variable Value Key in architecture_dict \"2D.ViT_L16\" Input_shape (384, 384) Standardization \"tf\" Reference - Implementation <p>Fausto Morales; https://github.com/faustomorales https://github.com/faustomorales/vit-keras </p> <p>Vo Van Tu; https://github.com/tuvovan https://github.com/tuvovan/Vision_Transformer_Keras </p> <p>Original: Google Research  https://github.com/google-research/vision_transformer </p> Reference - Publication <pre><code>@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n\n@article{tolstikhin2021mixer,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\n@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n\n@article{chen2021outperform,\n  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},\n  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},\n  journal={arXiv preprint arXiv:2106.01548},\n  year={2021},\n}\n\n@article{zhai2022lit,\n  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},\n  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={CVPR},\n  year={2022}\n}\n</code></pre>"},{"location":"reference/neural_network/architectures/image/vit_l32/","title":"Vit l32","text":"<p>The classification variant of the Vision Transformer (ViT) version L32 architecture.</p> <p>Warning</p> <p>The ViT architectures only work for RGB encoding (channel size = 3).</p> Architecture Variable Value Key in architecture_dict \"2D.ViT_L32\" Input_shape (384, 384) Standardization \"tf\" Reference - Implementation <p>Fausto Morales; https://github.com/faustomorales https://github.com/faustomorales/vit-keras </p> <p>Vo Van Tu; https://github.com/tuvovan https://github.com/tuvovan/Vision_Transformer_Keras </p> <p>Original: Google Research  https://github.com/google-research/vision_transformer </p> Reference - Publication <pre><code>@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n\n@article{tolstikhin2021mixer,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\n@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n\n@article{chen2021outperform,\n  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},\n  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},\n  journal={arXiv preprint arXiv:2106.01548},\n  year={2021},\n}\n\n@article{zhai2022lit,\n  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},\n  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={CVPR},\n  year={2022}\n}\n</code></pre>"},{"location":"reference/neural_network/architectures/image/xception/","title":"Xception","text":"<p>The classification variant of the Xception architecture.</p> Architecture Variable Value Key in architecture_dict \"2D.Xception\" Input_shape (299, 299) Standardization \"tf\" Reference - Implementation <p>https://keras.io/api/applications/xception/ </p> Reference - Publication <p>Fran\u00e7ois Chollet. 07 Oct 2016. Xception: Deep Learning with Depthwise Separable Convolutions.  https://arxiv.org/abs/1610.02357</p>"},{"location":"reference/neural_network/architectures/volume/","title":"Volume","text":""},{"location":"reference/neural_network/architectures/volume/#aucmedi.neural_network.architectures.volume.architecture_dict","title":"<code>architecture_dict = {'Vanilla': Vanilla, 'DenseNet121': DenseNet121, 'DenseNet169': DenseNet169, 'DenseNet201': DenseNet201, 'ResNet18': ResNet18, 'ResNet34': ResNet34, 'ResNet50': ResNet50, 'ResNet101': ResNet101, 'ResNet152': ResNet152, 'ResNeXt50': ResNeXt50, 'ResNeXt101': ResNeXt101, 'MobileNet': MobileNet, 'MobileNetV2': MobileNetV2, 'VGG16': VGG16, 'VGG19': VGG19, 'ConvNeXtTiny': ConvNeXtTiny, 'ConvNeXtSmall': ConvNeXtSmall, 'ConvNeXtBase': ConvNeXtBase, 'ConvNeXtLarge': ConvNeXtLarge}</code>  <code>module-attribute</code>","text":"<p>Dictionary of implemented 3D Architectures Methods in AUCMEDI.</p> <p>The base key (str) or an initialized Architecture can be passed to the NeuralNetwork class as <code>architecture</code> parameter.</p> Example Recommended via NeuralNetwork class<pre><code>my_model = NeuralNetwork(n_labels=4, channels=1, architecture=\"3D.ResNet50\",\n                          input_shape(128,128,128), activation_output=\"softmax\")\n</code></pre> Manual via architecture_dict import<pre><code>from aucmedi.neural_network.architectures import Classifier, architecture_dict\n\nclassification_head = Classifier(n_labels=4, activation_output=\"softmax\")\nmy_arch = architecture_dict[\"3D.ResNet50\"](classification_head,\n                                           channels=1, input_shape=(128,128,128))\nmy_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n</code></pre> Manual via module import<pre><code>from aucmedi.neural_network.architectures import Classifier\nfrom aucmedi.neural_network.architectures.volume import ResNet50\n\nclassification_head = Classifier(n_labels=4, activation_output=\"softmax\")\nmy_arch = ResNet50(classification_head,\n                                channels=1, input_shape=(128,128,128))\n\nmy_model = NeuralNetwork(n_labels=None, channels=None, architecture=my_arch)\n</code></pre> Warning <p>If passing an architecture key to the NeuralNetwork class, be aware that you have to add \"3D.\" in front of it.</p> <p>For example: <pre><code># for the volume architecture \"ResNeXt101\"\narchitecture=\"3D.ResNeXt101\"\n</code></pre></p> <p>Architectures are based on the abstract base class aucmedi.neural_network.architectures.arch_base.Architecture_Base.</p>"},{"location":"reference/neural_network/architectures/volume/#aucmedi.neural_network.architectures.volume.supported_standardize_mode","title":"<code>supported_standardize_mode = {'Vanilla': 'z-score', 'DenseNet121': 'torch', 'DenseNet169': 'torch', 'DenseNet201': 'torch', 'ResNet18': 'grayscale', 'ResNet34': 'grayscale', 'ResNet50': 'grayscale', 'ResNet101': 'grayscale', 'ResNet152': 'grayscale', 'ResNeXt50': 'grayscale', 'ResNeXt101': 'grayscale', 'MobileNet': 'tf', 'MobileNetV2': 'tf', 'VGG16': 'caffe', 'VGG19': 'caffe', 'ConvNeXtTiny': None, 'ConvNeXtSmall': None, 'ConvNeXtBase': None, 'ConvNeXtLarge': None}</code>  <code>module-attribute</code>","text":"<p>Dictionary of recommended Standardize techniques for 3D Architectures Methods in AUCMEDI.</p> <p>The base key (str) can be passed to the DataGenerator as <code>standardize_mode</code> parameter.</p> Info <p>If training a new model from scratch, any Standardize technique can be used at will.  However, if training via transfer learning, it is required to use the recommended Standardize technique!</p> Example Recommended via the NeuralNetwork class<pre><code>my_model = NeuralNetwork(n_labels=8, channels=3, architecture=\"3D.DenseNet121\")\n\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=my_model.meta_input,                  # (64, 64, 64)\n                      standardize_mode=my_model.meta_standardize)  # \"torch\"\n</code></pre> Manual via supported_standardize_mode import<pre><code>from aucmedi.neural_network.architectures import supported_standardize_mode\nsf_norm = supported_standardize_mode[\"3D.DenseNet121\"]\n\nmy_dg = DataGenerator(samples, \"images_dir/\", labels=None,\n                      resize=(64, 64, 64),                         # (64, 64, 64)\n                      standardize_mode=sf_norm)                    # \"torch\"\n</code></pre> Warning <p>If using an architecture key for the supported_standardize_mode dictionary, be aware that you have to add \"3D.\" in front of it.</p> <p>For example: <pre><code># for the volume architecture \"ResNeXt101\"\nfrom aucmedi.neural_network.architectures import supported_standardize_mode\nsf_norm = supported_standardize_mode[\"3D.ResNeXt101\"]\n</code></pre></p>"},{"location":"reference/neural_network/architectures/volume/convnext_base/","title":"Convnext base","text":"<p>The classification variant of the ConvNeXt Base architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ConvNeXtBase\" Input_shape (64, 64, 64) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/volume/convnext_large/","title":"Convnext large","text":"<p>The classification variant of the ConvNeXt Large architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ConvNeXtLarge\" Input_shape (64, 64, 64) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/volume/convnext_small/","title":"Convnext small","text":"<p>The classification variant of the ConvNeXt Small architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ConvNeXtSmall\" Input_shape (64, 64, 64) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/volume/convnext_tiny/","title":"Convnext tiny","text":"<p>The classification variant of the ConvNeXt Tiny architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ConvNeXtTiny\" Input_shape (64, 64, 64) Standardization None <p>Warning</p> <p>ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with values in the [0-255] range.  Standardization is applied inside the architecture.</p> Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 10 Jan 2022. A ConvNet for the 2020s.  https://arxiv.org/abs/2201.03545</p>"},{"location":"reference/neural_network/architectures/volume/densenet121/","title":"Densenet121","text":"<p>The classification variant of the DenseNet121 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.DenseNet121\" Input_shape (64, 64, 64) Standardization \"torch\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/volume/densenet169/","title":"Densenet169","text":"<p>The classification variant of the DenseNet169 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.DenseNet169\" Input_shape (64, 64, 64) Standardization \"torch\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/volume/densenet201/","title":"Densenet201","text":"<p>The classification variant of the DenseNet201 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.DenseNet201\" Input_shape (64, 64, 64) Standardization \"torch\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. 25 Aug 2016. Densely Connected Convolutional Networks.  https://arxiv.org/abs/1608.06993</p>"},{"location":"reference/neural_network/architectures/volume/mobilenet/","title":"Mobilenet","text":"<p>The classification variant of the MobileNet architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.MobileNet\" Input_shape (64, 64, 64) Standardization \"tf\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. 17 Apr 2017. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.  https://arxiv.org/abs/1704.04861</p>"},{"location":"reference/neural_network/architectures/volume/mobilenetv2/","title":"Mobilenetv2","text":"<p>The classification variant of the MobileNetV2 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.MobileNetV2\" Input_shape (64, 64, 64) Standardization \"tf\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. 13 Jan 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks.  https://arxiv.org/abs/1801.04381</p>"},{"location":"reference/neural_network/architectures/volume/resnet101/","title":"Resnet101","text":"<p>The classification variant of the ResNet101 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNet101\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/volume/resnet152/","title":"Resnet152","text":"<p>The classification variant of the ResNet152 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNet152\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/volume/resnet18/","title":"Resnet18","text":"<p>The classification variant of the ResNet18 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNet18\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/volume/resnet34/","title":"Resnet34","text":"<p>The classification variant of the ResNet34 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNet34\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/volume/resnet50/","title":"Resnet50","text":"<p>The classification variant of the ResNet50 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNet50\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 10 Dec 2015. Deep Residual Learning for Image Recognition.  https://arxiv.org/abs/1512.03385</p>"},{"location":"reference/neural_network/architectures/volume/resnext101/","title":"Resnext101","text":"<p>The classification variant of the ResNeXt101 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNeXt101\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He. 16 Nov 2016. Aggregated Residual Transformations for Deep Neural Networks.  https://arxiv.org/abs/1611.05431</p>"},{"location":"reference/neural_network/architectures/volume/resnext50/","title":"Resnext50","text":"<p>The classification variant of the ResNeXt50 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.ResNeXt50\" Input_shape (64, 64, 64) Standardization \"grayscale\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He. 16 Nov 2016. Aggregated Residual Transformations for Deep Neural Networks.  https://arxiv.org/abs/1611.05431</p>"},{"location":"reference/neural_network/architectures/volume/vanilla/","title":"Vanilla","text":"<p>The classification variant of the Vanilla architecture.</p> <p>No intensive hardware requirements, which makes it ideal for debugging.</p> Architecture Variable Value Key in architecture_dict \"3D.Vanilla\" Input_shape (128, 128, 128) Standardization \"z-score\" Reference - Implementation <p>https://github.com/wanghsinwei/isic-2019/ </p>"},{"location":"reference/neural_network/architectures/volume/vgg16/","title":"Vgg16","text":"<p>The classification variant of the VGG16 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.VGG16\" Input_shape (64, 64, 64) Standardization \"caffe\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Karen Simonyan, Andrew Zisserman. 04 Sep 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.  https://arxiv.org/abs/1409.1556</p>"},{"location":"reference/neural_network/architectures/volume/vgg19/","title":"Vgg19","text":"<p>The classification variant of the VGG19 architecture.</p> Architecture Variable Value Key in architecture_dict \"3D.VGG19\" Input_shape (64, 64, 64) Standardization \"caffe\" Reference - Implementation <p>Solovyev, Roman &amp; Kalinin, Alexandr &amp; Gabruseva, Tatiana. (2021).  3D Convolutional Neural Networks for Stalled Brain Capillary Detection.  https://github.com/ZFTurbo/classification_models_3D </p> Reference - Publication <p>Karen Simonyan, Andrew Zisserman. 04 Sep 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.  https://arxiv.org/abs/1409.1556</p>"},{"location":"reference/sampling/","title":"Sampling","text":"<p>Interface for k-fold and percentage-split sampling functions in AUCMEDI.</p> Function Description aucmedi.sampling.split Simple wrapper function for calling percentage split sampling functions. aucmedi.sampling.kfold Simple wrapper function for calling k-fold cross-validation sampling functions. Recommended Import <pre><code># Import sampling\nfrom aucmedi.sampling import sampling_split, sampling_kfold\n\n# Run selected sampling\nds_ps = sampling_split(samples, labels, sampling=[0.7, 0.25, 0.05])\nds_ks = sampling_kfold(samples, labels, n_splits=3)\n</code></pre>"},{"location":"reference/sampling/iterative/","title":"Iterative","text":"<p>Internal classes to allow iterative stratification in percentage-split and     k-fold cross-validation for multi-label sampling.</p> <p>Use the corresponding core functions from aucmedi.sampling.split and aucmedi.sampling.kfold with the parameter <code>iterative=True</code>.</p> Personal Note <p>This code originates from https://github.com/trent-b.</p> <p>If you are reading this, leave trent-b a star on his GitHub! :)   His code is open-source, really well written and structured.</p> Reference - Implementation <p>Author: trend-b  GitHub Profile: https://github.com/trent-b https://github.com/trent-b/iterative-stratification </p> Reference - Publication <p>Sechidis K., Tsoumakas G., Vlahavas I. 2011. On the Stratification of Multi-Label Data. Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin, Heidelberg. Aristotle University of Thessaloniki.  https://link.springer.com/chapter/10.1007/978-3-642-23808-6_10</p>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedKFold","title":"<code>MultilabelStratifiedKFold</code>","text":"<p>         Bases: <code>_BaseKFold</code></p> <p>Multilabel stratified K-Folds cross-validator.</p> <p>Provides train/test indices to split multilabel data into train/test sets. This cross-validation object is a variation of KFold that returns stratified folds for multilabel data. The folds are made by preserving the percentage of samples for each label.</p> Example <pre><code>&gt;&gt;&gt; from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n&gt;&gt;&gt; y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n&gt;&gt;&gt; mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n&gt;&gt;&gt; mskf.get_n_splits(X, y)\n2\n&gt;&gt;&gt; print(mskf)  # doctest: +NORMALIZE_WHITESPACE\nMultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n&gt;&gt;&gt; for train_index, test_index in mskf.split(X, y):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [0 3 4 6] TEST: [1 2 5 7]\nTRAIN: [1 2 5 7] TEST: [0 3 4 6]\n</code></pre> Note <p>Train and test sizes may be slightly different in each fold.</p> See also <p>RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold n times.</p> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>class MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator.\n\n    Provides train/test indices to split multilabel data into train/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n\n    ??? example\n        ```python\n        &gt;&gt;&gt; from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n        &gt;&gt;&gt; y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n        &gt;&gt;&gt; mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n        &gt;&gt;&gt; mskf.get_n_splits(X, y)\n        2\n        &gt;&gt;&gt; print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n        MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n        &gt;&gt;&gt; for train_index, test_index in mskf.split(X, y):\n        ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        ...    X_train, X_test = X[train_index], X[test_index]\n        ...    y_train, y_test = y[train_index], y[test_index]\n        TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n        TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n        ```\n\n    ???+ note\n        Train and test sizes may be slightly different in each fold.\n\n    ???+ note \"See also\"\n        RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n        n times.\n\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        \"\"\"\n        Args:\n            n_splits (int, default=3):      Number of folds. Must be at least 2.\n            shuffle (boolean, optional):    Whether to shuffle each stratification of the data before splitting\n                                            into batches.\n            random_state (int, RandomState instance or None, optional, default=None): If int, random_state is the\n                                            seed used by the random number generator;\n                                            If RandomState instance, random_state is the random number generator;\n                                            If None, the random number generator is the RandomState instance used\n                                            by `np.random`. Unlike StratifiedKFold that only uses random_state\n                                            when ``shuffle`` == True, this multilabel implementation\n                                            always uses the random_state since the iterative stratification\n                                            algorithm breaks ties randomly.\n        \"\"\"\n        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 / self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\" Generate indices to split data into training and test set.\n\n        ???+ note\n            Randomized CV splitters may return different results for each call of\n            split. You can make the results identical by setting ``random_state``\n            to an integer.: train-&gt;     The training set indices for that split.\n\n        Args:\n            X (array-like, shape (n_samples, n_features) ): Training data, where n_samples is the number of samples\n                                                            and n_features is the number of features.\n                                                            Note that providing ``y`` is sufficient to generate the splits and\n                                                            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n                                                            ``X`` instead of actual training data.\n            y (array-like, shape (n_samples, n_labels) ):   The target variable for supervised learning problems.\n                                                            Multilabel stratification is done based on the y labels.\n            groups (object, optional):                      Always ignored, exists for compatibility.\n\n        Returns:\n          train (numpy.ndarray):        The training set indices for that split.\n          test (numpy.ndarray):         The testing set indices for that split.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedKFold.__init__","title":"<code>__init__(n_splits=3, shuffle=False, random_state=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int, default=3</code> <p>Number of folds. Must be at least 2.</p> <code>3</code> <code>shuffle</code> <code>boolean</code> <p>Whether to shuffle each stratification of the data before splitting                             into batches.</p> <code>False</code> <code>random_state</code> <code>int, RandomState instance or None, optional, default=None</code> <p>If int, random_state is the                             seed used by the random number generator;                             If RandomState instance, random_state is the random number generator;                             If None, the random number generator is the RandomState instance used                             by <code>np.random</code>. Unlike StratifiedKFold that only uses random_state                             when <code>shuffle</code> == True, this multilabel implementation                             always uses the random_state since the iterative stratification                             algorithm breaks ties randomly.</p> <code>None</code> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>def __init__(self, n_splits=3, shuffle=False, random_state=None):\n    \"\"\"\n    Args:\n        n_splits (int, default=3):      Number of folds. Must be at least 2.\n        shuffle (boolean, optional):    Whether to shuffle each stratification of the data before splitting\n                                        into batches.\n        random_state (int, RandomState instance or None, optional, default=None): If int, random_state is the\n                                        seed used by the random number generator;\n                                        If RandomState instance, random_state is the random number generator;\n                                        If None, the random number generator is the RandomState instance used\n                                        by `np.random`. Unlike StratifiedKFold that only uses random_state\n                                        when ``shuffle`` == True, this multilabel implementation\n                                        always uses the random_state since the iterative stratification\n                                        algorithm breaks ties randomly.\n    \"\"\"\n    super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedKFold.split","title":"<code>split(X, y, groups=None)</code>","text":"<p>Generate indices to split data into training and test set.</p> Note <p>Randomized CV splitters may return different results for each call of split. You can make the results identical by setting <code>random_state</code> to an integer.: train-&gt;     The training set indices for that split.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape (n_samples, n_features) </code> <p>Training data, where n_samples is the number of samples                                             and n_features is the number of features.                                             Note that providing <code>y</code> is sufficient to generate the splits and                                             hence <code>np.zeros(n_samples)</code> may be used as a placeholder for                                             <code>X</code> instead of actual training data.</p> required <code>y</code> <code>array-like, shape (n_samples, n_labels) </code> <p>The target variable for supervised learning problems.                                             Multilabel stratification is done based on the y labels.</p> required <code>groups</code> <code>object</code> <p>Always ignored, exists for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>train</code> <code>numpy.ndarray</code> <p>The training set indices for that split.</p> <code>test</code> <code>numpy.ndarray</code> <p>The testing set indices for that split.</p> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>def split(self, X, y, groups=None):\n    \"\"\" Generate indices to split data into training and test set.\n\n    ???+ note\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.: train-&gt;     The training set indices for that split.\n\n    Args:\n        X (array-like, shape (n_samples, n_features) ): Training data, where n_samples is the number of samples\n                                                        and n_features is the number of features.\n                                                        Note that providing ``y`` is sufficient to generate the splits and\n                                                        hence ``np.zeros(n_samples)`` may be used as a placeholder for\n                                                        ``X`` instead of actual training data.\n        y (array-like, shape (n_samples, n_labels) ):   The target variable for supervised learning problems.\n                                                        Multilabel stratification is done based on the y labels.\n        groups (object, optional):                      Always ignored, exists for compatibility.\n\n    Returns:\n      train (numpy.ndarray):        The training set indices for that split.\n      test (numpy.ndarray):         The testing set indices for that split.\n    \"\"\"\n    y = check_array(y, ensure_2d=False, dtype=None)\n    return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedShuffleSplit","title":"<code>MultilabelStratifiedShuffleSplit</code>","text":"<p>         Bases: <code>BaseShuffleSplit</code></p> <p>Multilabel Stratified ShuffleSplit cross-validator.</p> <p>Provides train/test indices to split data into train/test sets. This cross-validation object is a merge of MultilabelStratifiedKFold and ShuffleSplit, which returns stratified randomized folds for multilabel data. The folds are made by preserving the percentage of each label. Note: like the ShuffleSplit strategy, multilabel stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.</p> Example <pre><code>&gt;&gt;&gt; from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n&gt;&gt;&gt; y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n&gt;&gt;&gt; msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n...    random_state=0)\n&gt;&gt;&gt; msss.get_n_splits(X, y)\n3\n&gt;&gt;&gt; print(mss)       # doctest: +ELLIPSIS\nMultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                 train_size=None)\n&gt;&gt;&gt; for train_index, test_index in msss.split(X, y):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 2 5 7] TEST: [0 3 4 6]\nTRAIN: [2 3 6 7] TEST: [0 1 4 5]\nTRAIN: [1 2 5 6] TEST: [0 3 4 7]\n</code></pre> Note <p>Train and test sizes may be slightly different from desired due to the preference of stratification over perfectly sized folds.</p> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Multilabel Stratified ShuffleSplit cross-validator.\n\n    Provides train/test indices to split data into train/test sets.\n    This cross-validation object is a merge of MultilabelStratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds for multilabel\n    data. The folds are made by preserving the percentage of each label.\n    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    ??? example\n        ```python\n        &gt;&gt;&gt; from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n        &gt;&gt;&gt; y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n        &gt;&gt;&gt; msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n        ...    random_state=0)\n        &gt;&gt;&gt; msss.get_n_splits(X, y)\n        3\n        &gt;&gt;&gt; print(mss)       # doctest: +ELLIPSIS\n        MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                         train_size=None)\n        &gt;&gt;&gt; for train_index, test_index in msss.split(X, y):\n        ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        ...    X_train, X_test = X[train_index], X[test_index]\n        ...    y_train, y_test = y[train_index], y[test_index]\n        TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n        TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n        TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n        ```\n\n    ???+ note\n        Train and test sizes may be slightly different from desired due to the\n        preference of stratification over perfectly sized folds.\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n                 random_state=None):\n        \"\"\"\n        Args:\n            n_splits (int):                         Number of re-shuffling &amp; splitting iterations.\n            test_size (float, int, None, optional): If float, should be between 0.0 and 1.0 and represent the proportion\n                                                    of the dataset to include in the test split. If int, represents the\n                                                    absolute number of test samples. If None, the value is set to the\n                                                    complement of the train size. By default, the value is set to 0.1.\n                                                    The default will change in version 0.21. It will remain 0.1 only\n                                                    if ``train_size`` is unspecified, otherwise it will complement\n                                                    the specified ``train_size``.\n            train_size (float, int, or None, default is None):  If float, should be between 0.0 and 1.0 and represent the\n                                                    proportion of the dataset to include in the train split. If\n                                                    int, represents the absolute number of train samples. If None,\n                                                    the value is automatically set to the complement of the test size.\n            random_state (int, RandomState instance or None, optional): If int, random_state is the seed used by the random number generator;\n                                                    If RandomState instance, random_state is the random number generator;\n                                                    If None, the random number generator is the RandomState instance used\n                                                    by `np.random`. Unlike StratifiedShuffleSplit that only uses\n                                                    random_state when ``shuffle`` == True, this multilabel implementation\n                                                    always uses the random_state since the iterative stratification\n                                                    algorithm breaks ties randomly.\n        \"\"\"\n        super(MultilabelStratifiedShuffleSplit, self).__init__(\n            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n                    type_of_target_y))\n\n        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n                                                  self.train_size)\n\n        n_samples = y.shape[0]\n        rng = check_random_state(self.random_state)\n        y_orig = y.copy()\n\n        r = np.array([n_train, n_test]) / (n_train + n_test)\n\n        for _ in range(self.n_splits):\n            indices = np.arange(n_samples)\n            rng.shuffle(indices)\n            y = y_orig[indices]\n\n            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n            test_idx = test_folds[np.argsort(indices)] == 1\n            test = np.where(test_idx)[0]\n            train = np.where(~test_idx)[0]\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        ???+ note\n            Randomized CV splitters may return different results for each call of\n            split. You can make the results identical by setting ``random_state``\n            to an integer.\n\n        Args:\n            X (array-like, shape (n_samples, n_features) ): Training data, where n_samples is the number of samples\n                                                            and n_features is the number of features.\n                                                            Note that providing ``y`` is sufficient to generate the splits and\n                                                            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n                                                            ``X`` instead of actual training data.\n            y (array-like, shape (n_samples, n_labels) ):   The target variable for supervised learning problems.\n                                                            Multilabel stratification is done based on the y labels.\n            groups (object, optional):                      Always ignored, exists for compatibility.\n\n\n        Returns:\n            train (numpy.ndarray):        The training set indices for that split.\n            test (numpy.ndarray):         The testing set indices for that split.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedShuffleSplit.__init__","title":"<code>__init__(n_splits=10, test_size='default', train_size=None, random_state=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of re-shuffling &amp; splitting iterations.</p> <code>10</code> <code>test_size</code> <code>float, int, None</code> <p>If float, should be between 0.0 and 1.0 and represent the proportion                                     of the dataset to include in the test split. If int, represents the                                     absolute number of test samples. If None, the value is set to the                                     complement of the train size. By default, the value is set to 0.1.                                     The default will change in version 0.21. It will remain 0.1 only                                     if <code>train_size</code> is unspecified, otherwise it will complement                                     the specified <code>train_size</code>.</p> <code>'default'</code> <code>train_size</code> <code>float, int, or None, default is None</code> <p>If float, should be between 0.0 and 1.0 and represent the                                     proportion of the dataset to include in the train split. If                                     int, represents the absolute number of train samples. If None,                                     the value is automatically set to the complement of the test size.</p> <code>None</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>If int, random_state is the seed used by the random number generator;                                     If RandomState instance, random_state is the random number generator;                                     If None, the random number generator is the RandomState instance used                                     by <code>np.random</code>. Unlike StratifiedShuffleSplit that only uses                                     random_state when <code>shuffle</code> == True, this multilabel implementation                                     always uses the random_state since the iterative stratification                                     algorithm breaks ties randomly.</p> <code>None</code> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n             random_state=None):\n    \"\"\"\n    Args:\n        n_splits (int):                         Number of re-shuffling &amp; splitting iterations.\n        test_size (float, int, None, optional): If float, should be between 0.0 and 1.0 and represent the proportion\n                                                of the dataset to include in the test split. If int, represents the\n                                                absolute number of test samples. If None, the value is set to the\n                                                complement of the train size. By default, the value is set to 0.1.\n                                                The default will change in version 0.21. It will remain 0.1 only\n                                                if ``train_size`` is unspecified, otherwise it will complement\n                                                the specified ``train_size``.\n        train_size (float, int, or None, default is None):  If float, should be between 0.0 and 1.0 and represent the\n                                                proportion of the dataset to include in the train split. If\n                                                int, represents the absolute number of train samples. If None,\n                                                the value is automatically set to the complement of the test size.\n        random_state (int, RandomState instance or None, optional): If int, random_state is the seed used by the random number generator;\n                                                If RandomState instance, random_state is the random number generator;\n                                                If None, the random number generator is the RandomState instance used\n                                                by `np.random`. Unlike StratifiedShuffleSplit that only uses\n                                                random_state when ``shuffle`` == True, this multilabel implementation\n                                                always uses the random_state since the iterative stratification\n                                                algorithm breaks ties randomly.\n    \"\"\"\n    super(MultilabelStratifiedShuffleSplit, self).__init__(\n        n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.MultilabelStratifiedShuffleSplit.split","title":"<code>split(X, y, groups=None)</code>","text":"<p>Generate indices to split data into training and test set.</p> Note <p>Randomized CV splitters may return different results for each call of split. You can make the results identical by setting <code>random_state</code> to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape (n_samples, n_features) </code> <p>Training data, where n_samples is the number of samples                                             and n_features is the number of features.                                             Note that providing <code>y</code> is sufficient to generate the splits and                                             hence <code>np.zeros(n_samples)</code> may be used as a placeholder for                                             <code>X</code> instead of actual training data.</p> required <code>y</code> <code>array-like, shape (n_samples, n_labels) </code> <p>The target variable for supervised learning problems.                                             Multilabel stratification is done based on the y labels.</p> required <code>groups</code> <code>object</code> <p>Always ignored, exists for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>train</code> <code>numpy.ndarray</code> <p>The training set indices for that split.</p> <code>test</code> <code>numpy.ndarray</code> <p>The testing set indices for that split.</p> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>def split(self, X, y, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n\n    ???+ note\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n\n    Args:\n        X (array-like, shape (n_samples, n_features) ): Training data, where n_samples is the number of samples\n                                                        and n_features is the number of features.\n                                                        Note that providing ``y`` is sufficient to generate the splits and\n                                                        hence ``np.zeros(n_samples)`` may be used as a placeholder for\n                                                        ``X`` instead of actual training data.\n        y (array-like, shape (n_samples, n_labels) ):   The target variable for supervised learning problems.\n                                                        Multilabel stratification is done based on the y labels.\n        groups (object, optional):                      Always ignored, exists for compatibility.\n\n\n    Returns:\n        train (numpy.ndarray):        The training set indices for that split.\n        test (numpy.ndarray):         The testing set indices for that split.\n    \"\"\"\n    y = check_array(y, ensure_2d=False, dtype=None)\n    return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)\n</code></pre>"},{"location":"reference/sampling/iterative/#aucmedi.sampling.iterative.IterativeStratification","title":"<code>IterativeStratification(labels, r, random_state)</code>","text":"<p>This function implements the Iterative Stratification algorithm described in the following paper:</p> Reference - Publication <p>Sechidis K., Tsoumakas G., Vlahavas I. 2011. On the Stratification of Multi-Label Data. Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin, Heidelberg. Aristotle University of Thessaloniki.  https://link.springer.com/chapter/10.1007/978-3-642-23808-6_10</p> Source code in <code>aucmedi/sampling/iterative.py</code> <pre><code>def IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n\n    ??? abstract \"Reference - Publication\"\n        Sechidis K., Tsoumakas G., Vlahavas I. 2011.\n        On the Stratification of Multi-Label Data.\n        Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2011.\n        Lecture Notes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\n        Aristotle University of Thessaloniki.\n        &lt;br&gt;\n        [https://link.springer.com/chapter/10.1007/978-3-642-23808-6_10](https://link.springer.com/chapter/10.1007/978-3-642-23808-6_10)\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] &gt; 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] &gt; 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] &gt; 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] &gt; 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n</code></pre>"},{"location":"reference/sampling/kfold/","title":"Kfold","text":""},{"location":"reference/sampling/kfold/#aucmedi.sampling.kfold.sampling_kfold","title":"<code>sampling_kfold(samples, labels, metadata=None, n_splits=3, stratified=True, iterative=False, seed=None)</code>","text":"<p>Simple wrapper function for calling k-fold cross-validation sampling functions.</p> <p>Allow usage of stratified and iterative sampling algorithm.</p> Warning <p>Be aware that multi-label data does not support random stratified sampling.</p> Example <p>The sampling is returned as list with length n_splits containing tuples with sampled data.</p> Example for n_splits=3<pre><code>cv = sampling_kfold(samples, labels, n_splits=3)\n\n# sampling in which x = samples and y = labels\n# cv &lt;-&gt; [(train_x, train_y, test_x, test_y),   # fold 1\n#         (train_x, train_y, test_x, test_y),   # fold 2\n#         (train_x, train_y, test_x, test_y)]   # fold 3\n\n# Recommended access on the folds\nfor fold in cv:\n    (train_x, train_y, test_x, test_y) = fold\n</code></pre> Example with metadata<pre><code>cv = sampling_kfold(samples, labels, metadata, n_splits=2)\n\n# sampling in which x = samples, y = labels and m = metadata\n# cv &lt;-&gt; [(train_x, train_y, train_m, test_x, test_y, test_m),      # fold 1\n#         (train_x, train_y, train_m, test_x, test_y, test_m)]      # fold 2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of str</code> <p>List of sample/index encoded as Strings.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>NumPy matrix containing the ohe encoded classification.</p> required <code>metadata</code> <code>numpy.ndarray</code> <p>NumPy matrix with additional metadata. Have to be shape (n_samples, meta_variables).</p> <code>None</code> <code>n_splits</code> <code>int</code> <p>Number of folds (k). Must be at least 2.</p> <code>3</code> <code>stratified</code> <code>bool</code> <p>Option whether to use stratified sampling based on provided labels.</p> <code>True</code> <code>iterative</code> <code>bool</code> <p>Option whether to use iterative sampling algorithm.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Seed to ensure reproducibility for random functions.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>sampling</code> <code>list of tuple</code> <p>List with length <code>n_splits</code> containing tuples with sampled data.</p> Source code in <code>aucmedi/sampling/kfold.py</code> <pre><code>def sampling_kfold(samples, labels, metadata=None, n_splits=3,\n                   stratified=True, iterative=False, seed=None):\n    \"\"\" Simple wrapper function for calling k-fold cross-validation sampling functions.\n\n    Allow usage of stratified and iterative sampling algorithm.\n\n    ???+ warning\n        Be aware that multi-label data does not support random stratified sampling.\n\n    ???+ example\n        The sampling is returned as list with length n_splits containing tuples with sampled data.\n\n        ```python title=\"Example for n_splits=3\"\n        cv = sampling_kfold(samples, labels, n_splits=3)\n\n        # sampling in which x = samples and y = labels\n        # cv &lt;-&gt; [(train_x, train_y, test_x, test_y),   # fold 1\n        #         (train_x, train_y, test_x, test_y),   # fold 2\n        #         (train_x, train_y, test_x, test_y)]   # fold 3\n\n        # Recommended access on the folds\n        for fold in cv:\n            (train_x, train_y, test_x, test_y) = fold\n        ```\n\n        ```python title=\"Example with metadata\"\n        cv = sampling_kfold(samples, labels, metadata, n_splits=2)\n\n        # sampling in which x = samples, y = labels and m = metadata\n        # cv &lt;-&gt; [(train_x, train_y, train_m, test_x, test_y, test_m),      # fold 1\n        #         (train_x, train_y, train_m, test_x, test_y, test_m)]      # fold 2\n        ```\n\n    Args:\n        samples (list of str):      List of sample/index encoded as Strings.\n        labels (numpy.ndarray):     NumPy matrix containing the ohe encoded classification.\n        metadata (numpy.ndarray):   NumPy matrix with additional metadata. Have to be shape (n_samples, meta_variables).\n        n_splits (int):             Number of folds (k). Must be at least 2.\n        stratified (bool):          Option whether to use stratified sampling based on provided labels.\n        iterative (bool):           Option whether to use iterative sampling algorithm.\n        seed (int):                 Seed to ensure reproducibility for random functions.\n\n    Returns:\n        sampling (list of tuple):   List with length `n_splits` containing tuples with sampled data.\n    \"\"\"\n    # Initialize variables\n    results = []\n    wk_labels = labels\n\n    # Initialize random sampler\n    if not stratified and not iterative:\n        sampler = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    # Initialize random stratified sampler\n    elif stratified and not iterative:\n        sampler = StratifiedKFold(n_splits=n_splits, shuffle=True,\n                                  random_state=seed)\n        wk_labels = np.argmax(wk_labels, axis=-1)\n    # Initialize iterative stratified sampler\n    else:\n        sampler = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True,\n                                            random_state=seed)\n\n    # Preprocess data\n    x = np.asarray(samples)\n    y = np.asarray(labels)\n    if metadata is not None : m = np.asarray(metadata)\n\n    # Apply sampling and generate folds\n    for train, test in sampler.split(X=samples, y=wk_labels):\n        # Simple sampling\n        if metadata is None:\n            fold = (x[train], y[train], x[test], y[test])\n        # Sampling with metadata\n        else:\n            fold = (x[train], y[train], m[train], x[test], y[test], m[test])\n        results.append(fold)\n\n    # Return result sampling\n    return results\n</code></pre>"},{"location":"reference/sampling/split/","title":"Split","text":""},{"location":"reference/sampling/split/#aucmedi.sampling.split.sampling_split","title":"<code>sampling_split(samples, labels, metadata=None, sampling=[0.8, 0.2], stratified=True, iterative=False, seed=None)</code>","text":"<p>Simple wrapper function for calling percentage split sampling functions.</p> <p>Allow usage of stratified and iterative sampling algorithm.</p> Warning <p>Be aware that multi-label data does not support random stratified sampling.</p> <p>Percentage split ratios have to be provided with a sampling list. Each percentage value in the list defines the approximate split size. Sum of percentage split ratios have to equal 1!</p> Example Example for common train/val/test split<pre><code>split_ratio = [0.7, 0.25, 0.05]\nds = sampling_split(samples, labels, sampling=split_ratio)\n\n# Returns a list with the following elements as tuples:\nprint(ds[0])  # -&gt; (samples_a, labels_a)                with 70% of complete dataset\nprint(ds[1])  # -&gt; (samples_b, labels_b)                with 25% of complete dataset\nprint(ds[2])  # -&gt; (samples_c, labels_c)                with  5% of complete dataset\n</code></pre> Example with metadata<pre><code>ds = sampling_split(samples, labels, metadata, sampling=[0.8, 0.2])\n\n# Returns a list with the following elements as tuples:\nprint(ds[0])  # -&gt; (samples_a, labels_a, metadata_a)    with 80% of complete dataset\nprint(ds[1])  # -&gt; (samples_b, labels_b, metadata_b)    with 20% of complete dataset\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of str</code> <p>List of sample/index encoded as Strings.</p> required <code>labels</code> <code>numpy.ndarray</code> <p>NumPy matrix containing the ohe encoded classification.</p> required <code>metadata</code> <code>numpy.ndarray</code> <p>NumPy matrix with additional metadata. Have to be shape (n_samples, meta_variables).</p> <code>None</code> <code>sampling</code> <code>list of float</code> <p>List of percentage values with split sizes.</p> <code>[0.8, 0.2]</code> <code>stratified</code> <code>bool</code> <p>Option whether to use stratified sampling based on provided labels.</p> <code>True</code> <code>iterative</code> <code>bool</code> <p>Option whether to use iterative sampling algorithm.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Seed to ensure reproducibility for random functions.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>list of tuple</code> <p>List with <code>len(sampling)</code> containing tuples with sampled data:                             (samples_a, labels_a) and with metadata (samples_a, labels_a, metadata_a).</p> Source code in <code>aucmedi/sampling/split.py</code> <pre><code>def sampling_split(samples, labels, metadata=None, sampling=[0.8, 0.2],\n                   stratified=True, iterative=False, seed=None):\n    \"\"\" Simple wrapper function for calling percentage split sampling functions.\n\n    Allow usage of stratified and iterative sampling algorithm.\n\n    ???+ warning\n        Be aware that multi-label data does not support random stratified sampling.\n\n    Percentage split ratios have to be provided with a sampling list.\n    Each percentage value in the list defines the approximate split size.\n    Sum of percentage split ratios have to equal 1!\n\n    ???+ example\n        ```python title=\"Example for common train/val/test split\"\n        split_ratio = [0.7, 0.25, 0.05]\n        ds = sampling_split(samples, labels, sampling=split_ratio)\n\n        # Returns a list with the following elements as tuples:\n        print(ds[0])  # -&gt; (samples_a, labels_a)                with 70% of complete dataset\n        print(ds[1])  # -&gt; (samples_b, labels_b)                with 25% of complete dataset\n        print(ds[2])  # -&gt; (samples_c, labels_c)                with  5% of complete dataset\n        ```\n\n        ```python title=\"Example with metadata\"\n        ds = sampling_split(samples, labels, metadata, sampling=[0.8, 0.2])\n\n        # Returns a list with the following elements as tuples:\n        print(ds[0])  # -&gt; (samples_a, labels_a, metadata_a)    with 80% of complete dataset\n        print(ds[1])  # -&gt; (samples_b, labels_b, metadata_b)    with 20% of complete dataset\n        ```\n\n    Args:\n        samples (list of str):          List of sample/index encoded as Strings.\n        labels (numpy.ndarray):         NumPy matrix containing the ohe encoded classification.\n        metadata (numpy.ndarray):       NumPy matrix with additional metadata. Have to be shape (n_samples, meta_variables).\n        sampling (list of float):       List of percentage values with split sizes.\n        stratified (bool):              Option whether to use stratified sampling based on provided labels.\n        iterative (bool):               Option whether to use iterative sampling algorithm.\n        seed (int):                     Seed to ensure reproducibility for random functions.\n\n    Returns:\n        results (list of tuple):        List with `len(sampling)` containing tuples with sampled data:\n                                        (samples_a, labels_a) and with metadata (samples_a, labels_a, metadata_a).\n    \"\"\"\n    # Verify sampling percentages\n    if not np.isclose(sum(sampling), 1.0):\n        raise ValueError(\"Sum of Percentage split ratios as sampling do not\" + \\\n                         \" equal 1\", sampling, np.sum(sampling))\n    # Initialize leftover with the complete dataset\n    leftover_samples = np.asarray(samples)\n    leftover_labels = np.asarray(labels)\n    if metadata is not None : leftover_meta = np.asarray(metadata)\n    leftover_p = 0.0\n    # Initialize result list\n    results = []\n\n    # Perform sampling for each percentage split\n    for i in range(0, len(sampling)):\n        # For last split, just take leftover data as subset\n        if i == len(sampling)-1:\n            # Generate split\n            if metadata is None : split = (leftover_samples, leftover_labels)\n            else : split = (leftover_samples, leftover_labels, leftover_meta)\n            # Append splitted data and stop\n            results.append(split)\n            break\n\n        # Identify split percentage for remaining data\n        p = sampling[i] / (1.0 - leftover_p)\n        # Initialize random sampler\n        if not stratified and not iterative:\n            sampler = ShuffleSplit(n_splits=1, random_state=seed,\n                                   train_size=(1.0-p), test_size=p)\n        # Initialize random stratified sampler\n        elif stratified and not iterative:\n            sampler = StratifiedShuffleSplit(n_splits=1, random_state=seed,\n                                             train_size=(1.0-p), test_size=p)\n        # Initialize iterative stratified sampler\n        else:\n            sampler = MultilabelStratifiedShuffleSplit(n_splits=1,\n                            random_state=seed, train_size=(1.0-p), test_size=p)\n\n        # Apply sampling\n        subset_generator = sampler.split(X=leftover_samples, y=leftover_labels)\n        subsets = next(subset_generator)\n        # Generate split\n        if metadata is None:\n            split = (leftover_samples[subsets[1]], leftover_labels[subsets[1]])\n        else : split = (leftover_samples[subsets[1]],\n                        leftover_labels[subsets[1]],\n                        leftover_meta[subsets[1]])\n        # Append splitted data\n        results.append(split)\n        # Update remaining data\n        leftover_p += sampling[i]\n        leftover_samples = leftover_samples[subsets[0]]\n        leftover_labels = leftover_labels[subsets[0]]\n        if metadata is not None : leftover_meta = leftover_meta[subsets[0]]\n\n    # Return result sampling\n    return results\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":"<p>aucmedi.utils is a collection of small functions and classes which make     common tasks shorter and easier.</p> <p>The core features of utils are the Class Weight computation and Visualizer functions.</p>"},{"location":"reference/utils/callbacks/","title":"Callbacks","text":""},{"location":"reference/utils/callbacks/#aucmedi.utils.callbacks.MinEpochEarlyStopping","title":"<code>MinEpochEarlyStopping</code>","text":"<p>         Bases: <code>EarlyStopping</code></p> <p>Changed baseline to act as a real baseline.</p> <p>The number of patience epochs are only counted when baseline loss is achieved.</p> Reference - Implementation <p>Author:   McLawrence   Source:   https://stackoverflow.com/questions/46287403/is-there-a-way-to-implement-early-stopping-in-keras-only-after-the-first-say-1  </p> Source code in <code>aucmedi/utils/callbacks.py</code> <pre><code>class MinEpochEarlyStopping(EarlyStopping):\n    \"\"\" Changed baseline to act as a real baseline.\n\n    The number of patience epochs are only counted when baseline loss is achieved.\n\n    ??? abstract \"Reference - Implementation\"\n        Author:   McLawrence  &lt;br&gt;\n        Source:   https://stackoverflow.com/questions/46287403/is-there-a-way-to-implement-early-stopping-in-keras-only-after-the-first-say-1  &lt;br&gt;\n    \"\"\"\n    def __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0,\n                 mode='auto', start_epoch = 100): # add argument for starting epoch\n        super(MinEpochEarlyStopping, self).__init__()\n        self.start_epoch = start_epoch\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch &gt; self.start_epoch:\n            super(MinEpochEarlyStopping, self).on_epoch_end(epoch, logs)\n</code></pre>"},{"location":"reference/utils/callbacks/#aucmedi.utils.callbacks.ThresholdEarlyStopping","title":"<code>ThresholdEarlyStopping</code>","text":"<p>         Bases: <code>EarlyStopping</code></p> <p>Changed baseline to act as a real baseline.</p> <p>The number of patience epochs are only counted when baseline loss is achieved.</p> Reference - Implementation <p>Author:   JBSnorro  Source:   https://stackoverflow.com/questions/53500047/stop-training-in-keras-when-accuracy-is-already-1-0  </p> Source code in <code>aucmedi/utils/callbacks.py</code> <pre><code>class ThresholdEarlyStopping(EarlyStopping):\n    \"\"\" Changed baseline to act as a real baseline.\n\n    The number of patience epochs are only counted when baseline loss is achieved.\n\n    ??? abstract \"Reference - Implementation\"\n        Author:   JBSnorro &lt;br&gt;\n        Source:   https://stackoverflow.com/questions/53500047/stop-training-in-keras-when-accuracy-is-already-1-0  &lt;br&gt;\n    \"\"\"\n    def __init__(self, *args, **kw):\n        super().__init__(*args, **kw)\n        self.baseline_attained = False\n\n    def on_epoch_end(self, epoch, logs=None):\n        if not self.baseline_attained:\n            current = self.get_monitor_value(logs)\n            if current is None:\n                return\n            if self.monitor_op(current, self.baseline):\n                if self.verbose &gt; 0:\n                    print('Baseline attained.')\n                self.baseline_attained = True\n            else:\n                return\n        super(ThresholdEarlyStopping, self).on_epoch_end(epoch, logs)\n</code></pre>"},{"location":"reference/utils/callbacks/#aucmedi.utils.callbacks.csv_to_history","title":"<code>csv_to_history(input_path)</code>","text":"<p>Utility function for reading a CSV file from the CSVLogger Callback and return a History dictionary object.</p> <p>Can be utilized in order to pass returned dictionary object to the evaluate_fitting() function of the AUCMEDI evaluation submodule.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to a CSV file generated by a CSVLogger Callback.</p> required <p>Returns:</p> Name Type Description <code>history</code> <code>dict</code> <p>A history dictionary from a Keras history object which contains several logs.</p> Source code in <code>aucmedi/utils/callbacks.py</code> <pre><code>def csv_to_history(input_path):\n    \"\"\" Utility function for reading a CSV file from the\n    [CSVLogger](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger)\n    Callback and return a History dictionary object.\n\n    Can be utilized in order to pass returned dictionary object to the\n    [evaluate_fitting()][aucmedi.evaluation.fitting] function of the AUCMEDI\n    [evaluation][aucmedi.evaluation] submodule.\n\n    Args:\n        input_path (str):           Path to a CSV file generated by a CSVLogger Callback.\n\n    Returns:\n        history (dict):       A history dictionary from a Keras history object which contains several logs.\n    \"\"\"\n    # Read logging data\n    dt = pd.read_csv(input_path, sep=\",\")\n    # Parse to dict and return results\n    return dt.to_dict(orient=\"list\")\n</code></pre>"},{"location":"reference/utils/class_weights/","title":"Class weights","text":""},{"location":"reference/utils/class_weights/#aucmedi.utils.class_weights.compute_class_weights","title":"<code>compute_class_weights(ohe_array, method='balanced')</code>","text":"<p>Simple wrapper function for scikit learn class_weight function.</p> <p>The class weights can be used for weighting the loss function on imbalanced data.</p> Info <p>NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).</p> Example <pre><code># Compute class weights\ncw_loss, cw_fit = compute_class_weights(class_ohe)\n\n# Provide class weights to loss function\nmodel = NeuralNetwork(nclasses, channels=3, loss=categorical_focal_loss(cw_loss))\n\n# Provide class weights to keras fit()\nmodel.train(index_list, epochs=50, class_weights=cw_fit)\n</code></pre> Based on Reference <p>Scikit-learn class_weight function:  https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html  </p> <p>Parameters:</p> Name Type Description Default <code>ohe_array</code> <code>numpy.ndarray</code> <p>NumPy matrix containing the ohe encoded classification.</p> required <code>method</code> <code>str</code> <p>Dictionary or modus, how class weights should be computed.</p> <code>'balanced'</code> <p>Returns:</p> Name Type Description <code>class_weights_list</code> <code>numpy.ndarray</code> <p>Class weight list which can be feeded to a loss function.</p> <code>class_weights_dict</code> <code>dict</code> <p>Class weight dictionary which can be feeded to                                     train()                                     or keras.model.fit().</p> Source code in <code>aucmedi/utils/class_weights.py</code> <pre><code>def compute_class_weights(ohe_array, method=\"balanced\"):\n    \"\"\" Simple wrapper function for scikit learn class_weight function.\n\n    The class weights can be used for weighting the loss function on imbalanced data.\n\n    ???+ info\n        NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).\n\n    ???+ example \"Example\"\n        ```python\n        # Compute class weights\n        cw_loss, cw_fit = compute_class_weights(class_ohe)\n\n        # Provide class weights to loss function\n        model = NeuralNetwork(nclasses, channels=3, loss=categorical_focal_loss(cw_loss))\n\n        # Provide class weights to keras fit()\n        model.train(index_list, epochs=50, class_weights=cw_fit)\n        ```\n\n    ??? abstract \"Based on Reference\"\n        Scikit-learn class_weight function: &lt;br&gt;\n        https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html  &lt;br&gt;\n\n    Args:\n        ohe_array (numpy.ndarray):  NumPy matrix containing the ohe encoded classification.\n        method (str):               Dictionary or modus, how class weights should be computed.\n\n    Returns:\n        class_weights_list (numpy.ndarray):     Class weight list which can be feeded to a loss function.\n        class_weights_dict (dict):              Class weight dictionary which can be feeded to\n                                                [train()][aucmedi.neural_network.model.NeuralNetwork.train]\n                                                or keras.model.fit().\n    \"\"\"\n    # Obtain sparse categorical array and number of classes\n    class_array = np.argmax(ohe_array, axis=-1)\n    n_classes = np.unique(class_array)\n    # Compute class weights with scikit learn\n    class_weights_list = compute_class_weight(class_weight=method,\n                                              classes=n_classes, y=class_array)\n    # Convert class weight array to dictionary\n    class_weights_dict = dict(enumerate(class_weights_list))\n    # Return resulting class weights as list and dictionary\n    return class_weights_list, class_weights_dict\n</code></pre>"},{"location":"reference/utils/class_weights/#aucmedi.utils.class_weights.compute_multilabel_weights","title":"<code>compute_multilabel_weights(ohe_array, method='balanced')</code>","text":"<p>Function for computing class weights individually for multi-label data.</p> <p>Class weights can be used for weighting the loss function on imbalanced data. Returned is a class weight list which can be passed to loss functions.</p> Info <p>NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).</p> Based on Reference <p>Class weight computation is based on Scikit learn class_weight function:  https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html  </p> <p>Parameters:</p> Name Type Description Default <code>ohe_array</code> <code>numpy.ndarray</code> <p>NumPy matrix containing the ohe encoded classification.</p> required <code>method</code> <code>str</code> <p>Dictionary or modus, how class weights should be computed.</p> <code>'balanced'</code> <p>Returns:</p> Name Type Description <code>class_weights</code> <code>numpy.ndarray</code> <p>Class weight list which can be fed to a loss function.</p> Source code in <code>aucmedi/utils/class_weights.py</code> <pre><code>def compute_multilabel_weights(ohe_array, method=\"balanced\"):\n    \"\"\" Function for computing class weights individually for multi-label data.\n\n    Class weights can be used for weighting the loss function on imbalanced data.\n    Returned is a class weight list which can be passed to loss functions.\n\n    ???+ info\n        NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).\n\n    ??? abstract \"Based on Reference\"\n        Class weight computation is based on Scikit learn class_weight function: &lt;br&gt;\n        https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html  &lt;br&gt;\n\n    Args:\n        ohe_array (numpy.ndarray):      NumPy matrix containing the ohe encoded classification.\n        method (str):                   Dictionary or modus, how class weights should be computed.\n\n    Returns:\n        class_weights (numpy.ndarray):      Class weight list which can be fed to a loss function.\n    \"\"\"\n    # Identify number of classes\n    n_classes = np.shape(ohe_array)[1]\n    # Initialize class weight list\n    class_weights = np.empty([n_classes])\n    # Compute weight for each class individually\n    for i in range(0, n_classes):\n        weight = compute_class_weight(class_weight=method, classes=np.array([0,1]),\n                                      y=ohe_array[:, i])\n        class_weights[i] = weight[1]\n    # Return resulting class weight list\n    return class_weights\n</code></pre>"},{"location":"reference/utils/class_weights/#aucmedi.utils.class_weights.compute_sample_weights","title":"<code>compute_sample_weights(ohe_array, method='balanced')</code>","text":"<p>Simple wrapper function for scikit learn sample_weight function.</p> <p>The sample weights can be used for weighting the loss function on imbalanced data. Returned sample weight array which can be directly fed to an AUCMEDI DataGenerator.</p> Info <p>NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).</p> Based on Reference <p>Scikit learn sample_weight function:  https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html </p> <p>Parameters:</p> Name Type Description Default <code>ohe_array</code> <code>numpy.ndarray</code> <p>NumPy matrix containing the ohe encoded classification.</p> required <code>method</code> <code>str</code> <p>Dictionary or modus, how class weights should be computed.</p> <code>'balanced'</code> <p>Returns:</p> Name Type Description <code>sample_weights</code> <code>numpy.ndarray</code> <p>Sample weight list which can be fed to an AUCMEDI                                 DataGenerator.</p> Source code in <code>aucmedi/utils/class_weights.py</code> <pre><code>def compute_sample_weights(ohe_array, method=\"balanced\"):\n    \"\"\" Simple wrapper function for scikit learn sample_weight function.\n\n    The sample weights can be used for weighting the loss function on imbalanced data.\n    Returned sample weight array which can be directly fed to an AUCMEDI [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n\n    ???+ info\n        NumPy array shape has to be (n_samples, n_classes) like this: (500, 4).\n\n    ??? abstract \"Based on Reference\"\n        Scikit learn sample_weight function: &lt;br&gt;\n        https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html &lt;br&gt;\n\n    Args:\n        ohe_array (numpy.ndarray):      NumPy matrix containing the ohe encoded classification.\n        method (str):                   Dictionary or modus, how class weights should be computed.\n\n    Returns:\n        sample_weights (numpy.ndarray):     Sample weight list which can be fed to an AUCMEDI\n                                            [DataGenerator][aucmedi.data_processing.data_generator.DataGenerator].\n    \"\"\"\n    # Compute sample weights with scikit learn\n    sample_weights = compute_sample_weight(class_weight=method, y=ohe_array)\n    # Return resulting sample weights\n    return sample_weights\n</code></pre>"},{"location":"reference/utils/visualizer/","title":"Visualizer","text":""},{"location":"reference/utils/visualizer/#aucmedi.utils.visualizer.visualize_heatmap","title":"<code>visualize_heatmap(image, heatmap, overlay=True, out_path=None, alpha=0.4)</code>","text":"<p>Simple wrapper function to visualize a heatmap encoded as NumPy matrix with a     [0-1] range as image/volume via matplotlib.</p> Reference - Implementation <p>Author: Fran\u00e7ois Chollet  Date: April 26, 2020  https://keras.io/examples/vision/grad_cam/ </p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>NumPy matrix containing an image or volume.</p> required <code>heatmap</code> <code>numpy.ndarray</code> <p>NumPy matrix containing a XAI heatmap.</p> required <code>out_path</code> <code>str</code> <p>Path in which image is stored (else live output).</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Transparency value for heatmap overlap on image (range: [0-1]).</p> <code>0.4</code> Source code in <code>aucmedi/utils/visualizer.py</code> <pre><code>def visualize_heatmap(image, heatmap, overlay=True, out_path=None, alpha=0.4):\n    \"\"\" Simple wrapper function to visualize a heatmap encoded as NumPy matrix with a\n        [0-1] range as image/volume via matplotlib.\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Fran\u00e7ois Chollet &lt;br&gt;\n        Date: April 26, 2020 &lt;br&gt;\n        https://keras.io/examples/vision/grad_cam/ &lt;br&gt;\n\n    Args:\n        image (numpy.ndarray):          NumPy matrix containing an image or volume.\n        heatmap (numpy.ndarray):        NumPy matrix containing a XAI heatmap.\n        out_path (str):                 Path in which image is stored (else live output).\n        alpha (float):                  Transparency value for heatmap overlap on image (range: [0-1]).\n    \"\"\"\n    # Grayscale normalization if required\n    if image.shape[-1] == 1 and (np.min(image) &lt; 0 or np.max(image) &gt; 255):\n        image = Standardize(mode=\"grayscale\").transform(image)\n    elif image.shape[-1] != 1 and (np.min(image) &lt; 0 or np.max(image) &gt; 255):\n        raise ValueError(\"Visualizer does not support multi-channel \" + \\\n                         \"non-RGB formats.\" + \\\n                         \" Array min: \" + str(np.min(image)) + \\\n                         \" Array max: \" + str(np.max(image))) \n    # If image is grayscale, convert to RGB\n    if image.shape[-1] == 1 : image = np.concatenate((image,)*3, axis=-1)\n    # Rescale heatmap to grayscale range\n    heatmap = np.uint8(heatmap * 255)\n    # Overlay the heatmap on the image \n    if overlay:\n        # Use jet colormap to colorize heatmap\n        jet = cm.get_cmap(\"jet\")\n        # Use RGB values of the colormap\n        jet_colors = jet(np.arange(256))[:,:3]\n        jet_heatmap = jet_colors[heatmap] * 255\n        # Superimpose the heatmap on original image\n        final_img = jet_heatmap * alpha + (1-alpha) * image\n    # Output just the heatmap\n    else : final_img = heatmap\n    # Apply corresponding 2D visualizer \n    if len(image.shape) == 3 : visualize_image(final_img, out_path=out_path)\n    elif len(image.shape) == 4 : visualize_volume(final_img, out_path=out_path)\n</code></pre>"},{"location":"reference/utils/visualizer/#aucmedi.utils.visualizer.visualize_image","title":"<code>visualize_image(array, out_path=None)</code>","text":"<p>Simple wrapper function to visualize a NumPy matrix as image via PIL.</p> Info <p>NumPy array shape has to be (x, y, channel) like this: (224, 224, 3)</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>numpy.ndarray</code> <p>NumPy matrix containing an image.</p> required <code>out_path</code> <code>str</code> <p>Path in which image is stored (else live output).</p> <code>None</code> Source code in <code>aucmedi/utils/visualizer.py</code> <pre><code>def visualize_image(array, out_path=None):\n    \"\"\" Simple wrapper function to visualize a NumPy matrix as image via PIL.\n\n    ???+ info\n        NumPy array shape has to be (x, y, channel) like this: (224, 224, 3)\n\n    Args:\n        array (numpy.ndarray):          NumPy matrix containing an image.\n        out_path (str):                 Path in which image is stored (else live output).\n    \"\"\"\n    # Ensure integer intensity values\n    array = np.uint8(array)\n    # Remove channel axis if grayscale\n    if array.shape[-1] == 1 : array = np.reshape(array, array.shape[:-1])\n    # Convert array to PIL image\n    image = Image.fromarray(array)\n    # Visualize or store image\n    if out_path is None : image.show()\n    else : image.save(out_path)\n</code></pre>"},{"location":"reference/utils/visualizer/#aucmedi.utils.visualizer.visualize_volume","title":"<code>visualize_volume(array, out_path=None, iteration_axis=1)</code>","text":"<p>Simple wrapper function to visualize/store a NumPy matrix as volume.</p> Info <p>NumPy array shape has to be (x, y, z, channel) like this: (128, 128, 128, 1)</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>numpy.ndarray</code> <p>NumPy matrix containing an image.</p> required <code>out_path</code> <code>str</code> <p>Path in which volume is stored (else live output).</p> <code>None</code> Source code in <code>aucmedi/utils/visualizer.py</code> <pre><code>def visualize_volume(array, out_path=None, iteration_axis=1):\n    \"\"\" Simple wrapper function to visualize/store a NumPy matrix as volume.\n\n    ???+ info\n        NumPy array shape has to be (x, y, z, channel) like this: (128, 128, 128, 1)\n\n    Args:\n        array (numpy.ndarray):          NumPy matrix containing an image.\n        out_path (str):                 Path in which volume is stored (else live output).\n    \"\"\"\n    # Identify output format\n    if out_path is None : format = \"gif\"\n    elif out_path.split(\".\")[-1].lower() in [\"gif\", \"mha\", \"nii\", \"gz\", \"npy\"]:\n        format = out_path.split(\".\")[-1]\n    else : raise ValueError(\"Visualizer does not support image format!\")\n\n    # Encode as GIF\n    if format == \"gif\":\n        # Grayscale normalization if required\n        if array.shape[-1] == 1 and (np.min(array) &lt; 0 or np.max(array) &gt; 255):\n            array = Standardize(mode=\"grayscale\").transform(array)\n        elif array.shape[-1] != 1 and (np.min(array) &lt; 0 or np.max(array) &gt; 255):\n            raise ValueError(\"Visualizer does not support multi-channel \" + \\\n                            \"non-RGB formats.\" + \\\n                            \" Array min: \" + str(np.min(array)) + \\\n                            \" Array max: \" + str(np.max(array))) \n        # Ensure integer intensity values\n        array = np.uint8(array)\n\n        # Create a figure and two axes objects from matplot\n        fig = plt.figure()\n        img = plt.imshow(np.take(array, 0, axis=iteration_axis),\n                        cmap='gray', vmin=0, vmax=255, animated=True)\n        # Update function to show the slice for the current frame\n        def update(i):\n            plt.suptitle(\"Slice: \" + str(i))\n            img.set_data(np.take(array, i, axis=iteration_axis))\n            return img\n        # Compute the animation (gif)\n        ani = animation.FuncAnimation(fig, update, \n                                    frames=array.shape[iteration_axis],\n                                    interval=5, \n                                    repeat_delay=0, \n                                    blit=False)\n        # Visualize or store gif\n        if out_path is None : plt.show()\n        else : ani.save(out_path, writer='imagemagick', fps=None, dpi=None)\n        # Close the matplot\n        plt.close()\n    # Encode as NumPy file\n    elif format == \"npy\" : np.save(out_path, array)\n    # Encode as ITK file\n    else:\n        array_sitk = sitk.GetImageFromArray(array)\n        sitk.WriteImage(array_sitk, out_path)\n</code></pre>"},{"location":"reference/xai/","title":"Xai","text":"<p>Core interface for XAI in AUCMEDI: aucmedi.xai.decoder.xai_decoder</p> XAI Methods <p>The XAI Decoder can be run with different XAI methods as backbone.</p> <p>A list of all implemented methods and their keys can be found here:  aucmedi.xai.methods</p> Example <pre><code># Create a DataGenerator for data I/O\ndatagen = DataGenerator(samples[:3], \"images_xray/\", labels=None, resize=(299, 299))\n\n# Get a model\nmodel = NeuralNetwork(n_labels=3, channels=3, architecture=\"Xception\",\n                       input_shape=(299,299))\nmodel.load(\"model.xray.keras\")\n\n# Make some predictions\npreds = model.predict(datagen)\n\n# Compute XAI heatmaps via Grad-CAM (resulting heatmaps are stored in out_path)\nxai_decoder(datagen, model, preds, method=\"gradcam\", out_path=\"xai.xray_gradcam\")\n</code></pre>"},{"location":"reference/xai/decoder/","title":"Decoder","text":""},{"location":"reference/xai/decoder/#aucmedi.xai.decoder.xai_decoder","title":"<code>xai_decoder(data_gen, model, preds=None, method='gradcam', layerName=None, overlay=True, alpha=0.25, preprocess_overlay=True, out_path=None)</code>","text":"<p>XAI Decoder function for automatic computation of Explainable AI heatmaps.</p> <p>This module allows to visualize which regions were crucial for the neural network model to compute a classification on the provided unknown images.</p> <ul> <li>If <code>out_path</code> parameter is None, heatmaps are returned as NumPy array.</li> <li>If a path is provided as <code>out_path</code>, then heatmaps are stored to disk as PNG files.</li> </ul> XAI Methods <p>The XAI Decoder can be run with different XAI methods as backbone.</p> <p>A list of all implemented methods and their keys can be found here:  aucmedi.xai.methods</p> Parameter: preprocess_overlay <p>The XAI method computation is based on the fully preprocessed image. However, sometimes it is needed to map the resulting XAI map to the original image.</p> <p>Subfunctions which drastically alter the image resolution like cropping lead to  an incorrect mapping process which is why a slight preprocessing of the images, on which the XAI heatmap is overlayed, is recommended.</p> Example <pre><code># Create a DataGenerator for data I/O\ndatagen = DataGenerator(samples[:3], \"images_xray/\", labels=None, resize=(299, 299))\n\n# Get a model\nmodel = NeuralNetwork(n_labels=3, channels=3, architecture=\"Xception\",\n                       input_shape=(299,299))\nmodel.load(\"model.xray.keras\")\n\n# Make some predictions\npreds = model.predict(datagen)\n\n# Compute XAI heatmaps via Grad-CAM (resulting heatmaps are stored in out_path)\nxai_decoder(datagen, model, preds, method=\"gradcam\", out_path=\"xai.xray_gradcam\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_gen</code> <code>DataGenerator</code> <p>A data generator which will be used for inference.</p> required <code>model</code> <code>NeuralNetwork</code> <p>Instance of a AUCMEDI neural network class.</p> required <code>preds</code> <code>numpy.ndarray</code> <p>NumPy Array of classification prediction encoded as OHE (output of a AUCMEDI prediction).</p> <code>None</code> <code>method</code> <code>str</code> <p>XAI method class instance or index. By default, GradCAM is used as XAI method.</p> <code>'gradcam'</code> <code>layerName</code> <code>str</code> <p>Layer name of the convolutional layer for heatmap computation. If <code>None</code>, the last conv layer is used.</p> <code>None</code> <code>overlay</code> <code>bool</code> <p>Switch deciding if XAI heatmap should be plotted as overlap on the original image.                                 If <code>False</code>, only the XAI heatmap will be stroed.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency value for heatmap overlap plotting on input image (range: [0-1]).</p> <code>0.25</code> <code>preprocess_overlay</code> <code>bool</code> <p>Switch for Subfunction application on visualization. Only relevant if heatmaps are saved to disk.</p> <code>True</code> <code>out_path</code> <code>str</code> <p>Output path in which heatmaps are saved to disk as provided <code>image_format</code> (DataGenerator).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>images</code> <code>numpy.ndarray</code> <p>Combined array of images. Will be only returned if <code>out_path</code> parameter is <code>None</code>.</p> <code>heatmaps</code> <code>numpy.ndarray</code> <p>Combined array of XAI heatmaps. Will be only returned if <code>out_path</code> parameter is <code>None</code>.</p> Source code in <code>aucmedi/xai/decoder.py</code> <pre><code>def xai_decoder(data_gen, model, preds=None, method=\"gradcam\", layerName=None,\n                overlay=True, alpha=0.25, preprocess_overlay=True, out_path=None):\n    \"\"\" XAI Decoder function for automatic computation of Explainable AI heatmaps.\n\n    This module allows to visualize which regions were crucial for the neural network model\n    to compute a classification on the provided unknown images.\n\n    - If `out_path` parameter is None, heatmaps are returned as NumPy array.\n    - If a path is provided as `out_path`, then heatmaps are stored to disk as PNG files.\n\n    ???+ info \"XAI Methods\"\n        The XAI Decoder can be run with different XAI methods as backbone.\n\n        A list of all implemented methods and their keys can be found here: &lt;br&gt;\n        [aucmedi.xai.methods][]\n\n    ??? info \"Parameter: preprocess_overlay\"\n        The XAI method computation is based on the fully preprocessed image.\n        However, sometimes it is needed to map the resulting XAI map to the original image.\n\n        Subfunctions which drastically alter the image resolution like cropping lead to \n        an incorrect mapping process which is why a slight preprocessing of the images, on\n        which the XAI heatmap is overlayed, is recommended.\n\n    ???+ example \"Example\"\n        ```python\n        # Create a DataGenerator for data I/O\n        datagen = DataGenerator(samples[:3], \"images_xray/\", labels=None, resize=(299, 299))\n\n        # Get a model\n        model = NeuralNetwork(n_labels=3, channels=3, architecture=\"Xception\",\n                               input_shape=(299,299))\n        model.load(\"model.xray.keras\")\n\n        # Make some predictions\n        preds = model.predict(datagen)\n\n        # Compute XAI heatmaps via Grad-CAM (resulting heatmaps are stored in out_path)\n        xai_decoder(datagen, model, preds, method=\"gradcam\", out_path=\"xai.xray_gradcam\")\n        ```\n\n    Args:\n        data_gen (DataGenerator):           A data generator which will be used for inference.\n        model (NeuralNetwork):              Instance of a AUCMEDI neural network class.\n        preds (numpy.ndarray):              NumPy Array of classification prediction encoded as OHE (output of a AUCMEDI prediction).\n        method (str):                       XAI method class instance or index. By default, GradCAM is used as XAI method.\n        layerName (str):                    Layer name of the convolutional layer for heatmap computation. If `None`, the last conv layer is used.\n        overlay (bool):                     Switch deciding if XAI heatmap should be plotted as overlap on the original image.\n                                            If `False`, only the XAI heatmap will be stroed.\n        alpha (float):                      Transparency value for heatmap overlap plotting on input image (range: [0-1]).\n        preprocess_overlay (bool):          Switch for Subfunction application on visualization. Only relevant if heatmaps are saved to disk.\n        out_path (str):                     Output path in which heatmaps are saved to disk as provided `image_format` (DataGenerator).\n\n    Returns:\n        images (numpy.ndarray):             Combined array of images. Will be only returned if `out_path` parameter is `None`.\n        heatmaps (numpy.ndarray):           Combined array of XAI heatmaps. Will be only returned if `out_path` parameter is `None`.\n    \"\"\"\n    # Initialize &amp; access some variables\n    batch_size = data_gen.batch_size\n    n_classes = model.n_labels\n    sample_list = data_gen.samples\n    # Prepare XAI output methods\n    res_img = []\n    res_xai = []\n    if out_path is not None and not os.path.exists(out_path) : os.mkdir(out_path)\n    # Initialize xai method\n    if isinstance(method, str) and method in xai_dict:\n        xai_method = xai_dict[method](model.model, layerName=layerName)\n    else : xai_method = method\n\n    # Iterate over all samples\n    for i in range(0, len(sample_list)):\n        # Load overlay image\n        if preprocess_overlay:\n            img_org = data_gen.preprocess_image(i, \n                                                run_resize=False, \n                                                run_aug=False, \n                                                run_standardize=False)\n            shape_org = img_org.shape[0:-1]\n        # Load original image\n        else:\n            img_org = data_gen.sample_loader(sample_list[i], \n                                    data_gen.path_imagedir,\n                                    image_format=data_gen.image_format,\n                                    grayscale=data_gen.grayscale,\n                                    **data_gen.kwargs)\n            shape_org = img_org.shape[0:-1]\n\n        # Load processed image\n        img_prc = data_gen.preprocess_image(i, run_aug=False)\n        img_batch = np.expand_dims(img_prc, axis=0)\n        # If preds given, compute heatmap only for argmax class\n        if preds is not None:\n            ci = np.argmax(preds[i])\n            xai_map = xai_method.compute_heatmap(img_batch, class_index=ci)\n            xai_map = Resize(shape=shape_org).transform(xai_map)\n            postprocess_output(sample_list[i], img_org, xai_map, \n                               n_classes, data_gen, res_img, res_xai, \n                               overlay, out_path, alpha)\n        # If no preds given, compute heatmap for all classes\n        else:\n            sample_maps = []\n            for ci in range(0, n_classes):\n                xai_map = xai_method.compute_heatmap(img_batch, class_index=ci)\n                xai_map = Resize(shape=shape_org).transform(xai_map)\n                sample_maps.append(xai_map)\n            sample_maps = np.array(sample_maps)\n            postprocess_output(sample_list[i], img_org, sample_maps, \n                               n_classes, data_gen, res_img, res_xai, \n                               overlay, out_path, alpha)\n    # Return output directly if no output path is defined\n    if out_path is None : return res_img, res_xai\n</code></pre>"},{"location":"reference/xai/methods/","title":"Methods","text":""},{"location":"reference/xai/methods/#aucmedi.xai.methods.xai_dict","title":"<code>xai_dict = {'gradcam': GradCAM, 'GradCAM': GradCAM, 'gc': GradCAM, 'gradcam++': GradCAMpp, 'GradCAM++': GradCAMpp, 'GradCAMpp': GradCAMpp, 'gc++': GradCAMpp, 'GuidedGradCAM': GuidedGradCAM, 'ggc': GuidedGradCAM, 'saliency': SaliencyMap, 'SaliencyMap': SaliencyMap, 'sm': SaliencyMap, 'guidedbackprop': GuidedBackpropagation, 'GuidedBackpropagation': GuidedBackpropagation, 'gb': GuidedBackpropagation, 'IntegratedGradients': IntegratedGradients, 'ig': IntegratedGradients, 'OcclusionSensitivity': OcclusionSensitivity, 'os': OcclusionSensitivity, 'LimePro': LimePro, 'lp': LimePro, 'LimeCon': LimeCon, 'lc': LimeCon}</code>  <code>module-attribute</code>","text":"<p>Dictionary of implemented XAI Methods in AUCMEDI.</p> <p>A key (str) or an initialized XAI Method can be passed to the aucmedi.xai.decoder.xai_decoder function as method parameter.</p> Example <pre><code># Select desired XAI Methods\nxai_list = [\"gradcam\", \"gc++\", OcclusionSensitivity(model), xai_dict[\"LimePro\"](model), \"lc\"]\n\n# Iterate over each method\nfor m in xai_list:\n    # Compute XAI heatmaps with method m\n    heatmaps = xai_decoder(datagen, model, preds, method=m)\n</code></pre> <p>XAI Methods are based on the abstract base class aucmedi.xai.methods.xai_base.</p>"},{"location":"reference/xai/methods/gradcam/","title":"Gradcam","text":""},{"location":"reference/xai/methods/gradcam/#aucmedi.xai.methods.gradcam.GradCAM","title":"<code>GradCAM</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Gradient-weighted Class Activation Mapping (Grad-CAM).</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation #1 <p>Author: Fran\u00e7ois Chollet  Date: April 26, 2020  https://keras.io/examples/vision/grad_cam/ </p> Reference - Implementation #2 <p>Author: Adrian Rosebrock  Date: March 9, 2020  https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/ </p> Reference - Publication <p>Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. 7 Oct 2016. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.  https://arxiv.org/abs/1610.02391</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Grad-CAM heatmap for an image with a model.</p> Source code in <code>aucmedi/xai/methods/gradcam.py</code> <pre><code>class GradCAM(XAImethod_Base):\n    \"\"\" XAI Method for Gradient-weighted Class Activation Mapping (Grad-CAM).\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation #1\"\n        Author: Fran\u00e7ois Chollet &lt;br&gt;\n        Date: April 26, 2020 &lt;br&gt;\n        [https://keras.io/examples/vision/grad_cam/](https://keras.io/examples/vision/grad_cam/) &lt;br&gt;\n\n    ??? abstract \"Reference - Implementation #2\"\n        Author: Adrian Rosebrock &lt;br&gt;\n        Date: March 9, 2020 &lt;br&gt;\n        [https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/](https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. 7 Oct 2016.\n        Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Grad-CAM heatmap for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating a Grad-CAM as XAI Method object.\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.layerName = layerName\n        # Try to find output layer if not defined\n        if self.layerName is None : self.layerName = self.find_output_layer()\n\n    #---------------------------------------------#\n    #            Identify Output Layer            #\n    #---------------------------------------------#\n    def find_output_layer(self):\n        \"\"\" Internal function. Applied if `layerName==None`.\n\n        Identify last/final convolutional layer in neural network architecture.\n        This layer is used to obtain activation outputs / feature map.\n        \"\"\"\n        # Iterate over all layers\n        for layer in reversed(self.model.layers):\n            # Check to see if the layer has a 4D output -&gt; Return layer\n            if len(layer.output.shape) &gt;= 4:\n                return layer.name\n        # Otherwise, throw exception\n        raise ValueError(\"Could not find 4D layer. Cannot apply Grad-CAM.\")\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Grad-CAM heatmap for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Grad-CAM for provided image.\n        \"\"\"\n        # Gradient model construction\n        layer_output = self.model.get_layer(self.layerName).output\n        model_output = self.model.output\n        if isinstance(model_output, list):\n            outputs = [layer_output] + model_output\n        else:\n            outputs = [layer_output, model_output]\n\n        gradModel = tf.keras.models.Model(inputs=self.model.inputs,\n                         outputs=outputs)\n        # Compute gradient for desired class index\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (conv_out, preds) = gradModel(inputs)\n            loss = preds[:, class_index]\n        grads = tape.gradient(loss, conv_out)\n        # Identify pooling axis\n        if len(image.shape) == 4 : pooling_axis = (0, 1, 2)\n        else : pooling_axis = (0, 1, 2, 3)\n        # Averaged output gradient based on feature map of last conv layer\n        pooled_grads = tf.reduce_mean(grads, axis=pooling_axis)\n        # Normalize gradients via \"importance\"\n        heatmap = conv_out[0] @ pooled_grads[..., tf.newaxis]\n        heatmap = tf.squeeze(heatmap).numpy()\n\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam/#aucmedi.xai.methods.gradcam.GradCAM.__init__","title":"<code>__init__(model, layerName=None)</code>","text":"<p>Initialization function for creating a Grad-CAM as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Layer name of the convolutional layer for heatmap computation.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/gradcam.py</code> <pre><code>def __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating a Grad-CAM as XAI Method object.\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.layerName = layerName\n    # Try to find output layer if not defined\n    if self.layerName is None : self.layerName = self.find_output_layer()\n</code></pre>"},{"location":"reference/xai/methods/gradcam/#aucmedi.xai.methods.gradcam.GradCAM.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Grad-CAM heatmap for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Grad-CAM for provided image.</p> Source code in <code>aucmedi/xai/methods/gradcam.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Grad-CAM heatmap for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Grad-CAM for provided image.\n    \"\"\"\n    # Gradient model construction\n    layer_output = self.model.get_layer(self.layerName).output\n    model_output = self.model.output\n    if isinstance(model_output, list):\n        outputs = [layer_output] + model_output\n    else:\n        outputs = [layer_output, model_output]\n\n    gradModel = tf.keras.models.Model(inputs=self.model.inputs,\n                     outputs=outputs)\n    # Compute gradient for desired class index\n    with tf.GradientTape() as tape:\n        inputs = tf.cast(image, tf.float32)\n        (conv_out, preds) = gradModel(inputs)\n        loss = preds[:, class_index]\n    grads = tape.gradient(loss, conv_out)\n    # Identify pooling axis\n    if len(image.shape) == 4 : pooling_axis = (0, 1, 2)\n    else : pooling_axis = (0, 1, 2, 3)\n    # Averaged output gradient based on feature map of last conv layer\n    pooled_grads = tf.reduce_mean(grads, axis=pooling_axis)\n    # Normalize gradients via \"importance\"\n    heatmap = conv_out[0] @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap).numpy()\n\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam/#aucmedi.xai.methods.gradcam.GradCAM.find_output_layer","title":"<code>find_output_layer()</code>","text":"<p>Internal function. Applied if <code>layerName==None</code>.</p> <p>Identify last/final convolutional layer in neural network architecture. This layer is used to obtain activation outputs / feature map.</p> Source code in <code>aucmedi/xai/methods/gradcam.py</code> <pre><code>def find_output_layer(self):\n    \"\"\" Internal function. Applied if `layerName==None`.\n\n    Identify last/final convolutional layer in neural network architecture.\n    This layer is used to obtain activation outputs / feature map.\n    \"\"\"\n    # Iterate over all layers\n    for layer in reversed(self.model.layers):\n        # Check to see if the layer has a 4D output -&gt; Return layer\n        if len(layer.output.shape) &gt;= 4:\n            return layer.name\n    # Otherwise, throw exception\n    raise ValueError(\"Could not find 4D layer. Cannot apply Grad-CAM.\")\n</code></pre>"},{"location":"reference/xai/methods/gradcam_guided/","title":"Gradcam guided","text":""},{"location":"reference/xai/methods/gradcam_guided/#aucmedi.xai.methods.gradcam_guided.GuidedGradCAM","title":"<code>GuidedGradCAM</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Guided Grad-CAM.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Author: Swapnil Ahlawat  Date: Jul 06, 2020  https://github.com/swapnil-ahlawat/Guided-GradCAM-Keras </p> Reference - Publication #1 <p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller. 21 Dec 2014. Striving for Simplicity: The All Convolutional Net.  https://arxiv.org/abs/1412.6806</p> Reference - Publication #2 <p>Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. 7 Oct 2016. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.  https://arxiv.org/abs/1610.02391</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Guided Grad-CAM heatmap for an image with a model.</p> Source code in <code>aucmedi/xai/methods/gradcam_guided.py</code> <pre><code>class GuidedGradCAM(XAImethod_Base):\n    \"\"\" XAI Method for Guided Grad-CAM.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Swapnil Ahlawat &lt;br&gt;\n        Date: Jul 06, 2020 &lt;br&gt;\n        [https://github.com/swapnil-ahlawat/Guided-GradCAM-Keras](https://github.com/swapnil-ahlawat/Guided-GradCAM-Keras) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication #1\"\n        Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller. 21 Dec 2014.\n        Striving for Simplicity: The All Convolutional Net.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)\n\n    ??? abstract \"Reference - Publication #2\"\n        Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. 7 Oct 2016.\n        Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Guided Grad-CAM heatmap for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating a Guided Grad-CAM as XAI Method object.\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n        \"\"\"\n        # Initialize XAI methods\n        self.bp = GuidedBackpropagation(model, layerName)\n        self.gc = GradCAM(model, layerName)\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Guided Grad-CAM heatmap for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Guided Grad-CAM for provided image.\n        \"\"\"\n        # Compute Guided Backpropagation\n        hm_bp = self.bp.compute_heatmap(image, class_index, eps)\n        # Compute Grad-CAM\n        hm_gc = self.gc.compute_heatmap(image, class_index, eps)\n        hm_gc = Resize(shape=image.shape[1:-1]).transform(hm_gc)\n        # Combine both XAI methods\n        heatmap = hm_bp * hm_gc\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam_guided/#aucmedi.xai.methods.gradcam_guided.GuidedGradCAM.__init__","title":"<code>__init__(model, layerName=None)</code>","text":"<p>Initialization function for creating a Guided Grad-CAM as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Layer name of the convolutional layer for heatmap computation.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/gradcam_guided.py</code> <pre><code>def __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating a Guided Grad-CAM as XAI Method object.\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n    \"\"\"\n    # Initialize XAI methods\n    self.bp = GuidedBackpropagation(model, layerName)\n    self.gc = GradCAM(model, layerName)\n</code></pre>"},{"location":"reference/xai/methods/gradcam_guided/#aucmedi.xai.methods.gradcam_guided.GuidedGradCAM.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Guided Grad-CAM heatmap for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Guided Grad-CAM for provided image.</p> Source code in <code>aucmedi/xai/methods/gradcam_guided.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Guided Grad-CAM heatmap for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Guided Grad-CAM for provided image.\n    \"\"\"\n    # Compute Guided Backpropagation\n    hm_bp = self.bp.compute_heatmap(image, class_index, eps)\n    # Compute Grad-CAM\n    hm_gc = self.gc.compute_heatmap(image, class_index, eps)\n    hm_gc = Resize(shape=image.shape[1:-1]).transform(hm_gc)\n    # Combine both XAI methods\n    heatmap = hm_bp * hm_gc\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam_pp/","title":"Gradcam pp","text":""},{"location":"reference/xai/methods/gradcam_pp/#aucmedi.xai.methods.gradcam_pp.GradCAMpp","title":"<code>GradCAMpp</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Grad-CAM++.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Author: Samson Woof  GitHub Profile: https://github.com/samson6460  Date: May 21, 2020  https://github.com/samson6460/tf_keras_gradcamplusplus </p> Reference - Publication <p>Aditya Chattopadhay; Anirban Sarkar; Prantik Howlader; Vineeth N Balasubramanian. 07 May 2018. Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks.  https://ieeexplore.ieee.org/document/8354201</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Grad-CAM++ heatmap for an image with a model.</p> Source code in <code>aucmedi/xai/methods/gradcam_pp.py</code> <pre><code>class GradCAMpp(XAImethod_Base):\n    \"\"\" XAI Method for Grad-CAM++.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Samson Woof &lt;br&gt;\n        GitHub Profile: [https://github.com/samson6460](https://github.com/samson6460) &lt;br&gt;\n        Date: May 21, 2020 &lt;br&gt;\n        [https://github.com/samson6460/tf_keras_gradcamplusplus](https://github.com/samson6460/tf_keras_gradcamplusplus) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Aditya Chattopadhay; Anirban Sarkar; Prantik Howlader; Vineeth N Balasubramanian. 07 May 2018.\n        Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks.\n        &lt;br&gt;\n        [https://ieeexplore.ieee.org/document/8354201](https://ieeexplore.ieee.org/document/8354201)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Grad-CAM++ heatmap for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating a Grad-CAM++ as XAI Method object.\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.layerName = layerName\n        # Try to find output layer if not defined\n        if self.layerName is None : self.layerName = self.find_output_layer()\n\n    #---------------------------------------------#\n    #            Identify Output Layer            #\n    #---------------------------------------------#\n    def find_output_layer(self):\n        \"\"\" Internal function. Applied if `layerName==None`.\n\n        Identify last/final convolutional layer in neural network architecture.\n        This layer is used to obtain activation outputs / feature map.\n        \"\"\"\n        # Iterate over all layers\n        for layer in reversed(self.model.layers):\n            # Check to see if the layer has a 4D output -&gt; Return layer\n            if len(layer.output.shape) &gt;= 4:\n                return layer.name\n        # Otherwise, throw exception\n        raise ValueError(\"Could not find 4D layer. Cannot apply Grad-CAM++.\")\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Grad-CAM++ heatmap for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Grad-CAM++ for provided image.\n        \"\"\"\n        # Gradient model construction\n        layer_output = self.model.get_layer(self.layerName).output\n        model_output = self.model.output\n        if isinstance(model_output, list):\n            outputs = [layer_output] + model_output\n        else:\n            outputs = [layer_output, model_output]\n\n        gradModel = tf.keras.models.Model(inputs=self.model.inputs,\n                         outputs=outputs)\n\n        # Compute gradient for desierd class index\n        with tf.GradientTape() as gtape1:\n            with tf.GradientTape() as gtape2:\n                with tf.GradientTape() as gtape3:\n                    inputs = tf.cast(image, tf.float32)\n                    (conv_output, preds) = gradModel(inputs)\n                    output = preds[:, class_index]\n                    conv_first_grad = gtape3.gradient(output, conv_output)\n                conv_second_grad = gtape2.gradient(conv_first_grad, conv_output)\n            conv_third_grad = gtape1.gradient(conv_second_grad, conv_output)\n        global_sum = np.sum(conv_output, axis=(0, 1, 2))\n\n        # Normalize constants\n        alpha_num = conv_second_grad[0]\n        alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum\n        alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, eps)\n        alphas = alpha_num / alpha_denom\n        alpha_normalization_constant = np.sum(alphas, axis=(0,1))\n        alphas /= alpha_normalization_constant\n\n        # Deep Linearization weighting\n        weights = np.maximum(conv_first_grad[0], 0.0)\n        deep_linearization_weights = np.sum(weights*alphas, axis=(0,1))\n        heatmap = np.sum(deep_linearization_weights*conv_output[0], axis=-1)\n\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam_pp/#aucmedi.xai.methods.gradcam_pp.GradCAMpp.__init__","title":"<code>__init__(model, layerName=None)</code>","text":"<p>Initialization function for creating a Grad-CAM++ as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Layer name of the convolutional layer for heatmap computation.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/gradcam_pp.py</code> <pre><code>def __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating a Grad-CAM++ as XAI Method object.\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                   Layer name of the convolutional layer for heatmap computation.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.layerName = layerName\n    # Try to find output layer if not defined\n    if self.layerName is None : self.layerName = self.find_output_layer()\n</code></pre>"},{"location":"reference/xai/methods/gradcam_pp/#aucmedi.xai.methods.gradcam_pp.GradCAMpp.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Grad-CAM++ heatmap for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Grad-CAM++ for provided image.</p> Source code in <code>aucmedi/xai/methods/gradcam_pp.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Grad-CAM++ heatmap for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Grad-CAM++ for provided image.\n    \"\"\"\n    # Gradient model construction\n    layer_output = self.model.get_layer(self.layerName).output\n    model_output = self.model.output\n    if isinstance(model_output, list):\n        outputs = [layer_output] + model_output\n    else:\n        outputs = [layer_output, model_output]\n\n    gradModel = tf.keras.models.Model(inputs=self.model.inputs,\n                     outputs=outputs)\n\n    # Compute gradient for desierd class index\n    with tf.GradientTape() as gtape1:\n        with tf.GradientTape() as gtape2:\n            with tf.GradientTape() as gtape3:\n                inputs = tf.cast(image, tf.float32)\n                (conv_output, preds) = gradModel(inputs)\n                output = preds[:, class_index]\n                conv_first_grad = gtape3.gradient(output, conv_output)\n            conv_second_grad = gtape2.gradient(conv_first_grad, conv_output)\n        conv_third_grad = gtape1.gradient(conv_second_grad, conv_output)\n    global_sum = np.sum(conv_output, axis=(0, 1, 2))\n\n    # Normalize constants\n    alpha_num = conv_second_grad[0]\n    alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum\n    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, eps)\n    alphas = alpha_num / alpha_denom\n    alpha_normalization_constant = np.sum(alphas, axis=(0,1))\n    alphas /= alpha_normalization_constant\n\n    # Deep Linearization weighting\n    weights = np.maximum(conv_first_grad[0], 0.0)\n    deep_linearization_weights = np.sum(weights*alphas, axis=(0,1))\n    heatmap = np.sum(deep_linearization_weights*conv_output[0], axis=-1)\n\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/gradcam_pp/#aucmedi.xai.methods.gradcam_pp.GradCAMpp.find_output_layer","title":"<code>find_output_layer()</code>","text":"<p>Internal function. Applied if <code>layerName==None</code>.</p> <p>Identify last/final convolutional layer in neural network architecture. This layer is used to obtain activation outputs / feature map.</p> Source code in <code>aucmedi/xai/methods/gradcam_pp.py</code> <pre><code>def find_output_layer(self):\n    \"\"\" Internal function. Applied if `layerName==None`.\n\n    Identify last/final convolutional layer in neural network architecture.\n    This layer is used to obtain activation outputs / feature map.\n    \"\"\"\n    # Iterate over all layers\n    for layer in reversed(self.model.layers):\n        # Check to see if the layer has a 4D output -&gt; Return layer\n        if len(layer.output.shape) &gt;= 4:\n            return layer.name\n    # Otherwise, throw exception\n    raise ValueError(\"Could not find 4D layer. Cannot apply Grad-CAM++.\")\n</code></pre>"},{"location":"reference/xai/methods/guided_backprop/","title":"Guided backprop","text":""},{"location":"reference/xai/methods/guided_backprop/#aucmedi.xai.methods.guided_backprop.GuidedBackpropagation","title":"<code>GuidedBackpropagation</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Guided Backpropagation.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation #1 <p>Author: Hoa Nguyen  GitHub Profile: https://nguyenhoa93.github.io/  Date: Jul 29, 2020  https://stackoverflow.com/questions/55924331/how-to-apply-guided-backprop-in-tensorflow-2-0 </p> Reference - Implementation #2 <p>Author: Huynh Ngoc Anh  GitHub Profile: https://github.com/experiencor  Date: Jun 23, 2017  https://github.com/experiencor/deep-viz-keras/ </p> Reference - Implementation #3 <p>Author: Tim  Date: Jan 25, 2019  https://stackoverflow.com/questions/54366935/make-a-deep-copy-of-a-keras-model-in-python </p> Reference - Publication <p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller. 21 Dec 2014. Striving for Simplicity: The All Convolutional Net.  https://arxiv.org/abs/1412.6806</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Guided Backpropagation for an image with a model.</p> Source code in <code>aucmedi/xai/methods/guided_backprop.py</code> <pre><code>class GuidedBackpropagation(XAImethod_Base):\n    \"\"\" XAI Method for Guided Backpropagation.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation #1\"\n        Author: Hoa Nguyen &lt;br&gt;\n        GitHub Profile: [https://nguyenhoa93.github.io/](https://nguyenhoa93.github.io/) &lt;br&gt;\n        Date: Jul 29, 2020 &lt;br&gt;\n        [https://stackoverflow.com/questions/55924331/how-to-apply-guided-backprop-in-tensorflow-2-0](https://stackoverflow.com/questions/55924331/how-to-apply-guided-backprop-in-tensorflow-2-0) &lt;br&gt;\n\n    ??? abstract \"Reference - Implementation #2\"\n        Author: Huynh Ngoc Anh &lt;br&gt;\n        GitHub Profile: [https://github.com/experiencor](https://github.com/experiencor) &lt;br&gt;\n        Date: Jun 23, 2017 &lt;br&gt;\n        [https://github.com/experiencor/deep-viz-keras/](https://github.com/experiencor/deep-viz-keras/) &lt;br&gt;\n\n    ??? abstract \"Reference - Implementation #3\"\n        Author: Tim &lt;br&gt;\n        Date: Jan 25, 2019 &lt;br&gt;\n        [https://stackoverflow.com/questions/54366935/make-a-deep-copy-of-a-keras-model-in-python](https://stackoverflow.com/questions/54366935/make-a-deep-copy-of-a-keras-model-in-python) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller. 21 Dec 2014.\n        Striving for Simplicity: The All Convolutional Net.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Guided Backpropagation for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating Guided Backpropagation as XAI Method object.\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                   Not required in Guided Backpropagation, but defined by Abstract Base Class.\n        \"\"\"\n        # Create a deep copy of the model\n        model_copy = tf.keras.models.clone_model(model)\n        model_copy.build(model.input.shape)\n        model_copy.compile(optimizer=model.optimizer, loss=model.loss)\n        model_copy.set_weights(model.get_weights())\n\n        # Define custom Relu activation function\n        @tf.custom_gradient\n        def guidedRelu(x):\n            def grad(dy):\n                return tf.cast(dy&gt;0, \"float32\") * tf.cast(x&gt;0, \"float32\") * dy\n            return tf.nn.relu(x), grad\n        # Replace Relu activation layers with custom Relu activation layer\n        layer_dict = [layer for layer in model_copy.layers if hasattr(layer, \"activation\")]\n        for layer in layer_dict:\n            if layer.activation == tf.keras.activations.relu:\n                layer.activation = guidedRelu\n        # Cache class parameters\n        self.model = model_copy\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Guided Backpropagation for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Guided Backpropagation for provided image.\n        \"\"\"\n        # Compute gradient for desierd class index\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            tape.watch(inputs)\n            preds = self.model(inputs)\n            loss = preds[:, class_index]\n        gradient = tape.gradient(loss, inputs)\n        # Obtain maximum gradient based on feature map of last conv layer\n        gradient = tf.reduce_max(gradient, axis=-1)\n        # Convert to NumPy &amp; Remove batch axis\n        heatmap = gradient.numpy()[0]\n\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/guided_backprop/#aucmedi.xai.methods.guided_backprop.GuidedBackpropagation.__init__","title":"<code>__init__(model, layerName=None)</code>","text":"<p>Initialization function for creating Guided Backpropagation as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in Guided Backpropagation, but defined by Abstract Base Class.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/guided_backprop.py</code> <pre><code>def __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating Guided Backpropagation as XAI Method object.\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                   Not required in Guided Backpropagation, but defined by Abstract Base Class.\n    \"\"\"\n    # Create a deep copy of the model\n    model_copy = tf.keras.models.clone_model(model)\n    model_copy.build(model.input.shape)\n    model_copy.compile(optimizer=model.optimizer, loss=model.loss)\n    model_copy.set_weights(model.get_weights())\n\n    # Define custom Relu activation function\n    @tf.custom_gradient\n    def guidedRelu(x):\n        def grad(dy):\n            return tf.cast(dy&gt;0, \"float32\") * tf.cast(x&gt;0, \"float32\") * dy\n        return tf.nn.relu(x), grad\n    # Replace Relu activation layers with custom Relu activation layer\n    layer_dict = [layer for layer in model_copy.layers if hasattr(layer, \"activation\")]\n    for layer in layer_dict:\n        if layer.activation == tf.keras.activations.relu:\n            layer.activation = guidedRelu\n    # Cache class parameters\n    self.model = model_copy\n</code></pre>"},{"location":"reference/xai/methods/guided_backprop/#aucmedi.xai.methods.guided_backprop.GuidedBackpropagation.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Guided Backpropagation for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Guided Backpropagation for provided image.</p> Source code in <code>aucmedi/xai/methods/guided_backprop.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Guided Backpropagation for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D or 3D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Guided Backpropagation for provided image.\n    \"\"\"\n    # Compute gradient for desierd class index\n    with tf.GradientTape() as tape:\n        inputs = tf.cast(image, tf.float32)\n        tape.watch(inputs)\n        preds = self.model(inputs)\n        loss = preds[:, class_index]\n    gradient = tape.gradient(loss, inputs)\n    # Obtain maximum gradient based on feature map of last conv layer\n    gradient = tf.reduce_max(gradient, axis=-1)\n    # Convert to NumPy &amp; Remove batch axis\n    heatmap = gradient.numpy()[0]\n\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/integrated_gradients/","title":"Integrated gradients","text":""},{"location":"reference/xai/methods/integrated_gradients/#aucmedi.xai.methods.integrated_gradients.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Integrated Gradients.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Author: Aakash Kumar Nain  GitHub Profile: https://github.com/AakashKumarNain  Date: Jun 02, 2020  https://keras.io/examples/vision/integrated_gradients </p> Reference - Publication <p>Mukund Sundararajan, Ankur Taly, Qiqi Yan. 04 Mar 2017. Axiomatic Attribution for Deep Networks.  https://arxiv.org/abs/1703.01365</p> <p>This class provides functionality for running the compute_heatmap function, which computes an Integrated Gradients Map for an image with a model.</p> Source code in <code>aucmedi/xai/methods/integrated_gradients.py</code> <pre><code>class IntegratedGradients(XAImethod_Base):\n    \"\"\" XAI Method for Integrated Gradients.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Aakash Kumar Nain &lt;br&gt;\n        GitHub Profile: [https://github.com/AakashKumarNain](https://github.com/AakashKumarNain) &lt;br&gt;\n        Date: Jun 02, 2020 &lt;br&gt;\n        [https://keras.io/examples/vision/integrated_gradients](https://keras.io/examples/vision/integrated_gradients) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Mukund Sundararajan, Ankur Taly, Qiqi Yan. 04 Mar 2017.\n        Axiomatic Attribution for Deep Networks.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes an Integrated Gradients Map for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None, num_steps=50):\n        \"\"\" Initialization function for creating a Integrated Gradients Map as XAI Method object.\n\n        Args:\n            model (keras.model):            Keras model object.\n            layerName (str):                Not required in Integrated Gradients Maps, but defined by Abstract Base Class.\n            num_steps (int):                Number of iterations for interpolation.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.num_steps = num_steps\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Integrated Gradients Map for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Integrated Gradients Map for provided image.\n        \"\"\"\n        # Perform interpolation\n        baseline = np.zeros(image.shape).astype(np.float32)\n        interpolated_imgs = []\n        for step in range(0, self.num_steps + 1):\n            cii = baseline + (step / self.num_steps) * (image - baseline)\n            interpolated_imgs.append(cii)\n        interpolated_imgs = np.array(interpolated_imgs).astype(np.float32)\n\n        # Get the gradients for each interpolated image\n        grads = []\n        for int_img in interpolated_imgs:\n            # Compute gradient\n            with tf.GradientTape() as tape:\n                inputs = tf.cast(int_img, tf.float32)\n                tape.watch(inputs)\n                preds = self.model(inputs)\n                loss = preds[:, class_index]\n            gradient = tape.gradient(loss, inputs)\n            # Add to gradient list\n            grads.append(gradient[0])\n        grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n\n        # Approximate the integral using the trapezoidal rule\n        grads = (grads[:-1] + grads[1:]) / 2.0\n        avg_grads = tf.reduce_mean(grads, axis=0)\n        # Calculate integrated gradients\n        integrated_grads = (image - baseline) * avg_grads\n        # Obtain maximum gradient\n        integrated_grads = tf.reduce_max(integrated_grads, axis=-1)\n\n        # Convert to NumPy &amp; Remove batch axis\n        heatmap = integrated_grads.numpy()[0,:,:]\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/integrated_gradients/#aucmedi.xai.methods.integrated_gradients.IntegratedGradients.__init__","title":"<code>__init__(model, layerName=None, num_steps=50)</code>","text":"<p>Initialization function for creating a Integrated Gradients Map as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in Integrated Gradients Maps, but defined by Abstract Base Class.</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>Number of iterations for interpolation.</p> <code>50</code> Source code in <code>aucmedi/xai/methods/integrated_gradients.py</code> <pre><code>def __init__(self, model, layerName=None, num_steps=50):\n    \"\"\" Initialization function for creating a Integrated Gradients Map as XAI Method object.\n\n    Args:\n        model (keras.model):            Keras model object.\n        layerName (str):                Not required in Integrated Gradients Maps, but defined by Abstract Base Class.\n        num_steps (int):                Number of iterations for interpolation.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.num_steps = num_steps\n</code></pre>"},{"location":"reference/xai/methods/integrated_gradients/#aucmedi.xai.methods.integrated_gradients.IntegratedGradients.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Integrated Gradients Map for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Integrated Gradients Map for provided image.</p> Source code in <code>aucmedi/xai/methods/integrated_gradients.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Integrated Gradients Map for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Integrated Gradients Map for provided image.\n    \"\"\"\n    # Perform interpolation\n    baseline = np.zeros(image.shape).astype(np.float32)\n    interpolated_imgs = []\n    for step in range(0, self.num_steps + 1):\n        cii = baseline + (step / self.num_steps) * (image - baseline)\n        interpolated_imgs.append(cii)\n    interpolated_imgs = np.array(interpolated_imgs).astype(np.float32)\n\n    # Get the gradients for each interpolated image\n    grads = []\n    for int_img in interpolated_imgs:\n        # Compute gradient\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(int_img, tf.float32)\n            tape.watch(inputs)\n            preds = self.model(inputs)\n            loss = preds[:, class_index]\n        gradient = tape.gradient(loss, inputs)\n        # Add to gradient list\n        grads.append(gradient[0])\n    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n\n    # Approximate the integral using the trapezoidal rule\n    grads = (grads[:-1] + grads[1:]) / 2.0\n    avg_grads = tf.reduce_mean(grads, axis=0)\n    # Calculate integrated gradients\n    integrated_grads = (image - baseline) * avg_grads\n    # Obtain maximum gradient\n    integrated_grads = tf.reduce_max(integrated_grads, axis=-1)\n\n    # Convert to NumPy &amp; Remove batch axis\n    heatmap = integrated_grads.numpy()[0,:,:]\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/lime_con/","title":"Lime con","text":""},{"location":"reference/xai/methods/lime_con/#aucmedi.xai.methods.lime_con.LimeCon","title":"<code>LimeCon</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for LIME Con.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Lime: Explaining the predictions of any machine learning classifier  GitHub: https://github.com/marcotcr/lime </p> Reference - Publication <p>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. 9 Aug 2016. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier  https://arxiv.org/abs/1602.04938</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Lime Con Map for an image with a model.</p> Source code in <code>aucmedi/xai/methods/lime_con.py</code> <pre><code>class LimeCon(XAImethod_Base):\n    \"\"\" XAI Method for LIME Con.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Lime: Explaining the predictions of any machine learning classifier &lt;br&gt;\n        GitHub: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. 9 Aug 2016.\n        \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\n        &lt;br&gt;\n        [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Lime Con Map for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None, num_samples=1000):\n        \"\"\" Initialization function for creating a Lime Con Map as XAI Method object.\n\n        Args:\n            model (keras.model):            Keras model object.\n            layerName (str):                Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.\n            num_samples (int):              Number of iterations for LIME instance explanation.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.num_samples = num_samples\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Lime Con Map for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Lime Con Map for provided image.\n        \"\"\"\n        # Initialize LIME explainer\n        explainer = lime_image.LimeImageExplainer()\n        explanation = explainer.explain_instance(image[0].astype(\"double\"),\n                                self.model.predict, hide_color=0,\n                                labels=(class_index,),\n                                num_samples=self.num_samples)\n        # Obtain CON explanation mask\n        temp, mask = explanation.get_image_and_mask(class_index, hide_rest=True,\n                                positive_only=False, negative_only=True)\n        heatmap = mask\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/lime_con/#aucmedi.xai.methods.lime_con.LimeCon.__init__","title":"<code>__init__(model, layerName=None, num_samples=1000)</code>","text":"<p>Initialization function for creating a Lime Con Map as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of iterations for LIME instance explanation.</p> <code>1000</code> Source code in <code>aucmedi/xai/methods/lime_con.py</code> <pre><code>def __init__(self, model, layerName=None, num_samples=1000):\n    \"\"\" Initialization function for creating a Lime Con Map as XAI Method object.\n\n    Args:\n        model (keras.model):            Keras model object.\n        layerName (str):                Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.\n        num_samples (int):              Number of iterations for LIME instance explanation.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.num_samples = num_samples\n</code></pre>"},{"location":"reference/xai/methods/lime_con/#aucmedi.xai.methods.lime_con.LimeCon.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Lime Con Map for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Lime Con Map for provided image.</p> Source code in <code>aucmedi/xai/methods/lime_con.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Lime Con Map for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Lime Con Map for provided image.\n    \"\"\"\n    # Initialize LIME explainer\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(image[0].astype(\"double\"),\n                            self.model.predict, hide_color=0,\n                            labels=(class_index,),\n                            num_samples=self.num_samples)\n    # Obtain CON explanation mask\n    temp, mask = explanation.get_image_and_mask(class_index, hide_rest=True,\n                            positive_only=False, negative_only=True)\n    heatmap = mask\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/lime_pro/","title":"Lime pro","text":""},{"location":"reference/xai/methods/lime_pro/#aucmedi.xai.methods.lime_pro.LimePro","title":"<code>LimePro</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for LIME Pro.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Lime: Explaining the predictions of any machine learning classifier  GitHub: https://github.com/marcotcr/lime </p> Reference - Publication <p>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. 9 Aug 2016. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier  https://arxiv.org/abs/1602.04938</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Lime Pro Map for an image with a model.</p> Source code in <code>aucmedi/xai/methods/lime_pro.py</code> <pre><code>class LimePro(XAImethod_Base):\n    \"\"\" XAI Method for LIME Pro.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Lime: Explaining the predictions of any machine learning classifier &lt;br&gt;\n        GitHub: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. 9 Aug 2016.\n        \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\n        &lt;br&gt;\n        [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Lime Pro Map for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None, num_samples=1000):\n        \"\"\" Initialization function for creating a Lime Pro Map as XAI Method object.\n\n        Args:\n            model (keras.model):            Keras model object.\n            layerName (str):                Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.\n            num_samples (int):              Number of iterations for LIME instance explanation.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.num_samples = num_samples\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Lime Pro Map for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Lime Pro Map for provided image.\n        \"\"\"\n        # Initialize LIME explainer\n        explainer = lime_image.LimeImageExplainer()\n        explanation = explainer.explain_instance(image[0].astype(\"double\"),\n                                self.model.predict, hide_color=0,\n                                labels=(class_index,),\n                                num_samples=self.num_samples)\n        # Obtain PRO explanation mask\n        temp, mask = explanation.get_image_and_mask(class_index, hide_rest=True,\n                                positive_only=True, negative_only=False)\n        heatmap = mask\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/lime_pro/#aucmedi.xai.methods.lime_pro.LimePro.__init__","title":"<code>__init__(model, layerName=None, num_samples=1000)</code>","text":"<p>Initialization function for creating a Lime Pro Map as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of iterations for LIME instance explanation.</p> <code>1000</code> Source code in <code>aucmedi/xai/methods/lime_pro.py</code> <pre><code>def __init__(self, model, layerName=None, num_samples=1000):\n    \"\"\" Initialization function for creating a Lime Pro Map as XAI Method object.\n\n    Args:\n        model (keras.model):            Keras model object.\n        layerName (str):                Not required in LIME Pro/Con Maps, but defined by Abstract Base Class.\n        num_samples (int):              Number of iterations for LIME instance explanation.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.num_samples = num_samples\n</code></pre>"},{"location":"reference/xai/methods/lime_pro/#aucmedi.xai.methods.lime_pro.LimePro.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Lime Pro Map for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Lime Pro Map for provided image.</p> Source code in <code>aucmedi/xai/methods/lime_pro.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Lime Pro Map for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Lime Pro Map for provided image.\n    \"\"\"\n    # Initialize LIME explainer\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(image[0].astype(\"double\"),\n                            self.model.predict, hide_color=0,\n                            labels=(class_index,),\n                            num_samples=self.num_samples)\n    # Obtain PRO explanation mask\n    temp, mask = explanation.get_image_and_mask(class_index, hide_rest=True,\n                            positive_only=True, negative_only=False)\n    heatmap = mask\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/occlusion_sensitivity/","title":"Occlusion sensitivity","text":""},{"location":"reference/xai/methods/occlusion_sensitivity/#aucmedi.xai.methods.occlusion_sensitivity.OcclusionSensitivity","title":"<code>OcclusionSensitivity</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Occlusion Sensitivity.</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation <p>Author: Raphael Meudec  GitHub Profile: https://gist.github.com/RaphaelMeudec  Date: Jul 18, 2019  https://gist.github.com/RaphaelMeudec/7985b0c5eb720a29021d52b0a0be549a </p> <p>This class provides functionality for running the compute_heatmap function, which computes a Occlusion Sensitivity Map for an image with a model.</p> Source code in <code>aucmedi/xai/methods/occlusion_sensitivity.py</code> <pre><code>class OcclusionSensitivity(XAImethod_Base):\n    \"\"\" XAI Method for Occlusion Sensitivity.\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation\"\n        Author: Raphael Meudec &lt;br&gt;\n        GitHub Profile: [https://gist.github.com/RaphaelMeudec](https://gist.github.com/RaphaelMeudec) &lt;br&gt;\n        Date: Jul 18, 2019 &lt;br&gt;\n        [https://gist.github.com/RaphaelMeudec/7985b0c5eb720a29021d52b0a0be549a](https://gist.github.com/RaphaelMeudec/7985b0c5eb720a29021d52b0a0be549a) &lt;br&gt;\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Occlusion Sensitivity Map for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None, patch_size=16):\n        \"\"\" Initialization function for creating a Occlusion Sensitivity Map as XAI Method object.\n\n        Args:\n            model (keras.model):            Keras model object.\n            layerName (str):                Not required in Occlusion Sensitivity Maps, but defined by Abstract Base Class.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n        self.patch_size = patch_size\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Occlusion Sensitivity Map for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Occlusion Sensitivity Map for provided image.\n        \"\"\"\n        # Utilize only image matrix instead of batch\n        image = image[0]\n        # Create empty sensitivity map\n        sensitivity_map = np.zeros((image.shape[0], image.shape[1]))\n        # Iterate the patch over the image\n        for top_left_x in range(0, image.shape[0], self.patch_size):\n            for top_left_y in range(0, image.shape[1], self.patch_size):\n                patch = apply_grey_patch(image, top_left_x, top_left_y,\n                                         self.patch_size)\n                prediction = self.model.predict(np.array([patch]))[0]\n                confidence = prediction[class_index]\n\n                # Save confidence for this specific patch in the map\n                sensitivity_map[\n                    top_left_y:top_left_y + self.patch_size,\n                    top_left_x:top_left_x + self.patch_size,\n                ] = 1 - confidence\n        # Return the resulting sensitivity map (automatically a heatmap)\n        return sensitivity_map\n</code></pre>"},{"location":"reference/xai/methods/occlusion_sensitivity/#aucmedi.xai.methods.occlusion_sensitivity.OcclusionSensitivity.__init__","title":"<code>__init__(model, layerName=None, patch_size=16)</code>","text":"<p>Initialization function for creating a Occlusion Sensitivity Map as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in Occlusion Sensitivity Maps, but defined by Abstract Base Class.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/occlusion_sensitivity.py</code> <pre><code>def __init__(self, model, layerName=None, patch_size=16):\n    \"\"\" Initialization function for creating a Occlusion Sensitivity Map as XAI Method object.\n\n    Args:\n        model (keras.model):            Keras model object.\n        layerName (str):                Not required in Occlusion Sensitivity Maps, but defined by Abstract Base Class.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n    self.patch_size = patch_size\n</code></pre>"},{"location":"reference/xai/methods/occlusion_sensitivity/#aucmedi.xai.methods.occlusion_sensitivity.OcclusionSensitivity.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Occlusion Sensitivity Map for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Occlusion Sensitivity Map for provided image.</p> Source code in <code>aucmedi/xai/methods/occlusion_sensitivity.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Occlusion Sensitivity Map for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Occlusion Sensitivity Map for provided image.\n    \"\"\"\n    # Utilize only image matrix instead of batch\n    image = image[0]\n    # Create empty sensitivity map\n    sensitivity_map = np.zeros((image.shape[0], image.shape[1]))\n    # Iterate the patch over the image\n    for top_left_x in range(0, image.shape[0], self.patch_size):\n        for top_left_y in range(0, image.shape[1], self.patch_size):\n            patch = apply_grey_patch(image, top_left_x, top_left_y,\n                                     self.patch_size)\n            prediction = self.model.predict(np.array([patch]))[0]\n            confidence = prediction[class_index]\n\n            # Save confidence for this specific patch in the map\n            sensitivity_map[\n                top_left_y:top_left_y + self.patch_size,\n                top_left_x:top_left_x + self.patch_size,\n            ] = 1 - confidence\n    # Return the resulting sensitivity map (automatically a heatmap)\n    return sensitivity_map\n</code></pre>"},{"location":"reference/xai/methods/occlusion_sensitivity/#aucmedi.xai.methods.occlusion_sensitivity.apply_grey_patch","title":"<code>apply_grey_patch(image, top_left_x, top_left_y, patch_size)</code>","text":"<p>Internal function.</p> <p>Replace a part of the image with a grey patch.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Input image</p> required <code>top_left_x</code> <code>int</code> <p>Top Left X position of the applied box</p> required <code>top_left_y</code> <code>int</code> <p>Top Left Y position of the applied box</p> required <code>patch_size</code> <code>int</code> <p>Size of patch to apply</p> required <p>Returns:</p> Name Type Description <code>patched_image</code> <code>numpy.ndarray</code> <p>Patched image</p> Source code in <code>aucmedi/xai/methods/occlusion_sensitivity.py</code> <pre><code>def apply_grey_patch(image, top_left_x, top_left_y, patch_size):\n    \"\"\" Internal function.\n\n    Replace a part of the image with a grey patch.\n\n    Args:\n        image (numpy.ndarray):                  Input image\n        top_left_x (int):                       Top Left X position of the applied box\n        top_left_y (int):                       Top Left Y position of the applied box\n        patch_size (int):                       Size of patch to apply\n\n    Returns:\n        patched_image (numpy.ndarray):          Patched image\n    \"\"\"\n    patched_image = np.array(image, copy=True)\n    patched_image[top_left_y:top_left_y + patch_size, top_left_x:top_left_x + patch_size, :] = 127.5\n    return patched_image\n</code></pre>"},{"location":"reference/xai/methods/saliency/","title":"Saliency","text":""},{"location":"reference/xai/methods/saliency/#aucmedi.xai.methods.saliency.SaliencyMap","title":"<code>SaliencyMap</code>","text":"<p>         Bases: <code>XAImethod_Base</code></p> <p>XAI Method for Saliency Map (also called Backpropagation).</p> <p>Normally, this class is used internally in the aucmedi.xai.decoder.xai_decoder in the AUCMEDI XAI module.</p> Reference - Implementation #1 <p>Author: Yasuhiro Kubota  GitHub Profile: https://github.com/keisen  Date: Aug 11, 2020  https://github.com/keisen/tf-keras-vis/ </p> Reference - Implementation #2 <p>Author: Huynh Ngoc Anh  GitHub Profile: https://github.com/experiencor  Date: Jun 23, 2017  https://github.com/experiencor/deep-viz-keras/ </p> Reference - Publication <p>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. 20 Dec 2013. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.  https://arxiv.org/abs/1312.6034</p> <p>This class provides functionality for running the compute_heatmap function, which computes a Saliency Map for an image with a model.</p> Source code in <code>aucmedi/xai/methods/saliency.py</code> <pre><code>class SaliencyMap(XAImethod_Base):\n    \"\"\" XAI Method for Saliency Map (also called Backpropagation).\n\n    Normally, this class is used internally in the [aucmedi.xai.decoder.xai_decoder][] in the AUCMEDI XAI module.\n\n    ??? abstract \"Reference - Implementation #1\"\n        Author: Yasuhiro Kubota &lt;br&gt;\n        GitHub Profile: [https://github.com/keisen](https://github.com/keisen) &lt;br&gt;\n        Date: Aug 11, 2020 &lt;br&gt;\n        [https://github.com/keisen/tf-keras-vis/](https://github.com/keisen/tf-keras-vis/) &lt;br&gt;\n\n    ??? abstract \"Reference - Implementation #2\"\n        Author: Huynh Ngoc Anh &lt;br&gt;\n        GitHub Profile: [https://github.com/experiencor](https://github.com/experiencor) &lt;br&gt;\n        Date: Jun 23, 2017 &lt;br&gt;\n        [https://github.com/experiencor/deep-viz-keras/](https://github.com/experiencor/deep-viz-keras/) &lt;br&gt;\n\n    ??? abstract \"Reference - Publication\"\n        Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. 20 Dec 2013.\n        Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.\n        &lt;br&gt;\n        [https://arxiv.org/abs/1312.6034](https://arxiv.org/abs/1312.6034)\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a Saliency Map for an image with a model.\n    \"\"\"\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating a Saliency Map as XAI Method object.\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                   Not required in Saliency Maps, but defined by Abstract Base Class.\n        \"\"\"\n        # Cache class parameters\n        self.model = model\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the Saliency Map for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap is encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed Saliency Map for provided image.\n        \"\"\"\n        # Compute gradient for desierd class index\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            tape.watch(inputs)\n            preds = self.model(inputs)\n            loss = preds[:, class_index]\n        gradient = tape.gradient(loss, inputs)\n        # Obtain maximum gradient based on feature map of last conv layer\n        gradient = tf.reduce_max(gradient, axis=-1)\n        # Convert to NumPy &amp; Remove batch axis\n        heatmap = gradient.numpy()[0]\n\n        # Intensity normalization to [0,1]\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n\n        # Return the resulting heatmap\n        return heatmap\n</code></pre>"},{"location":"reference/xai/methods/saliency/#aucmedi.xai.methods.saliency.SaliencyMap.__init__","title":"<code>__init__(model, layerName=None)</code>","text":"<p>Initialization function for creating a Saliency Map as XAI Method object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Not required in Saliency Maps, but defined by Abstract Base Class.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/saliency.py</code> <pre><code>def __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating a Saliency Map as XAI Method object.\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                   Not required in Saliency Maps, but defined by Abstract Base Class.\n    \"\"\"\n    # Cache class parameters\n    self.model = model\n</code></pre>"},{"location":"reference/xai/methods/saliency/#aucmedi.xai.methods.saliency.SaliencyMap.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the Saliency Map for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap is encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed Saliency Map for provided image.</p> Source code in <code>aucmedi/xai/methods/saliency.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the Saliency Map for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap is encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed Saliency Map for provided image.\n    \"\"\"\n    # Compute gradient for desierd class index\n    with tf.GradientTape() as tape:\n        inputs = tf.cast(image, tf.float32)\n        tape.watch(inputs)\n        preds = self.model(inputs)\n        loss = preds[:, class_index]\n    gradient = tape.gradient(loss, inputs)\n    # Obtain maximum gradient based on feature map of last conv layer\n    gradient = tf.reduce_max(gradient, axis=-1)\n    # Convert to NumPy &amp; Remove batch axis\n    heatmap = gradient.numpy()[0]\n\n    # Intensity normalization to [0,1]\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n\n    # Return the resulting heatmap\n    return heatmap\n</code></pre>"},{"location":"reference/xai/methods/xai_base/","title":"Xai base","text":""},{"location":"reference/xai/methods/xai_base/#aucmedi.xai.methods.xai_base.XAImethod_Base","title":"<code>XAImethod_Base</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class for a XAI Method class.</p> <p>Normally, this class is used internally in the xai_decoder function in the AUCMEDI XAI module.</p> <p>This class provides functionality for running the compute_heatmap function, which computes a heatmap for an image with a model.</p> Create a custom XAImethod <pre><code>from aucmedi.xai.methods.xai_base import XAImethod_Base\n\nclass My_custom_XAImethod(XAImethod_Base):\n    def __init__(self, model, layerName=None):\n        pass\n\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        pass\n</code></pre> Required Functions Function Description <code>__init__()</code> Object creation function. <code>compute_heatmap()</code> Application of the XAI Method on an image. Source code in <code>aucmedi/xai/methods/xai_base.py</code> <pre><code>class XAImethod_Base(ABC):\n    \"\"\" An abstract base class for a XAI Method class.\n\n    Normally, this class is used internally in the xai_decoder function in the AUCMEDI XAI module.\n\n    This class provides functionality for running the compute_heatmap function,\n    which computes a heatmap for an image with a model.\n\n    ???+ example \"Create a custom XAImethod\"\n        ```python\n        from aucmedi.xai.methods.xai_base import XAImethod_Base\n\n        class My_custom_XAImethod(XAImethod_Base):\n            def __init__(self, model, layerName=None):\n                pass\n\n            def compute_heatmap(self, image, class_index, eps=1e-8):\n                pass\n        ```\n\n    ???+ info \"Required Functions\"\n        | Function            | Description                                |\n        | ------------------- | ------------------------------------------ |\n        | `__init__()`        | Object creation function.                  |\n        | `compute_heatmap()` | Application of the XAI Method on an image. |\n\n    \"\"\"\n    #---------------------------------------------#\n    #                Initialization               #\n    #---------------------------------------------#\n    @abstractmethod\n    def __init__(self, model, layerName=None):\n        \"\"\" Initialization function for creating a XAI Method object.\n        ```\n        __init__(model, layerName=None)\n        ```\n\n        Args:\n            model (keras.model):               Keras model object.\n            layerName (str):                Layer name of the convolutional layer for heatmap computation.\n        \"\"\"\n        pass\n\n    #---------------------------------------------#\n    #             Heatmap Computation             #\n    #---------------------------------------------#\n    def compute_heatmap(self, image, class_index, eps=1e-8):\n        \"\"\" Core function for computing the XAI heatmap for a provided image and for specific classification outcome.\n\n        ???+ attention\n            Be aware that the image has to be provided in batch format.\n\n        Args:\n            image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n            class_index (int):                  Classification index for which the heatmap should be computed.\n            eps (float):                        Epsilon for rounding.\n\n        The returned heatmap should be encoded within a range of [0,1]\n\n        ???+ attention\n            The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n        Returns:\n            heatmap (numpy.ndarray):            Computed XAI heatmap for provided image.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/xai/methods/xai_base/#aucmedi.xai.methods.xai_base.XAImethod_Base.__init__","title":"<code>__init__(model, layerName=None)</code>  <code>abstractmethod</code>","text":"<p>Initialization function for creating a XAI Method object. <pre><code>__init__(model, layerName=None)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.model</code> <p>Keras model object.</p> required <code>layerName</code> <code>str</code> <p>Layer name of the convolutional layer for heatmap computation.</p> <code>None</code> Source code in <code>aucmedi/xai/methods/xai_base.py</code> <pre><code>@abstractmethod\ndef __init__(self, model, layerName=None):\n    \"\"\" Initialization function for creating a XAI Method object.\n    ```\n    __init__(model, layerName=None)\n    ```\n\n    Args:\n        model (keras.model):               Keras model object.\n        layerName (str):                Layer name of the convolutional layer for heatmap computation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/xai/methods/xai_base/#aucmedi.xai.methods.xai_base.XAImethod_Base.compute_heatmap","title":"<code>compute_heatmap(image, class_index, eps=1e-08)</code>","text":"<p>Core function for computing the XAI heatmap for a provided image and for specific classification outcome.</p> Attention <p>Be aware that the image has to be provided in batch format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>numpy.ndarray</code> <p>Image matrix encoded as NumPy Array (provided as one-element batch).</p> required <code>class_index</code> <code>int</code> <p>Classification index for which the heatmap should be computed.</p> required <code>eps</code> <code>float</code> <p>Epsilon for rounding.</p> <code>1e-08</code> <p>The returned heatmap should be encoded within a range of [0,1]</p> Attention <p>The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.</p> <p>Returns:</p> Name Type Description <code>heatmap</code> <code>numpy.ndarray</code> <p>Computed XAI heatmap for provided image.</p> Source code in <code>aucmedi/xai/methods/xai_base.py</code> <pre><code>def compute_heatmap(self, image, class_index, eps=1e-8):\n    \"\"\" Core function for computing the XAI heatmap for a provided image and for specific classification outcome.\n\n    ???+ attention\n        Be aware that the image has to be provided in batch format.\n\n    Args:\n        image (numpy.ndarray):              Image matrix encoded as NumPy Array (provided as one-element batch).\n        class_index (int):                  Classification index for which the heatmap should be computed.\n        eps (float):                        Epsilon for rounding.\n\n    The returned heatmap should be encoded within a range of [0,1]\n\n    ???+ attention\n        The shape of the returned heatmap is 2D -&gt; batch and channel axis will be removed.\n\n    Returns:\n        heatmap (numpy.ndarray):            Computed XAI heatmap for provided image.\n    \"\"\"\n    pass\n</code></pre>"}]}